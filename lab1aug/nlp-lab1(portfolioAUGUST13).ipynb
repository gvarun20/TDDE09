{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Tokenisation and embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will build an understanding of how text can be transformed into representations that computers can process and learn from. Specifically, you will explore two key concepts: *tokenisation* and *embeddings*. Tokenisation splits text into smaller units such as words, subwords, or characters. Embeddings are dense, fixed-size vector representations of tokens in a continuous space.\n",
    "\n",
    "*Tasks you can choose for the oral exam are marked with the graduation cap 🎓 emoji.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the lab, you will code and analyse a tokeniser based on the Byte Pair Encoding (BPE) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BPE tokeniser transforms text into a list of integers representing tokens. As a warm-up, you will implement two utility functions on such lists. To simplify things, we define a shorthand for the type of pairs of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pair = tuple[int, int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🎈 Task 1.01: Counting pairs\n",
    "\n",
    "Write a function that counts all occurrences of pairs of consecutive token IDs in a given list. The function should return a dictionary that maps each pair to its count. Skip counts that are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [1, 2, 3, 2, 1, 2, 3, 1, 2, 3 ,1 , 1, 2, 2, 3, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 2): 5, (2, 3): 4, (3, 2): 1, (2, 1): 1, (3, 1): 3, (1, 1): 1, (2, 2): 1}\n"
     ]
    }
   ],
   "source": [
    "def count(ids: list[int]) -> dict[Pair, int]:\n",
    "    pair_freqs = {}\n",
    "    \n",
    "    # Iterate through the list of token IDs\n",
    "    for i in range(len(ids) - 1):\n",
    "        pair = (ids[i], ids[i + 1])\n",
    "        pair_freqs[pair] = pair_freqs.get(pair, 0) + 1\n",
    "    \n",
    "    return pair_freqs\n",
    "\n",
    "# Example usage:\n",
    "#ids = [1, 2, 3, 2, 1, 2, 3, 1]\n",
    "print(count(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🎈 Task 1.02: Replacing pairs\n",
    "\n",
    "Write a function that replaces all occurrences of a specified pair of consecutive token IDs in a given list by a new ID. The function should return the modified list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69, 3, 2, 69, 3, 69, 3, 1, 69, 2, 3, 69]\n"
     ]
    }
   ],
   "source": [
    "def replace(ids: list[int], pair: Pair, new_id: int) -> list[int]:\n",
    "    modified_ids = ids.copy()  # copy of the original list\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(modified_ids) - 1:\n",
    "        if (modified_ids[i], modified_ids[i + 1]) == pair:\n",
    "            # Replace pair with new ID\n",
    "            modified_ids[i : i + 2] = [new_id]\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return modified_ids\n",
    "\n",
    "pair = (1, 2)\n",
    "new_id = 69\n",
    "print(replace(ids, pair, new_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the core code for the tokeniser in the form of a class `Tokenizer`. This class implements two methods: `encode()` converts an input text to a list of token IDs by exhaustively applying rules for merging pairs of consecutive IDs, and `decode()` reverses this process by looking up the tokens corresponding to the token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.merges = {}\n",
    "        self.vocab = {i: bytes([i]) for i in range(2**8)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "        while True:\n",
    "            counts = count(ids)\n",
    "            mergeable_pairs = counts.keys() & self.merges.keys()\n",
    "            if len(mergeable_pairs) == 0:\n",
    "                break\n",
    "            to_merge = min(mergeable_pairs, key=self.merges.get)\n",
    "            ids = replace(ids, to_merge, self.merges[to_merge])\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return b\"\".join((self.vocab[i] for i in ids)).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.merges: Dict[tuple, int] = {}  # Mapping of pairs to merge and their priority\n",
    "        self.vocab: Dict[int, bytes] = {i: bytes([i]) for i in range(2**8)}  # Vocabulary mapping integers to byte values\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encodes a string of text into a list of integer IDs based on the tokenizer's merges.\"\"\"\n",
    "        ids: List[int] = list(text.encode(\"utf-8\"))  # List of integers representing the text in UTF-8 encoding\n",
    "        while True:\n",
    "            counts = count(ids)  # Count occurrences of pairs in `ids`\n",
    "            mergeable_pairs = counts.keys() & self.merges.keys()  # Find intersecting pairs\n",
    "            if len(mergeable_pairs) == 0:\n",
    "                break\n",
    "            to_merge = min(mergeable_pairs, key=self.merges.get)  # Select the pair with the lowest merge score\n",
    "            ids = replace(ids, to_merge, self.merges[to_merge])  # Replace the pair with the merged value\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Decodes a list of integer IDs back into a string.\"\"\"\n",
    "        return b\"\".join((self.vocab[i] for i in ids)).decode(\"utf-8\")  # Join and decode into UTF-8 string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🎓 Task 1.03: Encoding and decoding\n",
    "\n",
    "Explain how the code implements the BPE algorithm. Use the following steps to check your understanding:\n",
    "\n",
    "**Step&nbsp;1.** Annotate the attributes and methods of the `Tokenizer` class with their Python types. In particular, what is the type of `self.merges`? Use the `Pair` shorthand.\n",
    "\n",
    "**Step&nbsp;2.** Explain how the implementation chooses which merge rule to apply. Provide an example that illustrates the logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### self.merges: This is a dictionary (dict) that maps token pairs to a new token ID.It represents the merging rules for BPE.Each key in self.merges is a Pair, meaning it is a tuple of two integers (each representing a byte token).Each value in self.merges is an integer, which represents the new token ID resulting from the merge. \n",
    "#### self.vocab:This is a dictionary mapping token IDs (integers) to their corresponding byte representations.Initially, it contains the byte values from 0 to 255, each mapped to its corresponding byte representation.\n",
    "\n",
    "## Step 1:Tokenizer class:This class has two methods:\n",
    "- **(1)** encode(self, text: str) -> list[int].Converts a given text into a list of token IDs.It:Encodes the text into bytes.Iteratively applies merge rules from self.merges to combine token pairs.Returns the final tokenized representation as a list of integers.\n",
    "- **(2)** decode(self, ids: list[int]) -> str:Converts a list of token IDs back into the original text.It:Looks up each token ID in self.vocab to retrieve the corresponding byte representation.Joins the bytes together.Decodes them back into a UTF-8 string.\n",
    "\n",
    "## Step 2: Explain How the Merge Rule is Applied::\n",
    "- **(1)** Identify adjacent token pairs The method count(ids) (which is missing in the provided code) is expected to count the occurrences of adjacent token pairs in ids\n",
    "- **(2)** Check which pairs exist in self.mergesThe intersection of counted token pairs and self.merges.keys() determines the possible merge candidates.\n",
    "- **(3)** Select the best pair to merge\n",
    "The merge rule with the lowest assigned integer value in self.merges is chosen first\n",
    "- **(4)** Replace the selected pair with the new merged token.The function replace(ids, to_merge, self.merges[to_merge]) (not defined) replaces occurrences of to_merge in ids with the new merged token ID.\n",
    "- **(5)** Repeat the process until no more merges are possible\n",
    "- Example:[101,102,103] ->pairs (101,102)and (102,103) .next it looks for pairs that exist in selfmerge.among them it selects the pair with smallest value in self merge. if self merge applied = {(101, 102): 256, (102, 103): 257}, the pair (101, 102) (value 256) is chosen over (102, 103) (value 257).the selected pair is replaced with 256."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback \n",
    "### 1.03: For step 1 you should annotate the code, you have just mentioned that self.merges is a dict, but could you elaborate more on what this dict contains. The question also asks you to “Annotate the attributes and methods of the `Tokenizer` class with their Python types.”, which you did not do. For step 2 you did not “Provide an example that illustrates the logic.”. If you are uncertain of how to do this, ask during the lab sessions. \n",
    "\n",
    "#### Respond/correction made: Most of the changes have been made  in the updated version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon initialisation, a tokeniser has an empty set of merge rules. Your next task is to complete the BPE algorithm and write code to learn these merge rules from a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🎓 Task 1.04: Training a tokeniser\n",
    "\n",
    "Write a function that induces a BPE tokeniser from a given text. The function should take the text (a string) and a target vocabulary size as input and return the trained tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def from_text(text: str, vocab_size: int) -> Tokenizer:\n",
    "    tokenizer = Tokenizer()\n",
    "    ids = list(text.encode(\"utf-8\"))\n",
    "   \n",
    "    counter = Counter(text)\n",
    "    tokenizer.vocab = list(counter)\n",
    "  #  ids = []\n",
    "#for i in range(len(tokenizer.vocab):\n",
    "  #      ids[i] = ord(tokenizer.vocab[i])\n",
    "    new_idx = 256\n",
    "    while len(tokenizer.vocab) < vocab_size:\n",
    "        counts = count(ids)\n",
    "        if not counts:\n",
    "            break\n",
    "        most_frequent = max(counts, key=counts.get)\n",
    "        #print(most_frequent)\n",
    "        tokenizer.merges[most_frequent] = new_idx\n",
    "        tokenizer.vocab.append( most_frequent[0] + most_frequent[1])\n",
    "        ids = replace(ids, most_frequent, new_idx)\n",
    "        new_idx+=1\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you test your implementation, we provide three text files together with tokenisers trained on these files. Each text file contains the first 1&nbsp;million Unicode characters in a language-specific Wikipedia:\n",
    "\n",
    "| Text file | Tokeniser file | Wikipedia |\n",
    "|---|---|---|\n",
    "| `wiki-en-1m.txt` | `wiki-en-1m.tok` | [Simple English](https://simple.wikipedia.org/) |\n",
    "| `wiki-is-1m.txt` | `wiki-is-1m.tok` | [Icelandic](https://is.wikipedia.org/) |\n",
    "| `wiki-sv-1m.txt` | `wiki-sv-1m.tok` | [Swedish](https://sv.wikipedia.org/) |\n",
    "\n",
    "A tokeniser file consists of lines specifying merge rules. For example, the first line in the tokeniser file for Swedish is `101 114`, which expresses that this rule combines the token with ID 101 (`e`) and the token with ID 114 (`r`). The ID of the new token (`er`) is 256 plus the (zero-indexed) line number on which the rule is found. The following code saves a `Tokenizer` to a file with this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_wikipedia(file_path: str, vocab_size: int) -> Tokenizer:\n",
    "    #with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        #text = file.read()\n",
    "    text = open(\"wiki-en-1m.txt\",'r').read()\n",
    "    \n",
    "    return from_text(text, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_wikipedia(file_path: str, vocab_size: int) -> Tokenizer:\n",
    "    #with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        #text = file.read()\n",
    "    text = open(\"wiki-is-lm.txt\",'r').read()\n",
    "    \n",
    "    return from_text(text, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(tokenizer: Tokenizer, filename: str) -> None:\n",
    "    with open(filename, \"w\") as f:\n",
    "        for fst, snd in tokenizer.merges:\n",
    "            print(f\"{fst} {snd}\", file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution for 1.04:\n",
    "Training a BPE Tokenizer\n",
    "The from_text() function trains a Byte Pair Encoding (BPE) tokenizer by progressively merging the most frequent adjacent character pairs into new tokens until the vocabulary reaches a specified size. This technique is widely used in natural language processing (NLP) to create compact and efficient text representations.\n",
    "\n",
    "##### The process begins by converting the input text into a sequence of byte values using UTF-8 encoding. Each character in the text is mapped to its corresponding byte representation. A Counter function is then used to extract unique characters, which are stored in the initial vocabulary. This ensures that the tokenizer starts with a minimal set of single-character tokens before learning larger units.\n",
    "\n",
    "To enable token merging, token IDs are assigned. Standard byte values range from 0-255, while new merged tokens receive IDs starting from 256 and increment as more merges occur. This prevents conflicts with existing byte values and allows for structured vocabulary expansion.\n",
    "\n",
    "The function then iteratively counts occurrences of adjacent token pairs. The most frequently occurring pair is selected and merged into a new token. This newly merged token is stored in the tokenizer.merges dictionary and added to the vocabulary. The token sequence is updated by replacing occurrences of the merged pair with its new token ID. This process repeats until the vocabulary reaches the desired size.\n",
    "\n",
    "Finally, the trained tokenizer is saved in a file, where each line represents a merge rule. The format specifies which token IDs should be merged, and new token IDs are assigned sequentially as 256 + (zero-indexed line number). This saved tokenizer can be reloaded later for text encoding and decoding, making it a key component in machine learning models such as GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback::Your “from_text” function changes the tokenizer.vocab from a dict to a list. Even though this might work, it is advised to keep the type of the vocabulary as it is instantiated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code, compare the saved tokeniser to the provided tokeniser using `diff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769,806d768\n",
      "< 328 361\n",
      "< 270 115\n",
      "< 275 115\n",
      "< 109 260\n",
      "< 101 451\n",
      "< 809 323\n",
      "< 98 344\n",
      "< 324 335\n",
      "< 521 792\n",
      "< 366 312\n",
      "< 50 49\n",
      "< 476 105\n",
      "< 50 32\n",
      "< 260 118\n",
      "< 429 292\n",
      "< 116 300\n",
      "< 266 302\n",
      "< 760 285\n",
      "< 557 674\n",
      "< 105 102\n",
      "< 44 417\n",
      "< 278 290\n",
      "< 584 434\n",
      "< 115 292\n",
      "< 384 116\n",
      "< 470 313\n",
      "< 422 103\n",
      "< 447 100\n",
      "< 97 119\n",
      "< 414 265\n",
      "< 108 421\n",
      "< 292 312\n",
      "< 546 268\n",
      "< 448 318\n",
      "< 111 262\n",
      "< 418 259\n",
      "< 938 617\n",
      "< 99 322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "with open('wiki-en-1m.txt',\"r\",encoding = \"utf-8\") as file:\n",
    "  text = file.read()\n",
    "#  Generate the tokenizer using the from_text function\n",
    "  tokenizer = from_text(text, 1024)\n",
    "  # print(tokenizer)\n",
    "\n",
    "#  Save the generated tokenizer using the save function\n",
    "  save(tokenizer, \"generated_tokenizer.tok\")\n",
    "  \n",
    "  #  Compare the generated tokenizer with the provided tokenizer using 'diff'\n",
    "os.system(\"diff generated_tokenizer.tok wiki-en-1m.tok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation quirks\n",
    "\n",
    "The tokeniser is a key component of language models, as it defines the minimal chunks of text the model can “see” and work with. As you will see in this section, tokenisation is also responsible for several deficiencies and unexpected behaviours of language models.\n",
    "\n",
    "One helpful tool for experimenting with tokenisers in language models is the web app [Tiktokenizer](https://tiktokenizer.vercel.app/). This app lets you play around with, among others, [`cl100k_base`](https://tiktokenizer.vercel.app/?model=cl100k_base), the tokeniser used in the free version of ChatGPT and OpenAI’s APIs, and [`o200k_base`](https://tiktokenizer.vercel.app/?model=o200k_base), used in GPT-4o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🎓 Task 1.05: Tokenisation quirks\n",
    "\n",
    "Prompt [ChatGPT](https://chatgpt.com/) to reverse the letters in the following words:\n",
    "\n",
    "```\n",
    "creativecommons\n",
    "MERCHANTABILITY\n",
    "NSNotification\n",
    "authentication\n",
    "```\n",
    "\n",
    "How many of these words come out right? What could be the problem when words come out wrong? Generate ideas by inspecting the words in Tiktokenizer. Try to come up with other prompts that illustrate problems related to tokenisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **(1)** to analyze how ChatGPT handles word reversal, particularly with composite words, capitalization, and tokenization structures. Initially, there was a theory that compound words like \"swimsuit\" or \"popcorn\" might be problematic for ChatGPT due to their meaning being tied to a single concept. However, ChatGPT handled these well despite them being split into multiple tokens in tiktokenizer.\n",
    "\n",
    "However, the model struggled more with random capitalization, likely because capitalized letters increase the number of tokens. The results from ChatGPT showed inconsistencies when reversing words, with only \"authentication\" being reversed correctly, while others like \"MERCHANTABILITY\" and \"NSNotification\" were incorrect. This suggests that uppercase letters disrupt word structure, making reversal more challenging.\n",
    "\n",
    " Factors like capitalization, special characters, and formatting can influence how the model interprets words, potentially leading to unexpected results.\n",
    "\n",
    "\n",
    "\n",
    "- The compound word \"creativecommons\" was tokenized into a single token, while a random word like \"unbelievable\" was correctly tokenized into three subwords. This difference illustrates that ChatGPT attempts to process \"creativecommons\" by performing character-level reversal, which ultimately leads to an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feedback:The image that you added is not visible to me. Perhaps it is because when it is included in the notebook, it is only included as a link.\n",
    "\n",
    "##### Solution to the feedback: corrections have been made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation and multi-linguality\n",
    "\n",
    "Many NLP systems and the tokenisers used with them are primarily trained on English data. In the next task, you will reflect on the effect this has when they are used to process non-English data.\n",
    "\n",
    "The *context length* of a language model is the maximum number of preceding tokens the model can condition on when predicting the next token. This number is fixed and cannot be changed after training the model. For example, the context length of GPT-2 ([Radford et al., 2019](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)) is 1,024. \n",
    "\n",
    "While the context length of a language model is fixed, the amount of information that can be squeezed into this context length will depend on the tokeniser. Informally speaking, a model that needs more tokens to represent a given text cannot condition on as much information as one that needs fewer tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🎓 Task 1.06: Tokenisation and multi-linguality\n",
    "\n",
    "Train a tokeniser on the English text file from Task&nbsp;1.04 and test it on the same text. How many tokens does it split the text into? Based on this, what is the expected number of Unicode characters of English text that can be fit into a context length of 1,024?\n",
    "\n",
    "What do the numbers look like if you test the English tokeniser on the Icelandic text instead? What could explain the differences?\n",
    "\n",
    "Interpreting the expected number of Unicode characters as a measure of representation efficiency, what do your results tell you about the efficiency of a language model primarily trained on English data when it is used to process non-English data? Why are these findings relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English - Number of tokens: 375309\n",
      "English - Number of characters: 1000000\n",
      "English - Number of bytes (UTF-8): 1001360\n",
      "English - Average bytes per token: 2.6680948231990174\n",
      "English - Expected number of Unicode characters in 1,024 bytes: 383.0\n"
     ]
    }
   ],
   "source": [
    "#  Load and Process the Text File (wiki-en.txt)\n",
    "with open('wiki-en-1m.txt', 'r', encoding='utf-8') as file:\n",
    "    english_text = file.read()\n",
    "\n",
    "# Train the tokenizer\n",
    "vocab_size = 1024  # Set the desired vocabulary size\n",
    "tokenizer = from_text(text, vocab_size)\n",
    "\n",
    "# Test the tokenizer by encoding and decoding\n",
    "encoded_text_en = tokenizer.encode(text)\n",
    "# decoded_text = tokenizer.decode(encoded_text)\n",
    "\n",
    "# Number of tokens in the English text\n",
    "num_tokens_en = len(encoded_text_en)\n",
    "\n",
    "# Number of characters in the English text\n",
    "num_chars_en = len(english_text)\n",
    "\n",
    "# Number of bytes in the UTF-8 encoded English text\n",
    "num_bytes_en = len(english_text.encode('utf-8'))\n",
    "\n",
    "# Estimate average bytes per token\n",
    "avg_bytes_per_token_en = num_bytes_en / num_tokens_en if num_tokens_en != 0 else 0\n",
    "\n",
    "# Expected number of Unicode characters that fit into a context length of 1,024 bytes\n",
    "max_chars_in_context_en = 1024 // avg_bytes_per_token_en\n",
    "\n",
    "print(f\"English - Number of tokens: {num_tokens_en}\")\n",
    "print(f\"English - Number of characters: {num_chars_en}\")\n",
    "print(f\"English - Number of bytes (UTF-8): {num_bytes_en}\")\n",
    "print(f\"English - Average bytes per token: {avg_bytes_per_token_en}\")\n",
    "print(f\"English - Expected number of Unicode characters in 1,024 bytes: {max_chars_in_context_en}\")\n",
    "\n",
    "# Optionally, save the trained tokenizer\n",
    "save(tokenizer, \"tokenizer_output.tok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Icelandic - Number of tokens: 752403\n",
      "Icelandic - Number of characters: 1000000\n",
      "Icelandic - Number of bytes (UTF-8): 1091189\n",
      "Icelandic - Average bytes per token: 1.4502719951940648\n",
      "Icelandic - Expected number of Unicode characters in 1,024 bytes: 706.0\n"
     ]
    }
   ],
   "source": [
    "# Read the Icelandic text file\n",
    "with open('wiki-is-1m.txt', 'r', encoding='utf-8') as file:\n",
    "    icelandic_text = file.read()\n",
    "\n",
    "# Encode the Icelandic text to get token ids\n",
    "encoded_text_is = tokenizer.encode(icelandic_text)\n",
    "\n",
    "# Number of tokens in the Icelandic text\n",
    "num_tokens_is = len(encoded_text_is)\n",
    "\n",
    "# Number of characters in the Icelandic text\n",
    "num_chars_is = len(icelandic_text)\n",
    "\n",
    "# Number of bytes in the UTF-8 encoded Icelandic text\n",
    "num_bytes_is = len(icelandic_text.encode('utf-8'))\n",
    "\n",
    "# Estimate average bytes per token\n",
    "avg_bytes_per_token_is = num_bytes_is / num_tokens_is if num_tokens_is != 0 else 0\n",
    "\n",
    "# Expected number of Unicode characters that fit into a context length of 1,024 bytes\n",
    "max_chars_in_context_is = 1024 // avg_bytes_per_token_is\n",
    "\n",
    "print(f\"Icelandic - Number of tokens: {num_tokens_is}\")\n",
    "print(f\"Icelandic - Number of characters: {num_chars_is}\")\n",
    "print(f\"Icelandic - Number of bytes (UTF-8): {num_bytes_is}\")\n",
    "print(f\"Icelandic - Average bytes per token: {avg_bytes_per_token_is}\")\n",
    "print(f\"Icelandic - Expected number of Unicode characters in 1,024 bytes: {max_chars_in_context_is}\")\n",
    "# Optionally, save the trained tokenizer\n",
    "save(tokenizer, \"tokenizer_output_is.tok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.06 Solution\n",
    "- Icelandic has more tokens than English due to its high inflection and morphological complexity. Its UTF-8 byte size is higher because of special characters, but it uses fewer bytes per token, making tokenization efficient. Conversely, English tokens consume more bytes due to inefficiencies with complex words. Icelandic text fits more characters into 1,024 bytes since it generally requires fewer bytes per token. \n",
    "\n",
    "- For English-trained models, processing non-English languages can be less efficient due to differing linguistic structures; for example, Icelandic may need more tokens to express the same idea because of its compound words. The efficiency of a model hinges on its ability to tokenize and encode characters properly, implying that models trained mainly on English may struggle with multilingual tasks.\n",
    "\n",
    "- These insights underscore the need for optimizing tokenization strategies and pre-trained models for multiple languages, emphasizing training on diverse datasets. tokenization efficiency across languages\n",
    "\n",
    "- When a tokenizer trained on English text is applied to both English and Icelandic texts, clear differences in tokenisation efficiency emerge. The English tokenizer performs efficiently on English data, producing fewer, longer tokens by capturing frequent patterns and subwords. In contrast, when applied to Icelandic, the same tokenizer splits the text into many more, shorter tokens. This happens because Icelandic contains characters and patterns unfamiliar to the English-trained tokenizer, leading to more frequent breaks and less compression. As a result, the same number of tokens (e.g., 1,024) can represent more characters in English than in Icelandic. This directly affects the performance of language models like GPT, which have fixed context limits in terms of tokens—not characters or bytes. When non-English languages require more tokens to express the same amount of text, the model can “see” less of the input at once, leading to reduced performance. This means that language models trained primarily on English tend to be less efficient and less accurate when processing non-English content. The findings highlight a core challenge in multilingual NLP: tokenisation strategies trained on a single language often underperform on others, reducing both efficiency and fairness. To address this, models need to be trained with multilingual or language-agnostic tokenisers that can handle a wide variety of scripts and language structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of the lab, you will explore embeddings. An embedding layer is a network component that assigns each item in a finite set of elements (often called a *vocabulary*) a fixed-size vector. At first, these vectors are filled with random values, but during training, they are adjusted to suit the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you build an intuition for embeddings and the vector representations learned by them, we will use a simple bag-of-words text classifier. The core part of this classifier only takes a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(self.embedding(x).mean(dim=-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🎈 Task 1.07: Bag-of-words classifier\n",
    "\n",
    "Explain how the bag-of-words classifier works. How does the code match the diagram you saw in the lectures? Why is there only one `nn.Embedding`, while the diagram shows three embedding layers? What does the keyword argument `dim=-2` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution for task 1.07\n",
    "- Bag-of-Words (BoW) Classifier: Represents text as an unordered collection of words, mapping each word to a dense vector using an embedding layer. The embeddings are averaged to create a fixed-size document representation, which is then passed through a linear layer for classification. Softmax is often applied to generate probabilities.\n",
    "\n",
    "- Code vs. Diagram:\n",
    "The code tokenizes words into unique IDs and maps them to dense vectors using nn.Embedding.\n",
    "The embeddings are averaged using .mean(dim=-2), ensuring a fixed-size representation.\n",
    "A single nn.Embedding layer is used in the code, though the diagram illustrates multiple embedding layers. PyTorch handles multiple words within a single embedding layer.\n",
    "\n",
    "- Role of dim=-2:\n",
    "The embedding layer outputs a tensor of shape (batch_size, num_words, embedding_dim).\n",
    "The .mean(dim=-2) operation averages embeddings across words, reducing the shape to (batch_size, embedding_dim), producing one vector per sentence.\n",
    "This ensures efficiency while preserving key semantic information for text classification tasks where word order is not important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will apply the classifier to a small dataset with Amazon customer reviews. This dataset is taken from [a much larger dataset](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/) first described by [Blitzer et al. (2007)](https://aclanthology.org/P07-1056/).\n",
    "\n",
    "The dataset contains whitespace-tokenised product reviews from two topics: cameras (`camera`) and music (`music`). Each review is additionally annotated for sentiment towards the product at hand: negative (`neg`) or positive (`pos`). The topic and sentiment labels are prepended to the review. As an example, here is the first review from the training data:\n",
    "\n",
    "```\n",
    "music neg oh man , this sucks really bad . good thing nu-metal is dead . thrash metal is real metal , this is for posers\n",
    "```\n",
    "\n",
    "The next cell contains a custom [`Dataset`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) class for the review dataset. To initialise an instance of this class, you specify the name of the file containing the reviews you want to load (`filename`) and which of the two labels you want to use (`label`): topic (0) or sentiment (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, filename: str, label: int = 0) -> None:\n",
    "        with open(filename) as f:\n",
    "            tokenized_lines = [line.split() for line in f]\n",
    "        self.items = [(tokens[2:], tokens[label]) for tokens in tokenized_lines]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[list[str], str]:\n",
    "        return self.items[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectoriser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed a review into the bag-of-words classifier, you first need to turn it into a vector of token IDs. Likewise, you need to convert the label (topic or sentiment) into an integer. The next cell contains a partially completed `ReviewVectoriser` class that handles this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "\n",
    "# Type abbreviation for review–label pairs\n",
    "Item = tuple[list[str], str]\n",
    "\n",
    "\n",
    "class ReviewVectorizer:\n",
    "    PAD = \"[PAD]\"\n",
    "    UNK = \"[UNK]\"\n",
    "\n",
    "    def __init__(self, dataset: ReviewDataset, n_vocab: int = 1024) -> None:\n",
    "        # Unzip the dataset into reviews and labels\n",
    "        reviews, labels = zip(*dataset)\n",
    "\n",
    "        # Count the tokens and get the most common ones\n",
    "        counter = Counter(t for r in reviews for t in r)\n",
    "        most_common = [t for t, _ in counter.most_common(n_vocab - 2)]\n",
    "\n",
    "        # Create the token-to-index and label-to-index mappings\n",
    "        self.t2i = {t: i for i, t in enumerate([self.PAD, self.UNK] + most_common)}\n",
    "        self.l2i = {l: i for i, l in enumerate(sorted(set(labels)))}\n",
    "\n",
    "    def __call__(self, items: list[Item]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        reviews, labels = zip(*items)\n",
    "                \n",
    "        # Convert reviews to lists of token IDs, replacing unknown tokens\n",
    "        tokenized_reviews = [[self.t2i.get(t, self.t2i[self.UNK]) for t in review] for review in reviews]\n",
    "        \n",
    "        # Pad the tokenized reviews to the same length\n",
    "        max_len = max(len(r) for r in tokenized_reviews)\n",
    "        padded_reviews = [r + [self.t2i[self.PAD]] * (max_len - len(r)) for r in tokenized_reviews]\n",
    "        \n",
    "        # Convert labels to indices\n",
    "        label_indices = [self.l2i[label] for label in labels]\n",
    "        \n",
    "        return torch.tensor(padded_reviews, dtype=torch.long), torch.tensor(label_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `ReviewVectoriser` maps tokens and labels to IDs using two Python dictionaries. These dictionaries are set up when the vectoriser is initialised and queried when the vectoriser is called on a batch of review–label pairs. They include IDs for two special tokens:\n",
    "\n",
    "`[PAD]` (Padding): Reviews can have different lengths, but PyTorch requires all vectors in a batch to be the same size. To handle this, the vectoriser adds `[PAD]` tokens to the end of shorter reviews so they match the length of the longest review in the batch.\n",
    "\n",
    "`[UNK]` (Unknown): If a review contains a token that is not in the token-to-ID dictionary, the vectoriser assigns it the ID of the `[UNK]` token instead of a regular ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🎓 Task 1.08: Vectoriser\n",
    "\n",
    "Explain and complete the code of the vectoriser. Follow these steps:\n",
    "\n",
    "**Step&nbsp;1.** Explain how unzipping works. What are the types of `reviews` and `labels`?\n",
    "\n",
    "**Step&nbsp;2.** Explain how the token-to-ID and label-to-ID mappings are constructed. How does the `most_common()` method deal with elements that occur equally often?\n",
    "\n",
    "**Step&nbsp;3.** Complete the implementation of the `__call__()` method. This method should convert a list of $m$ review–label pairs into a pair $(X, y)$ where $X$ is a matrix containing the vectors with token IDs for the reviews, and $y$ is a vector containing the IDs of the corresponding labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION FOR 1.08\n",
    "\n",
    "The `ReviewVectorizer` class is designed to convert text-based reviews into numerical representations for machine learning models using PyTorch. The class takes a dataset of text reviews with their labels and provides a method to transform a batch of reviews into fixed-length numerical tensors.\n",
    "\n",
    "---\n",
    "\n",
    "##### **Understanding the Initialization (`__init__` Method)**\n",
    "\n",
    "When an instance of the `ReviewVectorizer` class is created, it takes two parameters:\n",
    "\n",
    "1. **`dataset`**: A list of review-label pairs, where:\n",
    "   - Each review is represented as a list of tokens (words).\n",
    "   - The label is a string.\n",
    "\n",
    "2. **`n_vocab`**: The maximum number of words included in the vocabulary (default is `1024`).\n",
    "\n",
    "##### **Processing the Dataset**\n",
    "- The dataset is split into separate lists:\n",
    "  - One containing all reviews.\n",
    "  - Another containing all labels.\n",
    "\n",
    "- A `Counter` object counts the frequency of each token (word).\n",
    "- The `most_common(n_vocab - 2)` function selects the most frequent words while reserving space for two special tokens:\n",
    "\n",
    "  1. **`\"[PAD]\"`** (Padding Token) → Ensures all reviews have the same length.\n",
    "  2. **`\"[UNK]\"`** (Unknown Token) → Represents rare or out-of-vocabulary words.\n",
    "\n",
    "##### **Creating the Token-to-Index Mapping (`t2i`)**\n",
    "The `t2i` dictionary assigns unique indices to each token:\n",
    "- `\"[PAD]\"` → Index **0**  \n",
    "- `\"[UNK]\"` → Index **1**  \n",
    "- Most common words are assigned subsequent indices.\n",
    "\n",
    "---\n",
    "\n",
    "##### **Understanding the `__call__` Method**\n",
    "\n",
    "This method enables the class instance to be used as a function to transform a batch of reviews into tensors.\n",
    "\n",
    "##### **Processing Steps:**\n",
    "1. The input batch (a list of review-label pairs) is split into:\n",
    "   - A list of reviews.\n",
    "   - A list of labels.\n",
    "\n",
    "2. Each review (a list of words) is converted into a list of numerical token IDs:\n",
    "   - If a word exists in `t2i`, its corresponding index is used.\n",
    "   - Otherwise, it is replaced with `\"[UNK]\"` (index **1**).\n",
    "\n",
    "3. Reviews are **padded** to match the length of the longest review:\n",
    "   - Shorter reviews are padded with `\"[PAD]\"` (index **0**).\n",
    "\n",
    "4. Labels are converted to numerical indices using the `l2i` dictionary.\n",
    "\n",
    "5. The processed reviews and labels are returned as **PyTorch tensors**, making them ready for deep learning models.\n",
    "\n",
    "---\n",
    "\n",
    "The `ReviewVectorizer` class is a crucial preprocessing step in NLP tasks, ensuring that text data is transformed into a structured numerical format compatible with deep learning models. By implementing tokenization, padding, and label encoding, this class efficiently prepares review data for training and evaluation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL CONTD:Step 1: Understanding Unzipping and Data Types\n",
    "In Python, the zip(*) function is used to \"unzip\" a list of tuples, effectively separating them into individual lists. In this case, the dataset consists of review-label pairs, where each review is a list of strings (tokens), and each label is a single string. When zip(*dataset) is called, it extracts all the reviews into one tuple and all the labels into another tuple. The reviews are tokenized text data, meaning each review is a list of words or subwords, while the labels represent categorical classifications, such as sentiment categories (\"positive,\" \"negative,\" etc.) or topic labels. Since zip(*) returns tuples, reviews and labels are tuples where reviews contain sequences of words, and labels contain string identifiers representing classes.\n",
    "\n",
    "##### Step 2: Constructing Token-to-ID and Label-to-ID Mappings\n",
    "The process begins with counting the occurrences of each token across all reviews using Counter, a specialized dictionary from the collections module. It collects word frequencies efficiently. Then, the most_common(n_vocab - 2) method selects the most frequently occurring tokens, ensuring that only the most relevant words are included in the vocabulary. The most_common() method resolves ties (words with the same frequency) based on their order of appearance in the dataset.\n",
    "\n",
    "The token-to-index (t2i) dictionary is then built by first including special tokens [PAD] (for padding shorter reviews) and [UNK] (for unknown words not in the vocabulary), followed by the most frequent tokens. This dictionary maps each token to a unique integer ID. Similarly, the label-to-index (l2i) dictionary is created by assigning an integer index to each unique label, sorted alphabetically to ensure consistency across different datasets. This guarantees that labels are converted into numerical values for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feedback:Unzipping actually splits a list of pairs into two separate lists. \n",
    "\n",
    "##### Solution:the corrections have been made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the vectoriser completed, you are ready to train a classifier. More specifically, you can train two separate classifiers: one to predict the topic of a review, and one to predict the sentiment. The next cell contains a simple training loop that you can adapt for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train(filename: str=\"reviews-train.txt\", # this is the path of the traing data\n",
    "          learning_rate: float=0.001,  #this is the learning rate of the optimizer\n",
    "          batch_size: int=16,       # here the barch size of the dataloader\n",
    "          num_epochs: int=100,       # number of training epoch\n",
    "          vocab_size: int =1024,      #size of the vocabulary\n",
    "        hidden_size : int =64 ):      #Hidden layer size of the classifier \n",
    "    \n",
    "    dataset = ReviewDataset(filename, label=0) #Load the data set and initialoze components & Column index for labels in the dataset\n",
    "    processor = ReviewVectorizer(dataset, vocab_size)\n",
    "    model = Classifier(vocab_size, hidden_size, len(processor.l2i))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=processor,\n",
    "    )                    #It configures the dataset with flexible batch size\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):  # this is the training loop with configurable epoch\n",
    "        model.train()\n",
    "        running_loss = 0                 #Reset accumulated loss for this epoch\n",
    "        # this for loop iterates over the batches of data\n",
    "        for bx, by in data_loader:    # bx andby are the batch od token ID and the batch of the labels         \n",
    "            optimizer.zero_grad()           \n",
    "            output = model(bx)# here is the forward pas where the model compute the predctions for the current batch\n",
    "            # crossentropy expects predictions and handles sofmax internally\n",
    "            loss = F.cross_entropy(output, by)\n",
    "            loss.backward()# here we compute the gradients of loss \n",
    "            optimizer.step()# this updates the model parameter using compute gradient\n",
    "            running_loss += loss.item()\n",
    "        # len(data_loader)is the number of batches,,,running_loss is the total loss accross all the batches    \n",
    "        print(f\"Epoch {epoch}, loss: {running_loss / len(data_loader):.4f}\")\n",
    "    return processor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train(filename: str=\"reviews-train.txt\", learning_rate: float=0.001, batch_size: int=16, num_epochs: int=100):\n",
    "    dataset = ReviewDataset(filename, label=1)\n",
    "    processor = ReviewVectorizer(dataset, 1024)\n",
    "    model = Classifier(1024, 64, len(processor.l2i))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=processor,\n",
    "    )\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for bx, by in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(bx)\n",
    "            loss = F.cross_entropy(output, by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch}, loss: {running_loss / len(data_loader):.4f}\")\n",
    "    return processor, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🎓 Task 1.09: Training loop\n",
    "\n",
    "Explain the training loop. Follow these steps:\n",
    "\n",
    "**Step&nbsp;1.** Go through the training loop line-by-line and add comments where you find it suitable. Your comments should be detailed enough for you to explain the main steps of the loop.\n",
    "\n",
    "**Step&nbsp;2.** The training loop contains various hard-coded values like filename, learning rate, batch size, and epoch count. This makes the code less flexible. Revise the code so that you can specify these values using keyword arguments. Use the concrete values from the code as defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION FOR TASK 1.09\n",
    "\n",
    "The `train` function is responsible for training a neural network model for text classification. Below is a detailed explanation of the training loop.\n",
    "\n",
    "### 1. Dataset Preparation\n",
    "- The dataset is loaded using `ReviewDataset`, which reads training data from a file (`reviews-train.txt` by default). It assumes a binary classification task.\n",
    "- `ReviewVectorizer` processes the dataset, converting textual data into numerical vectors of a fixed size (`1024`).\n",
    "\n",
    "### 2. Model and Optimizer Initialization\n",
    "- The `Classifier` model is created with:\n",
    "  - An input size of `1024`\n",
    "  - A hidden layer of `64` neurons\n",
    "  - An output layer corresponding to the number of unique labels in the dataset (`len(processor.l2i)`).\n",
    "- The Adam optimizer is initialized with a learning rate of `0.001` to update the model’s parameters during training.\n",
    "\n",
    "### 3. Data Loader Setup\n",
    "- The dataset is wrapped in a PyTorch `DataLoader`, which enables efficient batching and shuffling.\n",
    "- The `collate_fn` argument ensures that `ReviewVectorizer` correctly formats batches of data for training.\n",
    "\n",
    "### 4. Training Loop Execution\n",
    "- The model is trained for `num_epochs` (default: `100`), iterating over the dataset multiple times to optimize performance.\n",
    "- At the start of each epoch:\n",
    "  - The model is set to training mode using `model.train()`, ensuring layers like dropout (if any) function correctly.\n",
    "  - `running_loss` is initialized to track the cumulative loss for the epoch.\n",
    "\n",
    "### 5. Mini-Batch Training\n",
    "For each batch:\n",
    "- The optimizer resets gradients using `optimizer.zero_grad()`.\n",
    "- The input batch (`bx`) is passed through the model to generate predictions.\n",
    "- The loss is computed using `F.cross_entropy(output, by)`, which measures the difference between predicted and actual labels.\n",
    "- Backpropagation is performed using `loss.backward()`, computing gradients for each model parameter.\n",
    "- The optimizer updates the model parameters using `optimizer.step()`.\n",
    "- The batch loss is added to `running_loss` to track the total loss for the epoch.\n",
    "\n",
    "### 6. Epoch Summary\n",
    "- After processing all batches, the average loss for the epoch is printed as:\n",
    "  ```python\n",
    "  print(f\"Epoch {epoch}, loss: {running_loss / len(data_loader):.4f}\")\n",
    "  ```\n",
    "- This helps monitor training progress.\n",
    "\n",
    "### 7. Final Return Values\n",
    "- After training, the function returns the trained `processor` and `model`, which can be used for inference or further evaluation.\n",
    "\n",
    "This training loop follows a standard supervised learning workflow in PyTorch, iterating through multiple epochs to minimize classification error and improve model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution for step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train(\n",
    "    filename: str = \"reviews-train.txt\",\n",
    "    learning_rate: float = 0.001,\n",
    "    batch_size: int = 16,\n",
    "    num_epochs: int = 100,\n",
    "    input_dim: int = 1024,\n",
    "    hidden_dim: int = 64,\n",
    "):\n",
    "    dataset = ReviewDataset(filename, label=1)\n",
    "    processor = ReviewVectorizer(dataset, input_dim)\n",
    "    model = Classifier(input_dim, hidden_dim, len(processor.l2i))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=processor,\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for bx, by in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(bx)\n",
    "            loss = F.cross_entropy(output, by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(data_loader):.4f}\")\n",
    "    \n",
    "    return processor, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Feedback:The comments for the training loop are missing.\n",
    "\n",
    "###### solution :Have completed this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🎈 Task 1.10: Training the classifier\n",
    "\n",
    "Adapt the next cell to train the classifier for the two prediction tasks. Based on the loss values, which task appears to be the harder one? What is the purpose of setting a seed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6932\n",
      "Epoch 2, Loss: 0.6878\n",
      "Epoch 3, Loss: 0.6784\n",
      "Epoch 4, Loss: 0.6719\n",
      "Epoch 5, Loss: 0.6615\n",
      "Epoch 6, Loss: 0.6488\n",
      "Epoch 7, Loss: 0.6327\n",
      "Epoch 8, Loss: 0.6122\n",
      "Epoch 9, Loss: 0.5928\n",
      "Epoch 10, Loss: 0.5724\n",
      "Epoch 11, Loss: 0.5532\n",
      "Epoch 12, Loss: 0.5334\n",
      "Epoch 13, Loss: 0.5134\n",
      "Epoch 14, Loss: 0.4928\n",
      "Epoch 15, Loss: 0.4726\n",
      "Epoch 16, Loss: 0.4564\n",
      "Epoch 17, Loss: 0.4424\n",
      "Epoch 18, Loss: 0.4286\n",
      "Epoch 19, Loss: 0.4135\n",
      "Epoch 20, Loss: 0.4016\n",
      "Epoch 21, Loss: 0.3921\n",
      "Epoch 22, Loss: 0.3804\n",
      "Epoch 23, Loss: 0.3690\n",
      "Epoch 24, Loss: 0.3612\n",
      "Epoch 25, Loss: 0.3550\n",
      "Epoch 26, Loss: 0.3437\n",
      "Epoch 27, Loss: 0.3353\n",
      "Epoch 28, Loss: 0.3294\n",
      "Epoch 29, Loss: 0.3198\n",
      "Epoch 30, Loss: 0.3149\n",
      "Epoch 31, Loss: 0.3082\n",
      "Epoch 32, Loss: 0.3022\n",
      "Epoch 33, Loss: 0.2955\n",
      "Epoch 34, Loss: 0.2879\n",
      "Epoch 35, Loss: 0.2832\n",
      "Epoch 36, Loss: 0.2774\n",
      "Epoch 37, Loss: 0.2736\n",
      "Epoch 38, Loss: 0.2698\n",
      "Epoch 39, Loss: 0.2643\n",
      "Epoch 40, Loss: 0.2604\n",
      "Epoch 41, Loss: 0.2550\n",
      "Epoch 42, Loss: 0.2483\n",
      "Epoch 43, Loss: 0.2444\n",
      "Epoch 44, Loss: 0.2418\n",
      "Epoch 45, Loss: 0.2375\n",
      "Epoch 46, Loss: 0.2322\n",
      "Epoch 47, Loss: 0.2292\n",
      "Epoch 48, Loss: 0.2255\n",
      "Epoch 49, Loss: 0.2259\n",
      "Epoch 50, Loss: 0.2181\n",
      "Epoch 51, Loss: 0.2165\n",
      "Epoch 52, Loss: 0.2111\n",
      "Epoch 53, Loss: 0.2089\n",
      "Epoch 54, Loss: 0.2068\n",
      "Epoch 55, Loss: 0.2034\n",
      "Epoch 56, Loss: 0.2013\n",
      "Epoch 57, Loss: 0.1984\n",
      "Epoch 58, Loss: 0.1930\n",
      "Epoch 59, Loss: 0.1900\n",
      "Epoch 60, Loss: 0.1903\n",
      "Epoch 61, Loss: 0.1862\n",
      "Epoch 62, Loss: 0.1858\n",
      "Epoch 63, Loss: 0.1799\n",
      "Epoch 64, Loss: 0.1819\n",
      "Epoch 65, Loss: 0.1752\n",
      "Epoch 66, Loss: 0.1729\n",
      "Epoch 67, Loss: 0.1703\n",
      "Epoch 68, Loss: 0.1685\n",
      "Epoch 69, Loss: 0.1679\n",
      "Epoch 70, Loss: 0.1637\n",
      "Epoch 71, Loss: 0.1608\n",
      "Epoch 72, Loss: 0.1603\n",
      "Epoch 73, Loss: 0.1578\n",
      "Epoch 74, Loss: 0.1557\n",
      "Epoch 75, Loss: 0.1535\n",
      "Epoch 76, Loss: 0.1503\n",
      "Epoch 77, Loss: 0.1502\n",
      "Epoch 78, Loss: 0.1479\n",
      "Epoch 79, Loss: 0.1453\n",
      "Epoch 80, Loss: 0.1439\n",
      "Epoch 81, Loss: 0.1409\n",
      "Epoch 82, Loss: 0.1392\n",
      "Epoch 83, Loss: 0.1380\n",
      "Epoch 84, Loss: 0.1359\n",
      "Epoch 85, Loss: 0.1354\n",
      "Epoch 86, Loss: 0.1334\n",
      "Epoch 87, Loss: 0.1305\n",
      "Epoch 88, Loss: 0.1315\n",
      "Epoch 89, Loss: 0.1301\n",
      "Epoch 90, Loss: 0.1269\n",
      "Epoch 91, Loss: 0.1244\n",
      "Epoch 92, Loss: 0.1221\n",
      "Epoch 93, Loss: 0.1222\n",
      "Epoch 94, Loss: 0.1204\n",
      "Epoch 95, Loss: 0.1180\n",
      "Epoch 96, Loss: 0.1169\n",
      "Epoch 97, Loss: 0.1146\n",
      "Epoch 98, Loss: 0.1144\n",
      "Epoch 99, Loss: 0.1146\n",
      "Epoch 100, Loss: 0.1112\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "vectorizer, model = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.10 Solution\n",
    "Based on the loss progression and task characteristics, sentiment classification appears to be the harder task in this setup. Here's the analysis:\n",
    " - Topic: Final loss = 0.223 (started at 0.6838)(label_column=0)\n",
    " - Sentiment: Final loss = 0.572 (started at 0.6932)(label_column=1)\n",
    " - The sentiment classifier retains ~2.5× higher loss after 10 epochs\n",
    " \n",
    "Setting the seed with torch.manual_seed(42) ensures that the random number generator in PyTorch produces the same sequence of random numbers every time the code is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have trained the classifier on two separate prediction tasks, it is interesting to inspect and compare the embedding vectors it learned in the process. For this you will use an online tool called the [Embedding Projector](http://projector.tensorflow.org). The next cell contains code to save the embeddings from a trained classifier in a format that can be loaded into this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(\n",
    "    vectorizer: ReviewVectorizer,\n",
    "    model: Classifier,\n",
    "    vectors_filename: str,\n",
    "    metadata_filename: str,\n",
    "):\n",
    "    i2t = {i: t for t, i in vectorizer.t2i.items()}\n",
    "    embeddings = model.embedding.weight.detach().numpy()\n",
    "    items = [(i2t[i], e) for i, e in enumerate(embeddings)]\n",
    "    with open(vectors_filename, \"wt\") as f1, open(metadata_filename, \"wt\") as f2:\n",
    "        for w, e in items:\n",
    "            print(\"\\t\".join(\"{:.5f}\".format(x) for x in e), file=f1)\n",
    "            print(w, file=f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call this code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(vectorizer, model, \"vectors.tsv\", \"metadata.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(vectorizer, model, \"vectors(label1feb15).tsv\", \"metadata(label1feb15).tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🎓 Task 1.11: Inspecting the embeddings\n",
    "\n",
    "Load the embeddings from the two classification tasks (topic classification and sentiment classification) into the Embedding Projector web app and inspect the vector spaces. How do they compare visually? Does the visualisation make sense to you?\n",
    "\n",
    "The Embedding Projector offers visualisations based on three dimensionality reduction methods: [UMAP](https://umap-learn.readthedocs.io/en/latest/), [T-SNE](https://en.m.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding), and [PCA](https://en.m.wikipedia.org/wiki/Principal_component_analysis). Which of these seems most useful to you?\n",
    "\n",
    "Focus on the embeddings for the words *repair* and *sturdy*. Are they close to each other or far away from another? What happens if you switch to the other task? How do you explain that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.11\n",
    "\n",
    "Both classifiers produced meaningful results, displaying a well-distributed set of words across the three vector spaces. Words with high cosine similarity were clustered together and exhibited shorter Euclidean distances.\n",
    "\n",
    "For PCA, we observed that it provides a more linear projection, drawing words with similarities closer together or placing them in the same vector space. UMAP and T-SNE, on the other hand, offered better clustering compared to PCA. While PCA helps in understanding the overall variance in the dataset and provides a global view of the data structure, UMAP and T-SNE excel at preserving local relationships and forming tighter clusters.\n",
    "\n",
    "In topic classification, the embeddings of \"repair\" and \"sturdy\" were positioned closer together because they are contextually related in product reviews, often appearing in discussions about durability and maintenance.\n",
    "\n",
    "In sentiment classification, however, \"repair\" and \"sturdy\" were placed farther apart since they represent different sentiment polarities. \"Repair\" is more likely associated with negative or neutral sentiments, while \"sturdy\" is often linked to positive sentiment, leading to their separation in the vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of embedding layers\n",
    "\n",
    "The error surfaces explored when training neural networks can be very complex. Because of this, it is crucial to choose “good” initial values for the parameters. In the final task of this lab, you will run a small experiment to see how alternative initialisations can affect a model’s performance.\n",
    "\n",
    "In PyTorch, the weights of the embedding layer are initially set by sampling from the standard normal distribution, $\\mathcal{N}(0, 1)$. However, research suggests other approaches may work better. For example, you have seen that embedding layers share similarities with linear layers, so it makes sense to use the same initialisation method for both. The default initialisation method for linear layers in PyTorch is the so-called Kaiming initialisation, introduced by [He et al. (2015)](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🎈 Task 1.12: Initialisation of embedding layers\n",
    "\n",
    "Check the [source code of `nn.Linear`](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear) to see how PyTorch initialises the weights of linear layers using the Kaiming initialisation method. Apply the same method to the embedding layer of your classifier and see how this affects the loss of your model and the vector spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "import math\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "       \n",
    "        self.linear = nn.Linear(embedding_dim, num_classes)\n",
    "          # Apply Kaiming initialization to the embedding layer weights\n",
    "        init.kaiming_uniform_(self.embedding.weight, a=math.sqrt(5))\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(self.embedding(x).mean(dim=-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Initialization:\n",
      "Epoch 1, Loss: 0.6934\n",
      "Epoch 2, Loss: 0.6821\n",
      "Epoch 3, Loss: 0.6622\n",
      "Epoch 4, Loss: 0.6278\n",
      "Epoch 5, Loss: 0.5816\n",
      "Epoch 6, Loss: 0.5352\n",
      "Epoch 7, Loss: 0.4911\n",
      "Epoch 8, Loss: 0.4530\n",
      "Epoch 9, Loss: 0.4237\n",
      "Epoch 10, Loss: 0.3966\n",
      "Epoch 11, Loss: 0.3732\n",
      "Epoch 12, Loss: 0.3535\n",
      "Epoch 13, Loss: 0.3347\n",
      "Epoch 14, Loss: 0.3225\n",
      "Epoch 15, Loss: 0.3102\n",
      "Epoch 16, Loss: 0.2939\n",
      "Epoch 17, Loss: 0.2811\n",
      "Epoch 18, Loss: 0.2727\n",
      "Epoch 19, Loss: 0.2606\n",
      "Epoch 20, Loss: 0.2526\n",
      "Epoch 21, Loss: 0.2447\n",
      "Epoch 22, Loss: 0.2391\n",
      "Epoch 23, Loss: 0.2296\n",
      "Epoch 24, Loss: 0.2219\n",
      "Epoch 25, Loss: 0.2146\n",
      "Epoch 26, Loss: 0.2102\n",
      "Epoch 27, Loss: 0.2026\n",
      "Epoch 28, Loss: 0.1961\n",
      "Epoch 29, Loss: 0.1907\n",
      "Epoch 30, Loss: 0.1843\n",
      "Epoch 31, Loss: 0.1795\n",
      "Epoch 32, Loss: 0.1760\n",
      "Epoch 33, Loss: 0.1737\n",
      "Epoch 34, Loss: 0.1686\n",
      "Epoch 35, Loss: 0.1608\n",
      "Epoch 36, Loss: 0.1579\n",
      "Epoch 37, Loss: 0.1530\n",
      "Epoch 38, Loss: 0.1505\n",
      "Epoch 39, Loss: 0.1449\n",
      "Epoch 40, Loss: 0.1455\n",
      "Epoch 41, Loss: 0.1397\n",
      "Epoch 42, Loss: 0.1360\n",
      "Epoch 43, Loss: 0.1311\n",
      "Epoch 44, Loss: 0.1279\n",
      "Epoch 45, Loss: 0.1256\n",
      "Epoch 46, Loss: 0.1224\n",
      "Epoch 47, Loss: 0.1202\n",
      "Epoch 48, Loss: 0.1175\n",
      "Epoch 49, Loss: 0.1129\n",
      "Epoch 50, Loss: 0.1112\n",
      "Epoch 51, Loss: 0.1098\n",
      "Epoch 52, Loss: 0.1061\n",
      "Epoch 53, Loss: 0.1035\n",
      "Epoch 54, Loss: 0.0990\n",
      "Epoch 55, Loss: 0.1026\n",
      "Epoch 56, Loss: 0.0968\n",
      "Epoch 57, Loss: 0.0941\n",
      "Epoch 58, Loss: 0.0942\n",
      "Epoch 59, Loss: 0.0907\n",
      "Epoch 60, Loss: 0.0886\n",
      "Epoch 61, Loss: 0.0858\n",
      "Epoch 62, Loss: 0.0867\n",
      "Epoch 63, Loss: 0.0822\n",
      "Epoch 64, Loss: 0.0807\n",
      "Epoch 65, Loss: 0.0803\n",
      "Epoch 66, Loss: 0.0760\n",
      "Epoch 67, Loss: 0.0750\n",
      "Epoch 68, Loss: 0.0740\n",
      "Epoch 69, Loss: 0.0742\n",
      "Epoch 70, Loss: 0.0709\n",
      "Epoch 71, Loss: 0.0685\n",
      "Epoch 72, Loss: 0.0680\n",
      "Epoch 73, Loss: 0.0661\n",
      "Epoch 74, Loss: 0.0649\n",
      "Epoch 75, Loss: 0.0638\n",
      "Epoch 76, Loss: 0.0630\n",
      "Epoch 77, Loss: 0.0607\n",
      "Epoch 78, Loss: 0.0602\n",
      "Epoch 79, Loss: 0.0577\n",
      "Epoch 80, Loss: 0.0573\n",
      "Epoch 81, Loss: 0.0553\n",
      "Epoch 82, Loss: 0.0564\n",
      "Epoch 83, Loss: 0.0533\n",
      "Epoch 84, Loss: 0.0515\n",
      "Epoch 85, Loss: 0.0512\n",
      "Epoch 86, Loss: 0.0481\n",
      "Epoch 87, Loss: 0.0487\n",
      "Epoch 88, Loss: 0.0463\n",
      "Epoch 89, Loss: 0.0479\n",
      "Epoch 90, Loss: 0.0471\n",
      "Epoch 91, Loss: 0.0455\n",
      "Epoch 92, Loss: 0.0442\n",
      "Epoch 93, Loss: 0.0426\n",
      "Epoch 94, Loss: 0.0443\n",
      "Epoch 95, Loss: 0.0400\n",
      "Epoch 96, Loss: 0.0395\n",
      "Epoch 97, Loss: 0.0383\n",
      "Epoch 98, Loss: 0.0386\n",
      "Epoch 99, Loss: 0.0369\n",
      "Epoch 100, Loss: 0.0358\n",
      "\n",
      "Kaiming Initialization:\n",
      "Epoch 1, Loss: 0.6909\n",
      "Epoch 2, Loss: 0.6797\n",
      "Epoch 3, Loss: 0.6542\n",
      "Epoch 4, Loss: 0.6116\n",
      "Epoch 5, Loss: 0.5591\n",
      "Epoch 6, Loss: 0.5093\n",
      "Epoch 7, Loss: 0.4702\n",
      "Epoch 8, Loss: 0.4345\n",
      "Epoch 9, Loss: 0.4058\n",
      "Epoch 10, Loss: 0.3781\n",
      "Epoch 11, Loss: 0.3572\n",
      "Epoch 12, Loss: 0.3387\n",
      "Epoch 13, Loss: 0.3229\n",
      "Epoch 14, Loss: 0.3079\n",
      "Epoch 15, Loss: 0.2968\n",
      "Epoch 16, Loss: 0.2811\n",
      "Epoch 17, Loss: 0.2723\n",
      "Epoch 18, Loss: 0.2609\n",
      "Epoch 19, Loss: 0.2535\n",
      "Epoch 20, Loss: 0.2443\n",
      "Epoch 21, Loss: 0.2336\n",
      "Epoch 22, Loss: 0.2252\n",
      "Epoch 23, Loss: 0.2195\n",
      "Epoch 24, Loss: 0.2147\n",
      "Epoch 25, Loss: 0.2062\n",
      "Epoch 26, Loss: 0.2010\n",
      "Epoch 27, Loss: 0.1937\n",
      "Epoch 28, Loss: 0.1869\n",
      "Epoch 29, Loss: 0.1812\n",
      "Epoch 30, Loss: 0.1787\n",
      "Epoch 31, Loss: 0.1725\n",
      "Epoch 32, Loss: 0.1657\n",
      "Epoch 33, Loss: 0.1603\n",
      "Epoch 34, Loss: 0.1585\n",
      "Epoch 35, Loss: 0.1553\n",
      "Epoch 36, Loss: 0.1491\n",
      "Epoch 37, Loss: 0.1482\n",
      "Epoch 38, Loss: 0.1418\n",
      "Epoch 39, Loss: 0.1386\n",
      "Epoch 40, Loss: 0.1331\n",
      "Epoch 41, Loss: 0.1306\n",
      "Epoch 42, Loss: 0.1288\n",
      "Epoch 43, Loss: 0.1229\n",
      "Epoch 44, Loss: 0.1208\n",
      "Epoch 45, Loss: 0.1176\n",
      "Epoch 46, Loss: 0.1141\n",
      "Epoch 47, Loss: 0.1098\n",
      "Epoch 48, Loss: 0.1092\n",
      "Epoch 49, Loss: 0.1061\n",
      "Epoch 50, Loss: 0.1053\n",
      "Epoch 51, Loss: 0.1005\n",
      "Epoch 52, Loss: 0.0974\n",
      "Epoch 53, Loss: 0.0965\n",
      "Epoch 54, Loss: 0.0925\n",
      "Epoch 55, Loss: 0.0909\n",
      "Epoch 56, Loss: 0.0901\n",
      "Epoch 57, Loss: 0.0845\n",
      "Epoch 58, Loss: 0.0853\n",
      "Epoch 59, Loss: 0.0834\n",
      "Epoch 60, Loss: 0.0811\n",
      "Epoch 61, Loss: 0.0789\n",
      "Epoch 62, Loss: 0.0782\n",
      "Epoch 63, Loss: 0.0755\n",
      "Epoch 64, Loss: 0.0723\n",
      "Epoch 65, Loss: 0.0706\n",
      "Epoch 66, Loss: 0.0684\n",
      "Epoch 67, Loss: 0.0678\n",
      "Epoch 68, Loss: 0.0669\n",
      "Epoch 69, Loss: 0.0644\n",
      "Epoch 70, Loss: 0.0615\n",
      "Epoch 71, Loss: 0.0626\n",
      "Epoch 72, Loss: 0.0620\n",
      "Epoch 73, Loss: 0.0577\n",
      "Epoch 74, Loss: 0.0590\n",
      "Epoch 75, Loss: 0.0581\n",
      "Epoch 76, Loss: 0.0551\n",
      "Epoch 77, Loss: 0.0539\n",
      "Epoch 78, Loss: 0.0526\n",
      "Epoch 79, Loss: 0.0509\n",
      "Epoch 80, Loss: 0.0507\n",
      "Epoch 81, Loss: 0.0498\n",
      "Epoch 82, Loss: 0.0483\n",
      "Epoch 83, Loss: 0.0459\n",
      "Epoch 84, Loss: 0.0444\n",
      "Epoch 85, Loss: 0.0441\n",
      "Epoch 86, Loss: 0.0433\n",
      "Epoch 87, Loss: 0.0414\n",
      "Epoch 88, Loss: 0.0410\n",
      "Epoch 89, Loss: 0.0411\n",
      "Epoch 90, Loss: 0.0392\n",
      "Epoch 91, Loss: 0.0393\n",
      "Epoch 92, Loss: 0.0376\n",
      "Epoch 93, Loss: 0.0365\n",
      "Epoch 94, Loss: 0.0368\n",
      "Epoch 95, Loss: 0.0348\n",
      "Epoch 96, Loss: 0.0346\n",
      "Epoch 97, Loss: 0.0324\n",
      "Epoch 98, Loss: 0.0327\n",
      "Epoch 99, Loss: 0.0322\n",
      "Epoch 100, Loss: 0.0312\n"
     ]
    }
   ],
   "source": [
    "# Compare training with default vs. Kaiming initialization\n",
    "print(\"Default Initialization:\")\n",
    "_, _ = train()\n",
    "\n",
    "print(\"\\nKaiming Initialization:\")\n",
    "_, _ = train()  # Now uses Kaiming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 Solution\n",
    "\n",
    "Comparing the loss values between Default and Kaiming initialization, we see that both methods follow a similar downward trend, but Kaiming initialization consistently results in slightly lower loss values after each epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🥳 Congratulations on finishing lab&nbsp;1!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
