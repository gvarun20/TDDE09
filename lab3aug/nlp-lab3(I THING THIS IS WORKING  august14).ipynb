{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Pretraining a GPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is about pretraining large language models. You will work through the full pretraining process for a GPT model, explore different settings, and implement optimisations that make training more efficient. You will also reflect on the impact of data curation on the quality of the pretrained model. By the end of the lab, you will have a solid understanding of how large language models are trained from scratch.\n",
    "\n",
    "*Tasks you can choose for the oral exam are marked with the graduation cap üéì emoji.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gpt2 import Config, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pretraining pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT pretraining pipeline builds on the basic training loop for neural language models you have seen before, but includes several enhancements that improve stability and efficiency when training large models:\n",
    "\n",
    "* It uses the [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) optimiser with weight decay instead of vanilla stochastic gradient descent.\n",
    "* It implements a cosine decay learning rate schedule with a linear warmup phase.\n",
    "* It accumulates gradient updates across multiple batches to allow training with larger effective batch sizes.\n",
    "* It uses gradient clipping to prevent exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by setting up a configuration object that defines the key parameters of the training process. The original 124M-parameter GPT-2 model was trained on WebText, a private dataset with 300B tokens of Internet data. Our training pipeline is configured to train a Chinchilla-optimal version of the same model using the [FineWeb-Edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) dataset and a single A100 GPU with 80GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    device: torch.device = torch.device(\"cuda\")\n",
    "    shard_dir: str = \"data\"\n",
    "    \n",
    "    # Training steps and data processing\n",
    "    n_steps: int = 4768 # The total amount of optimization steps during training\n",
    "    n_tokens_per_step: int = 524288 # The number of tokens in a single step\n",
    "    batch_size: int = 16 #64 # The size of each batch which is training examples prossessed before parameters are updated.\n",
    "    sequence_len: int = 1024 # The number of input tokens in the language model.\n",
    "    n_vocab: int = 50304 # The number of unique tokens.\n",
    "\n",
    "    # Optimisation and learning rate scheduling\n",
    "    weight_decay: float = 0.1 # The factor variable for weight decay regularization.\n",
    "    max_lr: float = 6e-4 # The maximum learning rate\n",
    "    min_lr: float = 6e-5 # The minimum learning rate\n",
    "    n_warmup_steps: int = 715 # Number of steps it takes to reach the first max learning rate before cosine decay on learning rate\n",
    "    n_decay_steps: int = 4053 # The amount of steps during the cosine decay on learning rate\n",
    "    betas: tuple[float, float] = (0.9, 0.95) # Beta coefficients for the adamw optimizer\n",
    "    clip_norm: float = 1.0 # The maximum length of the norm of a vector with gradients before the gradient starts to be scaled down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sequence_len = 1024\n",
    "- batch_size = 64 (micro-batch)\n",
    "- n_tokens_per_step = 524288 (macro-batch)\n",
    "\n",
    "So:\n",
    "- Each micro-batch processes:  \n",
    "  64  sequences * 1024 tokens per sequence = 65,536  tokens\n",
    "- To reach macro-batch (n_tokens_per_step = 524288), we need to accumulate gradients from:  \n",
    "  524,288 / 65,536 = 8 micro-batches.\n",
    "- Only after these 8 micro-batches (gradient accumulation steps) do you perform a single optimizer step (parameter update)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéì Task 3.01: Explaining the training parameters\n",
    "\n",
    "Your first task is to explain the purpose of the training parameters. Some of them will already be known to you from the lectures, while others will become clear first as you progress through the lab and see how everything fits together. Because of this, it is best to revisit and complete this task towards the end of the lab when you have a full understanding of the training process.\n",
    "\n",
    "One parameter to note is the vocabulary size (`n_vocab`). The GPT-2 tokeniser has a default vocabulary size of 50,257, but for training on a GPU, it is helpful to use numbers that are more hardware-friendly. Specifically, numbers with many factors of&nbsp;2 can lead to more efficient computation. To achieve this, we set the vocabulary size to 50,304, which is slightly larger than needed but has many factors of&nbsp;2. Note that the extra tokens will not be used in practice ‚Äî they simply act as placeholders without meaningful embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  FEEDBACK:Could you also explain what the sequence_len implies and elaborate more on the weight_decay?\n",
    "###### SOLUTION:THIS HAS BEEN UPDATED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up the GPT model. Our goal is to match the original training setup of GPT-2 as closely as possible. To do this, we follow the initialisation strategy outlined in the key research papers about GPT ([Radford et al., 2018](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf); [Radford et al., 2019](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf); [Brown et al., 2020](https://arxiv.org/pdf/2005.14165)) as well as the official implementation ([link](https://github.com/openai/gpt-2)). Here is a summary of this strategy:\n",
    "\n",
    "* Token embeddings ‚Üí Normal distribution with mean $0$ and variance $0.02$.\n",
    "* Position embeddings ‚Üí Normal distribution with mean $0$ and variance $0.01$.\n",
    "* Weights of the linear layers ‚Üí Normal distribution with mean $0$ and variance $0.02$.\n",
    "* Biases of the linear layers ‚Üí Initialised to zeros.\n",
    "* Weight sharing between the final linear layer and the token embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéà Task 3.02: Initialising the model\n",
    "\n",
    "Expand the skeleton code below to create a fresh model and initialise it according to the GPT-2 strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1049908/2671198488.py:8: FutureWarning: `nn.init.normal` is now deprecated in favor of `nn.init.normal_`.\n",
      "  nn.init.normal(mod_dict[param], mean=0, std=math.sqrt(0.02))\n",
      "/tmp/ipykernel_1049908/2671198488.py:10: FutureWarning: `nn.init.normal` is now deprecated in favor of `nn.init.normal_`.\n",
      "  nn.init.normal(mod_dict[param], mean=0, std=math.sqrt(0.01))\n",
      "/tmp/ipykernel_1049908/2671198488.py:15: FutureWarning: `nn.init.normal` is now deprecated in favor of `nn.init.normal_`.\n",
      "  nn.init.normal(mod_dict[param], mean=0, std=math.sqrt(0.02))\n",
      "/tmp/ipykernel_1049908/2671198488.py:13: FutureWarning: `nn.init.normal` is now deprecated in favor of `nn.init.normal_`.\n",
      "  nn.init.normal(mod_dict[param], mean=0, std=math.sqrt(0.02) * (1/math.sqrt(model.config.n_layer * 2))) # Factor for residual intitialization\n"
     ]
    }
   ],
   "source": [
    "def configure_model(config: TrainingConfig) -> Model:\n",
    "    # TODO: Replace the following line with your own code\n",
    "    model = Model(Config(n_vocab=config.n_vocab))\n",
    "    mod_dict = model.state_dict()\n",
    "\n",
    "    for param in mod_dict.keys():\n",
    "        if param.startswith(\"wte\"):\n",
    "            nn.init.normal(mod_dict[param], mean=0, std=math.sqrt(0.02))\n",
    "        elif param.startswith(\"wpe\"):\n",
    "            nn.init.normal(mod_dict[param], mean=0, std=math.sqrt(0.01))\n",
    "        elif param.endswith(\"c_proj.weight\"):\n",
    "            # task 3.03 step 3\n",
    "            nn.init.normal(mod_dict[param], mean=0, std=math.sqrt(0.02) * (1/math.sqrt(model.config.n_layer * 2))) # Factor for residual intitialization\n",
    "        elif param.endswith(\"weight\"):\n",
    "            nn.init.normal(mod_dict[param], mean=0, std=math.sqrt(0.02))\n",
    "        elif param.endswith(\"bias\"):\n",
    "            nn.init.zeros_(mod_dict[param])\n",
    "    model.lm_head.weight = model.wte.weight\n",
    "    return model\n",
    "model = configure_model(TrainingConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled residual initialisation\n",
    "\n",
    "There is one important detail in the GPT-2 initialisation strategy that we have not addressed yet. [Radford et al. (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) write (Section&nbsp;2.3):\n",
    "\n",
    "> A modified initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of $1/\\sqrt{N}$ where $N$ is the number of residual layers.\n",
    "\n",
    "Why is this necessary? One of the challenges in training large language models is controlling the variance of activations. However, there are two points in the GPT architecture where variance can grow uncontrollably: the residual connections after the multi-head attention and the MLP. Since these connections simply add activations from previous layers, their variances increases with depth. To see this, note that if we sum $N$ independent normally distributed variables with variance $\\sigma^2$, the result has variance $N \\sigma^2$. The factor $1/\\sqrt{N}$ compensates for this growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéì Task 3.03: Implementing the scaled residual initialisation\n",
    "\n",
    "**Step&nbsp;1.** Suppose each summand in a sum of $N$ independent normally distributed variables with variance $\\sigma^2$ is scaled by a factor of&nbsp;$k$. The total variance then becomes $N k^2 \\sigma^2$. What happens to the total variance if we choose $k = 1/\\sqrt{N}$, as in GPT-2?\n",
    "\n",
    "##### SOLUTION FO STEP 1:\n",
    "\n",
    "If $k = 1/\\sqrt{N}$, the total variance stays the same as a single summand‚Äôs variance.\n",
    "\n",
    "This is because Total variance = $ N (1/\\sqrt{N})^2 \\sigma^2 = N (1/{N}) \\sigma^2 = \\sigma^2$\n",
    "\n",
    "This means that by $1/\\sqrt{N}$, we ensure that when we sum $N$ ‚Äúresidual paths,‚Äù we don't keep increasing the net variance layer after layer which stabilizes the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step&nbsp;2.** Test the mathematical theory with a simulation. Generate normally distributed activations using `torch.randn()`. Sum the activations across $N$ hypothetical residual layers, first without scaling and then with the $1/\\sqrt{N}$ adjustment. Compare the two cases by producing a plot showing the variance at each layer. To compute the variance of a tensor, use `torch.var()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_layers=6\n",
    "num_samples=10000\n",
    "sums_no_scale = torch.zeros(num_samples)\n",
    "sums_scaled   = torch.zeros(num_samples)\n",
    "vars_no_scale = []\n",
    "vars_scaled   = []\n",
    "\n",
    "for i in range(1, num_layers + 1):\n",
    "    new_activations = torch.randn(num_samples)\n",
    "    \n",
    "    # 1 NO scaling\n",
    "    sums_no_scale += new_activations\n",
    "    vars_no_scale.append(sums_no_scale.var())\n",
    "    \n",
    "    # 2 with 1/sqrt(N) scaling\n",
    "    # scale each new activation by 1/sqrt(num_layers),\n",
    "    new_activations_scaled = new_activations / (num_layers**0.5)\n",
    "    sums_scaled += new_activations_scaled\n",
    "    vars_scaled.append(sums_scaled.var())\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(range(1, num_layers + 1), vars_no_scale, label=\"No scaling\")\n",
    "plt.plot(range(1, num_layers + 1), vars_scaled,   label=\"1/sqrt(N) scaling\")\n",
    "plt.xlabel(\"Layer index\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.title(\"Variance accumulation across layers\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step&nbsp;3.** Update your model initialisation from the previous task to include the scaled residual initialisation. The adjustment should only be applied to the linear layer at the end of the multi-head attention and MLP blocks (`c_proj`). Note that a GPT model with $L$&nbsp;layers has $N = 2 L$ residual layers, because there are two residual connections in each layer (after the multi-head attention and the MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, GPT-2 was pretrained on a 300B non-public dataset collected by OpenAI. Our pretraining data comes from the public [FineWeb-Edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) dataset. The full dataset contains 1.3&nbsp;trillion tokens, but here, we will only work with a small sample. We have preprocessed the data by tokenising it with the GPT-2 tokeniser and storing the token indices in equal-sized NumPy arrays, which we call **shards**. The function below loads these shards from a given directory and yields them as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shards(shard_dir: str):\n",
    "    for shard in sorted(os.listdir(shard_dir)):\n",
    "        yield torch.from_numpy(np.load(os.path.join(shard_dir, shard)).astype(np.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, our training data consists of 300M tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(s.numel() for s in shards(\"/courses/TDDE09/labs/lab3/data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéì Task 3.04: Batching the data\n",
    "\n",
    "Implement a function `make_batches()` that packages the token indices in the shards into pairs of input and output batches suitable for language modelling training. One efficient way to do this is sketched in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "tensor([[ 7,  8,  9],\n",
      "        [10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "shard = torch.tensor(range(15), dtype=torch.long)\n",
    "\n",
    "# Suppose we want to package the tokens in this shard into batches of shape (2, 3).\n",
    "\n",
    "# Step 1: Segment the shard into overlapping chunks of size 2 * 3 + 1\n",
    "chunk1 = shard[0:7]\n",
    "chunk2 = shard[6:13]\n",
    "excess = shard[12:]\n",
    "\n",
    "# Step 2: Create batches from all but the last and all but the first token in each chunk\n",
    "x1, y1 = chunk1[:-1].view(2, 3), chunk1[1:].view(2, 3)\n",
    "x2, y2 = chunk2[:-1].view(2, 3), chunk2[1:].view(2, 3)\n",
    "\n",
    "print(x1)\n",
    "print(y1)\n",
    "print(x2)\n",
    "print(y2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell shows the signature of `make_batches()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello!\n",
      "hello!\n",
      "Excess 0\n",
      "hello!\n",
      "Excess 0\n",
      "18310\n"
     ]
    }
   ],
   "source": [
    "def make_batches(config: TrainingConfig):\n",
    "    # TODO: Replace the following line with your own code\n",
    "    offset = 0\n",
    "    num_arg_batch = config.batch_size * config.sequence_len\n",
    "    excess = torch.tensor([],dtype=torch.long)\n",
    "    for s in shards(\"/courses/TDDE09/labs/lab3/data\"):\n",
    "        batch_nr = 0\n",
    "        print(\"hello!\")\n",
    "        while True:\n",
    "            if len(excess) > 0:\n",
    "                offset = len(excess)\n",
    "\n",
    "            chunk1_upper_limit = (batch_nr + 1) * num_arg_batch + 1 - offset\n",
    "            \n",
    "            if chunk1_upper_limit > len(s):\n",
    "                excess = s[batch_nr * num_arg_batch - offset:]\n",
    "                \n",
    "                break\n",
    "            else:\n",
    "                if len(excess) > 0:\n",
    "                    chunk1 = torch.concatenate((excess, s[batch_nr * num_arg_batch : chunk1_upper_limit]))\n",
    "                    excess = torch.tensor([],dtype=torch.long)\n",
    "                    print(\"Excess\", len(excess))\n",
    "                else:\n",
    "                    chunk1 = s[batch_nr * num_arg_batch - offset : chunk1_upper_limit]\n",
    "\n",
    "                x1, y1 = chunk1[:-1].view(config.batch_size, config.sequence_len), chunk1[1:].view(config.batch_size, config.sequence_len)\n",
    "                yield x1, y1\n",
    "                \n",
    "            batch_nr += 1\n",
    "\n",
    "config = TrainingConfig()\n",
    "count = 0\n",
    "for i in make_batches(config):\n",
    "    #print(i[1].shape)\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints and considerations:**\n",
    "\n",
    "* Batches can stretch across shards. You will have to carry over excess tokens at the end of shards to the next batch.\n",
    "* All batches should have the same shape. Drop excess tokens at the end of the last shard.\n",
    "* The correct number of batches of shape $(64, 1024)$ for the training shards is $4577$.\n",
    "\n",
    "#### solution for the task 3.04\n",
    "The reason for the differences in batches in our case is that used a batch size of 2 to get training to run. So if we divide 146484 with 32 we get 4577.625, were the excess would be not be included making the amount of batches 4577."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below configures the [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) optimiser. It uses weight decay on all parameters with two or more dimensions (e.g., weights in linear layers), and no decay on the remaining parameters. If the model is on a CUDA device, the code uses the ‚Äúfused‚Äù implementation of the optimiser for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(model: Model, config: TrainingConfig):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    decay_params = [p for p in params if p.dim() >= 2]\n",
    "    no_decay_params = [p for p in params if p.dim() < 2]\n",
    "    param_groups = [\n",
    "        {\"params\": decay_params, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": no_decay_params, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    return torch.optim.AdamW(\n",
    "        param_groups,\n",
    "        lr=config.max_lr,\n",
    "        betas=config.betas,\n",
    "        fused=(config.device.type == \"cuda\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéì Task 3.05: Exploring the beta parameters\n",
    "\n",
    "The AdamW optimiser is controlled by two hyperparameters $\\beta_1$ and $\\beta_2$:\n",
    "\n",
    "* $\\beta_1$ controls the moving average of past gradients.\n",
    "* $\\beta_2$ controls the moving average of past squared gradients, which affects the adaptive learning rate scaling.\n",
    "\n",
    "Lower values correspond to reduced effect of the respective average, that is, less smoothing and faster adaptation to recent values. In this task, you will explore how different beta values affect the convergence behaviour of the optimiser.\n",
    "\n",
    "The code below applies the optimiser to the function $f(x) = (x-2)^2 + (y+3)^2$, which has a global minimum at $(2, -3)$. The code yields the trajectories of the parameter values $(x, y)$ visited by Adam when started at $(-4, 5)$ for different values of&nbsp;$\\beta_1$ and $\\beta_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_trajectories(betas):\n",
    "    def loss_function(x, y):\n",
    "        return (x - 2) ** 2 + (y + 3) ** 2\n",
    "\n",
    "    for beta1, beta2 in betas:\n",
    "        x = torch.tensor([-4.0], requires_grad=True)\n",
    "        y = torch.tensor([5.0], requires_grad=True)\n",
    "        optimizer = torch.optim.AdamW((x, y), lr=0.125, betas=(beta1, beta2))\n",
    "        trajectory = []\n",
    "        for _ in range(100):\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_function(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            trajectory.append((x.item(), y.item()))\n",
    "        yield trajectory, f\"Œ≤‚ÇÅ = {beta1}, Œ≤‚ÇÇ = {beta2}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Your task is to visualise different trajectories in a plot and analyse the results. Proceed as follows:\n",
    "\n",
    "* Start by plotting the trajectory for the default values for $\\beta_1$ and $\\beta_2$. (Consult the PyTorch documentation to find these.)\n",
    "* Add the plot for the beta values used in our training configuration. For this simple example, do you see a significant difference?\n",
    "* Add more plots to see what happens if the beta values are too high or too low.\n",
    "\n",
    "### solution for 3.05:\n",
    "There is no significant difference between the two results that we got. However, with the training configuration, the trajectories overshooted a bit.\n",
    "We also did another test for fun with two much smaller values that overshooted by alot but also probably got the closes result to the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9, 0.95)\n",
      "[(-3.869999885559082, 4.868749618530273), (-3.740241765975952, 4.737722396850586), (-3.6107797622680664, 4.606956958770752), (-3.481668710708618, 4.4764933586120605), (-3.35296630859375, 4.346372604370117), (-3.2247302532196045, 4.216635704040527), (-3.0970211029052734, 4.087325096130371), (-2.9698984622955322, 3.9584827423095703), (-2.84342360496521, 3.830150842666626), (-2.717658042907715, 3.702371597290039), (-2.5926642417907715, 3.5751872062683105), (-2.4685051441192627, 3.4486403465270996), (-2.345243215560913, 3.322772979736328), (-2.2229416370391846, 3.197627067565918), (-2.101663112640381, 3.073244094848633), (-1.9814705848693848, 2.949664831161499), (-1.8624260425567627, 2.826930046081543), (-1.744591474533081, 2.7050795555114746), (-1.6280277967453003, 2.584153175354004), (-1.512795329093933, 2.464189052581787), (-1.3989530801773071, 2.3452248573303223), (-1.2865592241287231, 2.22729754447937), (-1.1756703853607178, 2.110442876815796), (-1.0663421154022217, 1.9946961402893066), (-0.9586281180381775, 1.8800910711288452), (-0.8525804877281189, 1.766660451889038), (-0.7482494711875916, 1.6544363498687744), (-0.6456832885742188, 1.5434491634368896), (-0.5449280738830566, 1.4337282180786133), (-0.446027934551239, 1.3253018856048584), (-0.3490244746208191, 1.218197226524353), (-0.25395703315734863, 1.1124399900436401), (-0.16086214780807495, 1.0080546140670776), (-0.06977403163909912, 0.9050643444061279), (0.01927594095468521, 0.8034909963607788), (0.10625909268856049, 0.7033553719520569), (0.1911497265100479, 0.6046766638755798), (0.27392494678497314, 0.5074729919433594), (0.3545648455619812, 0.41176095604896545), (0.43305253982543945, 0.3175559937953949), (0.5093742609024048, 0.22487211227416992), (0.5835191607475281, 0.13372215628623962), (0.6554795503616333, 0.044117532670497894), (0.7252508997917175, -0.0439315065741539), (0.7928317189216614, -0.1304161250591278), (0.858223557472229, -0.2153286635875702), (0.9214309453964233, -0.2986627519130707), (0.98246169090271, -0.38041335344314575), (1.0413262844085693, -0.46057653427124023), (1.0980384349822998, -0.5391496419906616), (1.1526143550872803, -0.6161313056945801), (1.205073356628418, -0.691521167755127), (1.2554373741149902, -0.7653203010559082), (1.3037307262420654, -0.8375307321548462), (1.3499805927276611, -0.908155620098114), (1.3942164182662964, -0.9771993160247803), (1.4364697933197021, -1.0446672439575195), (1.47677481174469, -1.1105659008026123), (1.515167474746704, -1.1749027967453003), (1.5516855716705322, -1.2376863956451416), (1.5863687992095947, -1.2989262342453003), (1.6192586421966553, -1.3586326837539673), (1.6503978967666626, -1.416817307472229), (1.6798310279846191, -1.4734925031661987), (1.7076034545898438, -1.528671383857727), (1.7337619066238403, -1.582368016242981), (1.7583539485931396, -1.6345973014831543), (1.7814279794692993, -1.6853749752044678), (1.8030329942703247, -1.7347172498703003), (1.8232187032699585, -1.7826414108276367), (1.8420352935791016, -1.8291652202606201), (1.8595329523086548, -1.8743071556091309), (1.8757622241973877, -1.918086290359497), (1.8907736539840698, -1.9605224132537842), (1.904617428779602, -2.001635789871216), (1.9173438549041748, -2.0414469242095947), (1.9290028810501099, -2.07997727394104), (1.9396437406539917, -2.11724853515625), (1.9493154287338257, -2.153282642364502), (1.9580659866333008, -2.1881020069122314), (1.9659432172775269, -2.2217297554016113), (1.9729937314987183, -2.2541887760162354), (1.9792633056640625, -2.2855026721954346), (1.9847968816757202, -2.315694808959961), (1.989638328552246, -2.3447892665863037), (1.9938305616378784, -2.372809886932373), (1.9974150657653809, -2.3997809886932373), (2.000432252883911, -2.4257266521453857), (2.0029215812683105, -2.450671434402466), (2.0049211978912354, -2.474639415740967), (2.006467342376709, -2.497655153274536), (2.0075957775115967, -2.5197432041168213), (2.00834059715271, -2.5409276485443115), (2.0087342262268066, -2.561232805252075), (2.0088083744049072, -2.5806827545166016), (2.0085928440093994, -2.599301815032959), (2.0081162452697754, -2.6171138286590576), (2.0074057579040527, -2.6341423988342285), (2.0064873695373535, -2.6504111289978027), (2.005385637283325, -2.6659433841705322)]\n",
      "[(-3.869999885559082, 4.868749618530273), (-3.740206718444824, 4.737696170806885), (-3.6106512546539307, 4.606860160827637), (-3.4813649654388428, 4.476264476776123), (-3.3523802757263184, 4.345931529998779), (-3.2237303256988525, 4.215884685516357), (-3.095449209213257, 4.086146831512451), (-2.96757173538208, 3.9567410945892334), (-2.8401331901550293, 3.827691078186035), (-2.713169574737549, 3.6990201473236084), (-2.5867178440093994, 3.570751905441284), (-2.460815191268921, 3.4429101943969727), (-2.3354995250701904, 3.3155181407928467), (-2.2108092308044434, 3.1885995864868164), (-2.0867834091186523, 3.0621776580810547), (-1.9634613990783691, 2.9362759590148926), (-1.840883493423462, 2.810917377471924), (-1.7190903425216675, 2.6861250400543213), (-1.59812331199646, 2.5619218349456787), (-1.4780241250991821, 2.4383301734924316), (-1.358835220336914, 2.3153727054595947), (-1.2405998706817627, 2.1930716037750244), (-1.1233619451522827, 2.071449041366577), (-1.0071659088134766, 1.950526475906372), (-0.8920572996139526, 1.8303261995315552), (-0.7780823707580566, 1.710869550704956), (-0.6652882695198059, 1.5921779870986938), (-0.5537232756614685, 1.4742728471755981), (-0.4434365928173065, 1.357175588607788), (-0.3344787061214447, 1.2409074306488037), (-0.22690130770206451, 1.125489592552185), (-0.12075735628604889, 1.0109432935714722), (-0.016101233661174774, 0.897290050983429), (0.08701121807098389, 0.7845513224601746), (0.1885225921869278, 0.6727486848831177), (0.2883739173412323, 0.5619041323661804), (0.38650453090667725, 0.4520397484302521), (0.48285210132598877, 0.3431779742240906), (0.5773525238037109, 0.2353416532278061), (0.6699399948120117, 0.12855401635169983), (0.7605469226837158, 0.02283880114555359), (0.8491040468215942, -0.08177968859672546), (0.935540497303009, -0.185276597738266), (1.019783854484558, -0.2876264154911041), (1.1017603874206543, -0.3888028860092163), (1.1813950538635254, -0.4887790083885193), (1.258612036705017, -0.5875269770622253), (1.3333349227905273, -0.6850180625915527), (1.405487060546875, -0.7812225818634033), (1.474991798400879, -0.8761099576950073), (1.5417734384536743, -0.9696484804153442), (1.6057575941085815, -1.0618054866790771), (1.6668717861175537, -1.152547001838684), (1.7250462770462036, -1.241837978363037), (1.7802149057388306, -1.3296422958374023), (1.8323160409927368, -1.4159224033355713), (1.8812931776046753, -1.500639796257019), (1.9270964860916138, -1.5837544202804565), (1.9696834087371826, -1.6652253866195679), (2.0090198516845703, -1.745010256767273), (2.04508113861084, -1.8230656385421753), (2.077853202819824, -1.8993468284606934), (2.1073334217071533, -1.9738084077835083), (2.133531093597412, -2.0464038848876953), (2.1564688682556152, -2.1170859336853027), (2.1761832237243652, -2.1858062744140625), (2.1927244663238525, -2.252516508102417), (2.20615816116333, -2.3171677589416504), (2.2165634632110596, -2.379711151123047), (2.2240350246429443, -2.4400980472564697), (2.2286815643310547, -2.4982805252075195), (2.230626344680786, -2.554211139678955), (2.2300057411193848, -2.6078436374664307), (2.2269692420959473, -2.6591339111328125), (2.2216784954071045, -2.7080395221710205), (2.214306116104126, -2.754521131515503), (2.2050344944000244, -2.798542022705078), (2.1940555572509766, -2.8400697708129883), (2.1815683841705322, -2.879075765609741), (2.167778253555298, -2.915536403656006), (2.152895450592041, -2.9494338035583496), (2.137133836746216, -2.980755567550659), (2.12070894241333, -3.0094966888427734), (2.1038365364074707, -3.035659074783325), (2.08673095703125, -3.0592525005340576), (2.0696029663085938, -3.0802948474884033), (2.0526585578918457, -3.0988128185272217), (2.0360968112945557, -3.114842176437378), (2.0201072692871094, -3.1284279823303223), (2.004868984222412, -3.139625072479248), (1.9905471801757812, -3.1484975814819336), (1.977292776107788, -3.1551196575164795), (1.96523916721344, -3.1595754623413086), (1.9545010328292847, -3.1619579792022705), (1.9451721906661987, -3.162369728088379), (1.9373247623443604, -3.160922050476074), (1.9310073852539062, -3.1577346324920654), (1.926244854927063, -3.152935028076172), (1.9230372905731201, -3.146657943725586), (1.9213603734970093, -3.1390445232391357)]\n",
      "[(-3.869999885559082, 4.868749618530273), (-3.739495277404785, 4.737160682678223), (-3.6085124015808105, 4.605258941650391), (-3.477196455001831, 4.473158836364746), (-3.3456838130950928, 4.340967178344727), (-3.214076519012451, 4.208762168884277), (-3.0824437141418457, 4.076597213745117), (-2.95082950592041, 3.9445064067840576), (-2.819260358810425, 3.8125107288360596), (-2.687751054763794, 3.680622100830078), (-2.5563080310821533, 3.548847198486328), (-2.4249329566955566, 3.417188882827759), (-2.2936248779296875, 3.2856483459472656), (-2.1623802185058594, 3.1542246341705322), (-2.031193733215332, 3.0229170322418213), (-1.9000592231750488, 2.891723871231079), (-1.7689697742462158, 2.7606430053710938), (-1.6379170417785645, 2.629672050476074), (-1.5068918466567993, 2.4988086223602295), (-1.375883936882019, 2.3680498600006104), (-1.2448816299438477, 2.2373926639556885), (-1.1138720512390137, 2.1068339347839355), (-0.9828405380249023, 1.9763704538345337), (-0.8517704010009766, 1.8459984064102173), (-0.7206426858901978, 1.7157138586044312), (-0.5894354581832886, 1.585512399673462), (-0.45812344551086426, 1.4553894996643066), (-0.326677143573761, 1.3253400325775146), (-0.19506190717220306, 1.1953585147857666), (-0.06323650479316711, 1.065438985824585), (0.06884858012199402, 0.935575008392334), (0.2012535035610199, 0.8057591915130615), (0.3340524435043335, 0.6759836673736572), (0.467338502407074, 0.5462396144866943), (0.6012305021286011, 0.41651731729507446), (0.7358837127685547, 0.2868058681488037), (0.8715067505836487, 0.1570931226015091), (1.0083891153335571, 0.02736535668373108), (1.1469500064849854, -0.10239293426275253), (1.287830114364624, -0.23219938576221466), (1.4320828914642334, -0.36207419633865356), (1.581629991531372, -0.49204060435295105), (1.7405879497528076, -0.6221256852149963), (1.920636534690857, -0.752360999584198), (2.1809825897216797, -0.8827838897705078), (2.201225996017456, -1.0134390592575073), (2.1453614234924316, -1.1443803310394287), (2.0478978157043457, -1.2756733894348145), (1.8912568092346191, -1.407400131225586), (1.9051238298416138, -1.5396639108657837), (1.9734348058700562, -1.6725980043411255), (2.0990467071533203, -1.8063786029815674), (2.057765483856201, -1.941245675086975), (1.9680622816085815, -2.077538013458252), (1.9455536603927612, -2.215757369995117), (2.0005910396575928, -2.356693983078003), (2.0866994857788086, -2.5017013549804688), (2.027343988418579, -2.6534078121185303), (1.9147406816482544, -2.818056344985962), (1.952791452407837, -3.0174334049224854), (2.039372682571411, -3.3087289333343506), (2.032916784286499, -3.2729651927948), (1.9671239852905273, -3.189979314804077), (1.9945812225341797, -3.072434902191162), (2.063648223876953, -2.898500442504883), (2.0021846294403076, -2.8789782524108887), (1.9000239372253418, -2.932142734527588), (1.9533061981201172, -3.0306708812713623), (2.0547900199890137, -3.077458143234253), (2.0404632091522217, -3.0212957859039307), (1.9702080488204956, -2.9118428230285645), (1.9817274808883667, -2.9541714191436768), (2.0474348068237305, -3.044621229171753), (1.9946022033691406, -3.0322282314300537), (1.9393476247787476, -2.963019847869873), (1.9956042766571045, -2.992227077484131), (2.098231792449951, -3.067748546600342), (2.040403366088867, -3.0066375732421875), (1.9323927164077759, -2.8969004154205322), (1.9553661346435547, -2.9474799633026123), (2.029759645462036, -3.0450074672698975), (2.025761604309082, -3.0445680618286133), (1.961459994316101, -2.9802958965301514), (2.001455307006836, -2.971365213394165), (2.0580008029937744, -3.029039144515991), (1.9960938692092896, -2.9924187660217285), (1.9155230522155762, -2.9885506629943848), (1.97018301486969, -3.0471439361572266), (2.0770132541656494, -2.9856350421905518), (2.037471055984497, -2.961737632751465), (1.9477728605270386, -3.017583131790161), (1.9727873802185059, -3.0136916637420654), (2.0491955280303955, -2.9508042335510254), (2.0073533058166504, -3.003544330596924), (1.920806884765625, -3.072234869003296), (1.9748609066009521, -3.010693073272705), (2.0820844173431396, -2.896389961242676), (2.0377285480499268, -2.9450135231018066), (1.9434127807617188, -3.040260076522827), (1.968719720840454, -3.047912836074829)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_config = TrainingConfig()\n",
    "print(training_config.betas)\n",
    "betas = training_config.betas\n",
    "a = torch.tensor(((0.9,0.999),betas,(0.5,0.1)))\n",
    "#print(a)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for a_t in adam_trajectories(betas=a):\n",
    "    print(a_t[0])\n",
    "    x,y = zip(*a_t[0])\n",
    "    plt.plot(x,y)\n",
    "    plt.plot(2,-3,\"o\",c=\"b\")\n",
    "    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to implement the learning rate scheduler. As mentioned before, GPT-2 is trained with cosine decay from a maximum to a minimum learning rate in conjunction with a linear warmup phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéì Task 3.06: Implementing cosine decay\n",
    "\n",
    "Implement a function `get_lr_factor()` that determines the learning for a given `step`. To work with the rest of the implementation, this function should return its result as a factor of the maximum learning rate. In particular, the result at the end of the linear warmup should be&nbsp;$1$. Validate your implementation by plotting the function against the training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGdCAYAAAAFcOm4AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAATAlJREFUeJzt3XlcVOX+B/DPLMwMAwwDAsOquCsioqCIZmZRXvPaXmbmQmlp1rXsVtotvbeNfi22aWqWW5u22aZZRmpqKAoq4ILiBqJssgz7wMz5/YEzSbmAMpw5M5/36zWvV555zsx3Tst8OvN9nkcmCIIAIiIiIgmTi10AERER0dVioCEiIiLJY6AhIiIiyWOgISIiIsljoCEiIiLJY6AhIiIiyWOgISIiIsljoCEiIiLJU4pdQEtYLBacPn0aXl5ekMlkYpdDRERELSAIAiorKxEcHAy53L73UCQRaE6fPo2wsDCxyyAiIqIrkJeXh9DQULu+hyQCjZeXF4CmC6LT6USuhoiIiFrCaDQiLCzM9j1uT5IINNafmXQ6HQMNERGRxLRHuwibgomIiEjyGGiIiIhI8hhoiIiISPIYaIiIiEjyGGiIiIhI8hhoiIiISPIYaIiIiEjyGGiIiIhI8hhoiIiISPJaHWh+//13jBkzBsHBwZDJZPj2228ve87mzZsxYMAAqNVqdOvWDStWrLiCUomIiIgurNWBprq6Gv369cPChQtbNP748eMYPXo0RowYgb179+Lxxx/HlClT8PPPP7e6WCIiIqILafVeTqNGjcKoUaNaPH7x4sXo3Lkz3nzzTQBA7969sW3bNrz11lsYOXJka9+eiIiI6G/svjllSkoKEhISmh0bOXIkHn/88YueU19fj/r6etufjUajvcqj86zLOIP03DKolHK4KeRQKWRQKeXw0rjBR+sGb3cVfDzc4KtVwc9TDbnc/puNERERtYTdA01BQQEMBkOzYwaDAUajEbW1tXB3d//bOUlJSfjf//5n79LoPKXVJjz2eTosQsvGuylkCPJ2R4jeHSE+7ujoq0X3AE90N3ghvIMWSgX7zYmIqP3YPdBciTlz5mDWrFm2PxuNRoSFhYlYkfPLzK+ARQD8PFW4NToEDWYLTI0W1DdaYKxtQHltA8prTCivaUBZjQkNZgG5pTXILa3522upFHJ08fdARLAO/cP0iA7zQa8gL7gx5BARkZ3YPdAEBgaisLCw2bHCwkLodLoL3p0BALVaDbVabe/S6DyZp8oBAEO6+uH5f0Zccmyj2YLCynrkl9Uiv7wG+WW1OF5SgyNFlThSWIXaBjMOFVTiUEElvknPBwColXJEhngjrrMvrunmhwGdfKBxU9j7YxERkYuwe6CJj4/H+vXrmx3buHEj4uPj7f3W1AqZ+RUAgL4h3pcdq1TIm35q0rsD8G32nMUiIL+8FocLK5FxqgJ78sqxL68cFbUNSDtZhrSTZXh/81GolXLEhvtgaDc/JPQ2oHuAJ2Qy9uQQEdGVaXWgqaqqQk5Oju3Px48fx969e+Hr64uOHTtizpw5yM/Px6pVqwAA06ZNw4IFC/D000/jgQcewG+//YYvvvgC69ata7tPQVct89S5QBN6+UBzKXK5DGG+WoT5anFD76beKUEQcLykGmkny5By9Cy2Hy1BobEe23POYnvOWby2IRvhHbS4McKAGyMCEdPJBwo2HBMRUSvIBEFoYRtok82bN2PEiBF/Oz5p0iSsWLECkydPxokTJ7B58+Zm5zzxxBM4cOAAQkND8fzzz2Py5Mktfk+j0Qhvb29UVFRAp9O1plxqgZKqesS+9CtkMiBj3k3w0rjZ9f0EQcDR4ipszzmLLYeLsS2nBKZGi+15P08V/hkVjNv6h6BfqDfv3BARSVR7fn+3OtCIgYHGvjZlFyFx+S508ffAb09e1+7vX13fiN8PF+OXA4X47VARKmobbM919vPArdHBuL1/CDp18Gj32oiI6Mq15/e3Q85yovZl/bkpqgX9M/bgoVZiVN8gjOobhAazBdtySvDtnnz8sr8Qx0uq8favR/D2r0dwTTc/3BfXETdGGDhjioiImmGgoT8bgkP14hYCwE0hx4ieARjRMwDV9Y345UABvknPx7acEtvD30uNsbFhuHdQGEJ9tGKXTEREDoCBhv5sCBbpDs3FeKiVuL1/KG7vH4q80hqs2ZWH1bvyUFxZjwWbcvD+5hyM6huEh6/tgigHCGNERCQe9tC4uKLKOgx6ORkyGZD135HwUDt2xjU1WvDrwUJ8suMk/jh61nY8rrMvHrq2C0b0DOCWDEREDoI9NNRuss793NTV39PhwwwAqJRy3Nw3CDf3DcKB00Z8uPUYvt93GjuPl2Ln8VJ0C/DEY9d3wz+jgjn1m4jIhbCz0sVliNwQfDUignWYPzYaW58ZgYev7QIvtRI5RVWYuXovRr79O77fdxrmlm5ORUREksZA4+Ky8ttmQT0xBXm7Y87NvfHHnOvx5I09oNM0BZt/fb7HFmwsDDZERE6NgcbFZThoQ/CV8NK44bEbumPb7L8Hm1sWbsMfOSVil0hERHbCQOPCCo11KKqsh1zW9PONs9CdF2yeSOgBT7USWflG3PfhTkxenorsgkqxSyQiojbGQOPCrNO1uwV4Qqty/Ibg1tJp3DAzoTu2PHUdJg8Jh1Iuw+bsYox653c8/dU+FBnrxC6RiIjaCAONC8uw7bCtF7cQO+vgqcZ/b+mDX2cNx819A2ERgC92n8L1b27Bh1uPocFsufyLEBGRQ2OgcWHWhuAoCTcEt0a4nwfeHx+Dr6cPQb8wParqG/HSuoMY/e5WpJy3pg0REUkPA42LEgTB1hAc6QQNwa0R08kHa6cPwf/d2Re+HiocLqzCuKU78Njne1BQwZ+hiIikiIHGRRUa61FSVQ+FXIaIIOdpCG4puVyGsQM74rcnh2NifCfIZcAP+04jYf4WfLzjJKd5ExFJDAONi8o4VQ4A6B7gCXeVQtxiRKTXqvDCrZH4/tFr0L9j089Qz3+bhXs/2IGjxVVil0dERC3EQOOibDtsu9jPTRcTGeKNr6YNwX/HRECrUiD1RClGvbMVCzflsGmYiEgCGGhcVKaLNQS3hEIuw+ShnfHLE9dieA9/mBoteP3nbIx5bxv2n64QuzwiIroEBhoXJAiCbQ0aV2sIbolQHy1WJA7EW2P7wUfrhkMFlbht4XYs3JTDvaGIiBwUA40LOlNRh7PVJijlMvR2wYbglpDJZLi9fyg2zhqOmyIMaDALeP3nbNyzJAUnz1aLXR4REf0FA40Lsk7X7m7wgsbNdRuCW8LPU40lE2Lw+l1R8FQrkXayDKPe2YrPduZCEHi3hojIUTDQuKDM/HIAQBR/bmoRmUyGu2PDsOHxYRjcxRc1JjOeXZuJKSt3o7TaJHZ5REQEBhqXlJlvBAD0ZUNwq4T6aPHZlMF4bnRvqJRyJB8qwqh3fseOY1xlmIhIbAw0LqapIbgcAKdsXwm5XIYpw7rguxlD0dXfA4XGety3dAfe+fUIG4aJiETEQONi8strUVbTADeFDL2CvMQuR7J6B+nww2PX4K6YUFgE4K1fD2P8hztQyB28iYhEwUDjYqzTtXsYvKBWsiH4amhVSrxxdz/Mv6cftCoFdhxrWoxvy+FisUsjInI5DDQuJoML6rW5OwaE4ofHrkHvIB1Kq02YvDwV7yUf4X5QRETtiIHGxWTZtjzQi1uIk+nq74m1jwzBuEEdIQjAmxsP46GPd6OitkHs0oiIXAIDjQsRBMG2Bg0bgtuexk2BpDv64rU7o6BSyvHrwSLcumAbsgsqxS6NiMjpMdC4kFNltaiobYBKIUePQE+xy3Fa9wwMw1fT4hGid8eJszW4beF2fL/vtNhlERE5NQYaF2K9O9MzkA3B9hYVqscPj12Da7r5obbBjH99vgcvrzvAqd1ERHbCQONCMs6tEMwF9dqHr4cKKx8YhEeu6woAWLr1OKas3IXKOvbVEBG1NQYaF2JtCOaWB+1HIZfh6X/0woL7+kOtlGNTdjHueP8P5J6tEbs0IiKnwkDjIppWCG4KNJEMNO3un1HB+HJaPAw6NY4UVeHWhdu4ZQIRURtioHERuaU1MNY1QqWUo4eBKwSLISpUj+9mXIOoUG+U1TTg/g93YnVqrthlERE5BQYaF2FtCO4d6AWVkn/bxRLorcGah+Lxz6ggNFoEzP4mEy/9eICL8BERXSV+s7mITOuCemwIFp27SoH3xvXHrBt7AAA+3HYcMz5LR12DWeTKiIiki4HGRVj7Z6K4QrBDkMlk+NcN3fHOvdFQKeT4KasA4z/cidJqk9ilERFJEgONC7BYBNsMJzYEO5Zbo0Ow6sFB0GmUSDtZhjsXcQYUEdGVYKBxASdLa1BZ3wi1Uo7uBq4Q7GgGd+mAr6cPQYjeHcdLqnH7+9uxN69c7LKIiCSFgcYFZJwqBwD0DtLBTcG/5Y6ou8ELax8Zgj7BOpytNuHeD1Kw8UCh2GUREUkGv91cgK1/hg3BDi1Ap8EXD8djeA9/1DVY8PDHu7FmF6d1ExG1BAONC7DNcGL/jMPzUCvx4aRY3BMbCosAPPN1JhZvOSp2WUREDo+Bxsmd3xDMKdvS4KaQ4//ujMK04U17QL360yEkrT8IQeBaNUREF8NA4+SOlVSj2mSGxk2Obv5sCJYKmUyG2aN6Yc6oXgCAJb8fw+yvM9FotohcGRGRY2KgcXLWuzMRQToo2RAsOQ8P74rX7oyCXAas2Z3HBfiIiC6C33BOLsPWEKwXtxC6YvcMDMP742OgUsjx8/5CJC7fhcq6BrHLIiJyKAw0Ti6LDcFO4R+RgVjxwEB4qpVIOXYW4z/cifIaripMRGTFQOPEzBYBWafZEOwshnT1w+dTB8NH64aMUxUYt3QnzlbVi10WEZFDYKBxYseKq1BjMsPdTYGubAh2Cn1DvbH6oXj4eapx8IwR936wA0WVdWKXRUQkOgYaJ2Zdf6ZPsA4KuUzkaqit9Az0wpqHByNQp8GRoircu2QHzlTUil0WEZGoGGicmLUhmD83OZ+u/p744uF4hOjdcaykGvcsSUFeKTe1JCLXxUDjxKwNwdzywDl17KDFF9PiEd5Bi7zSWoxdkoLjJdVil0VEJAoGGifVaLZg/2kjAM5wcmYheneseTgeXf09cLqiDmOXpCCnqFLssoiI2h0DjZM6WlyN2gYzPFQKdPZjQ7AzM+g0WPNwPHoFeqGosh73frATOUVVYpdFRNSuGGic1J8Nwd5sCHYBfp5qfD51MCKCdCipqse4pTtwtJihhohcBwONk8o8VQ6ADcGuxMdDhU+nxKFXoBeKK+sx7oMdOMZQQ0QugoHGSWWyIdglWUNNT0PTz0/jlu7ACTYKE5ELuKJAs3DhQoSHh0Oj0SAuLg6pqamXHP/222+jZ8+ecHd3R1hYGJ544gnU1XExMHs5vyE4kg3BLqeDpxqfTo1DD4MnCo1NoebkWYYaInJurQ40a9aswaxZszBv3jykp6ejX79+GDlyJIqKii44/rPPPsPs2bMxb948HDx4EB999BHWrFmDZ5999qqLpws7UlSF+kYLPNVKdO7gIXY5JAI/TzU+nTIY3QI8caaiDuM+2IHcs1ynhoicV6sDzfz58zF16lQkJiYiIiICixcvhlarxbJlyy44/o8//sDQoUNx3333ITw8HDfddBPGjRt32bs6dOXOXyFYzoZgl+XvpcZnU+NsU7rHLd3BxfeIyGm1KtCYTCakpaUhISHhzxeQy5GQkICUlJQLnjNkyBCkpaXZAsyxY8ewfv163HzzzRd9n/r6ehiNxmYParnMU+yfoSYBXhp8PnUwuvh5IL+8FuOWcpsEInJOrQo0JSUlMJvNMBgMzY4bDAYUFBRc8Jz77rsPL7zwAq655hq4ubmha9euuO666y75k1NSUhK8vb1tj7CwsNaU6fKsd2j6hurFLYQcQoBOg88fGozOfh44VVaL8R/uRAl36SYiJ2P3WU6bN2/GK6+8gvfffx/p6en45ptvsG7dOrz44osXPWfOnDmoqKiwPfLy8uxdptNoMFtw4AxXCKbmDDoNPpkS17T3U3E1JnyUioqaBrHLIiJqM60KNH5+flAoFCgsLGx2vLCwEIGBgRc85/nnn8eECRMwZcoU9O3bF7fffjteeeUVJCUlwWKxXPActVoNnU7X7EEtc7iwEqZGC7w0SnTy1YpdDjmQEL07PpkSBz9PNQ6eMWLS8lRU1TeKXRYRUZtoVaBRqVSIiYlBcnKy7ZjFYkFycjLi4+MveE5NTQ3k8uZvo1AoAACCILS2XroM64aUkcHebAimv+ns54FPp8RBr3XD3rxyTFm5C3UNZrHLIiK6aq3+yWnWrFlYunQpVq5ciYMHD2L69Omorq5GYmIiAGDixImYM2eObfyYMWOwaNEirF69GsePH8fGjRvx/PPPY8yYMbZgQ20ngw3BdBk9A72w6oFB8FQrseNYKaZ/kgZT44XvlhIRSYWytSeMHTsWxcXFmDt3LgoKChAdHY0NGzbYGoVzc3Ob3ZF57rnnIJPJ8NxzzyE/Px/+/v4YM2YMXn755bb7FGSTZWsIZqChi4sK1WPZ5IGYuGwnNmUX44k1e/HOvdFQKrh4OBFJk0yQwO8+RqMR3t7eqKioYD/NJZgaLYic9zNMZgu2PHUdOnFRPbqMLYeLMXXlbpjMFtw5IBSv3xXFnyqJqM205/c3/3fMiRwurITJbIFOo0RHNgRTCwzv4Y93x/WHQi7D1+mn8MKPB9jbRkSSxEDjRDLP+7lJJuP/ZVPL/CMyEG/cHQUAWPHHCSz4LUfkioiIWo+BxolYG4L7hujFLYQk5/b+oZg3JgIA8ObGw/hkx0mRKyIiah0GGidibQjmDCe6EolDO+Ox67sBAJ7/LgvrM8+IXBERUcsx0DiJ+kYzDhVwhWC6OrNu7IH74jpCEIDHV+/F9pwSsUsiImoRBhonkV1QiQazAL3WDaE+7mKXQxIlk8nw4q2RuLlvIExmCx5atRsZp8rFLouI6LIYaJyErSE4hA3BdHUUchneGhuNIV07oNpkxuTlu3CsuErssoiILomBxklknvoz0BBdLbVSgQ8mxqJviDdKq02Y8FEqCirqxC6LiOiiGGicRCYbgqmNeaqVWJE4EF38PJBfXouJy3aivMYkdllERBfEQOME6hrMyC6oBABE8g4NtaEOnmqsenAQDDo1DhdW4cGVu7mZJRE5JAYaJ3CooBKNFgG+HiqE6NkQTG0r1EeLjx+Mg06jRNrJMsxcvQdmC1cTJiLHwkDjBNgQTPbWw+CFDybGQqWQ4+f9hXiRWyQQkYNhoHECmeem1bIhmOxpcJcOmD+2H4CmLRKWbj0mckVERH9ioHECmfnnFtRjQzDZ2T+jgvHc6N4AgFfWH8J3e/NFroiIqAkDjcTVNZhxuLCpIZh3aKg9TBnWBQ8M7QwA+PeX+/DHUa4mTETiY6CRuANnjDBbBPh5qhDkrRG7HHIRz43ujZv7BqLBLODhj9Ns224QEYmFgUbistgQTCKQy2WYf080Bob7oLKuEYnLd+FMRa3YZRGRC2OgkbgMrhBMItG4KbB0Yiy6BXjiTEUdEpfvgrGuQeyyiMhFMdBInO0OTahe3ELIJem1KqxIHAh/LzUOFVTi4VVpMDVaxC6LiFwQA42E1ZrYEEziC/XRYkXiQHioFEg5dhbPfJ3BNWqIqN0x0EjYgTMVsAiAv5caBp1a7HLIhfUJ9sai+2OgkMuwdk8+3kk+InZJRORiGGgkzLrDdhQbgskBXNvDHy/dFgkAePvXI1i755TIFRGRK2GgkbCMc/0z3JCSHMW4QR3x8PAuAIBnvsrEzmNnRa6IiFwFA42EWRuCo7hCMDmQZ0b2wqjIQJjMFjz8SRqOl1SLXRIRuQAGGomqrm9ETlEVADYEk2ORy2V4a2w0osP0KK9pQOLyVJRVm8Qui4icHAONRB04Y4RFAAw6NQJ0XCGYHIt1jZpQH3ecOFuDhz7ejfpGs9hlEZETY6CRqEzbgnp6cQshugh/LzWWTx4IL40Su06U4emvOJ2biOyHgUaiMvO5QjA5vu4GLywaHwOlXIbv9p7G279yOjcR2QcDjURlsiGYJOKa7n54+fam6dzvJB/B12mczk1EbY+BRoKq6htxtLipIZhTtkkKxg7siOnXdQUAzP4mAzs4nZuI2hgDjQTtz6+AIABB3hr4e3GFYJKGp27qidF9g9BgFvDwx2k4di6UExG1BQYaCWL/DEmRXC7Dm/f0Q/+OelTUNmDKyt2oqOHu3ETUNhhoJIiBhqRK46bABxNiEeytwbGSajzyWRoazNydm4iuHgONBNkCDRuCSYL8vdT4cNJAaFUKbM85ixd+OCB2SUTkBBhoJKayrgHHipuWkucdGpKqiGAd3rm3P2Qy4OMdJ7Eq5YTYJRGRxDHQSExWvhEAEKJ3RwdPNgSTdN0YYcAz/+gFAPjfDwew9UixyBURkZQx0EhMFvtnyIk8fG0X3DkgFGaLgEc+TbftT0ZE1FoMNBKTwf4ZciIymQyv3BGJ2E4+qKxrxJSVu1Bew40siaj1GGgkhndoyNmolQosnhCDEH3TRpbTP0nnzCciajUGGgmpqG3A8RI2BJPz8fNU46PJsfBQKZBy7CzmfrefG1kSUasw0EjI/nN3Z0J93OHjoRK5GqK21StQh3fHNc18+jw1Fyv+OCF2SUQkIQw0EsINKcnZ3dDbgGdH9QYAvPjjAWzOLhK5IiKSCgYaCbE2BHNDSnJmU4Z1xj2xobAIwGOf7cGRwkqxSyIiCWCgkRBrQ3BUiF7cQojsSCaT4aXb+mJQuC8q6xvx4MrdKK3mzCciujQGGomoqGnAybM1AIDIEJ3I1RDZl0opx6L7ByDM1x25pTV45FPu+UREl8ZAIxHW/pmOvlrotWwIJufXwVONjyYNhIdKgR3HSvHij9zziYgujoFGIrghJbmiHgYvvH1uz6dVKSfx2c5csUsiIgfFQCMRmfnlALj+DLmeGyMMePLGHgCAud9lIfV4qcgVEZEjYqCRCNuUbQYackEzRnTD6KggNFoETP8kDafKasQuiYgcDAONBJRVm5BXWgsA6MNAQy5IJpPh9bui0CdYh7PVJjy0Kg01pkaxyyIiB8JAIwHWuzPhHbTwdncTuRoicWhVSnwwMRZ+niocOGPEU19mcHsEIrJhoJGAPxuC9eIWQiSyEL07Ft0fAzeFDOsyz2DBbzlil0REDoKBRgIyT1l32Ob6M0QDw33x4q2RAIA3Nx7GL/sLRK6IiBwBA40E2O7QcIVgIgDAvYM6YlJ8JwDAE2v2IruA2yMQuToGGgd3tqoe+eXWhmDeoSGyeu6fEYjv0gHVJjOmrNqFMm6PQOTSGGgcnPXuTBc/D+g0bAgmsnJTyPH++KbtEfJKazHjs3Ruj0Dkwq4o0CxcuBDh4eHQaDSIi4tDamrqJceXl5djxowZCAoKglqtRo8ePbB+/forKtjVZHGFYKKL8vFQ4cOJTdsj/HH0LF5ed1DskohIJK0ONGvWrMGsWbMwb948pKeno1+/fhg5ciSKioouON5kMuHGG2/EiRMn8NVXXyE7OxtLly5FSEjIVRfvCjJsDcEMNEQX0jPQC2+NjQYArPjjBFancnsEIlfU6kAzf/58TJ06FYmJiYiIiMDixYuh1WqxbNmyC45ftmwZSktL8e2332Lo0KEIDw/H8OHD0a9fv6su3hX82RDMQEN0MTf1CbRtj/D8d1nYdYLbIxC5mlYFGpPJhLS0NCQkJPz5AnI5EhISkJKScsFzvv/+e8THx2PGjBkwGAyIjIzEK6+8ArPZfNH3qa+vh9FobPZwRcWV9ThTUQeZjCsEE13Oo9d3w+i+QWgwN22PYG2mJyLX0KpAU1JSArPZDIPB0Oy4wWBAQcGF14I4duwYvvrqK5jNZqxfvx7PP/883nzzTbz00ksXfZ+kpCR4e3vbHmFhYa0p02lkndcQ7KlWilwNkWOTyWR4/e4oRATpUFJlwkOrdqPWdPH/cSIi52L3WU4WiwUBAQH44IMPEBMTg7Fjx+I///kPFi9efNFz5syZg4qKCtsjLy/P3mU6JNuGlFwhmKhFmrZHiEEHDxX2nzbiqa/2cXsEIhfRqkDj5+cHhUKBwsLCZscLCwsRGBh4wXOCgoLQo0cPKBQK27HevXujoKAAJtOF141Qq9XQ6XTNHq7I2hAcyZ+biFos1EeLRffHQCmX4ceMM1i05ajYJRFRO2hVoFGpVIiJiUFycrLtmMViQXJyMuLj4y94ztChQ5GTkwOL5c/1IQ4fPoygoCCoVKorLNs1ZOaXAwCiOGWbqFUGdfbF/27tAwB4/edsJB8svMwZRCR1rf7JadasWVi6dClWrlyJgwcPYvr06aiurkZiYiIAYOLEiZgzZ45t/PTp01FaWoqZM2fi8OHDWLduHV555RXMmDGj7T6FEyoy1qHQWA+ZDIgIcs07VERXY3xcJ4yP6whBAGau3oucIm6PQOTMWt1pOnbsWBQXF2Pu3LkoKChAdHQ0NmzYYGsUzs3NhVz+Z04KCwvDzz//jCeeeAJRUVEICQnBzJkz8cwzz7Tdp3BC1v6Zbv6e8GBDMNEVmTemD44UVSH1eCmmrkrDtzOGwtudK24TOSOZIIGOOaPRCG9vb1RUVLhMP83bvx7G278ewR0DQjD/nmixyyGSrJKqety6YDvyy2sxvIc/lk0eCIVcJnZZRC6hPb+/uZeTg8rkCsFEbcLPU40PJsZA4ybHlsPFeG3DIbFLIiI7YKBxUBm2KdsMNERXq0+wN964u2l18iW/H8O3e/JFroiI2hoDjQMqNNahuLIechkQEcRAQ9QW/hkVjBkjugIAnvk6AxmnysUtiIjaFAONA7KuP9M9wAvuKsVlRhNRSz15Y0/c0CsA9Y0WPLQqDUWVdWKXRERthIHGAdk2pOTPTURtSi6X4e17o9EtwBMFxjpM/yQd9Y3cHoHIGTDQOKDMc7fC2RBM1Pa8NG5YOjEWOo0SaSfLMPfb/dwegcgJMNA4GEEQeIeGyM46+3ngvfsGQC4D1uzOw6qUk2KXRERXiYHGwRQY61BSZYJCLuMKwUR2NLyHP+aM6g0AeOHHA/jjaInIFRHR1WCgcTB/NgR7QuPGhmAie5oyrDNu7x8Cs0XAjE/TkVdaI3ZJRHSFGGgcTBbXnyFqNzKZDEl39EVUqDfKahowddVuVNc3il0WEV0BBhoHk8EVgonalcZNgSUTYuDnqcahgkr8+8t9sFjYJEwkNQw0DqR5Q7Be3GKIXEiQtzuWTIiBSiHHT1kFeO+3HLFLIqJWYqBxIKcr6lBabYJSLkOvQC+xyyFyKTGdfPDSbZEAgLd+PYyf9xeIXBERtQYDjQOxrj/Tw+DFhmAiEdwzMAyTh4QDAGat2YvsgkpxCyKiFmOgcSCZbAgmEt1/RvfGkK4dUG0yY+qq3SirNoldEhG1AAONA7E2BEeyIZhING4KORbeNwBhvu7ILa3Bo5+no9FsEbssIroMBhoHcX5DMO/QEInLx0OFpRNjoVUpsD3nLF5Zf0jskojoMhhoHMSpslqU1zTATSFDTzYEE4muV6AO8+/pBwBYtv04vtydJ3JFRHQpDDQOwnp3pmegF9RKNgQTOYJ/RAZh5g3dAQD/WZuF9NwykSsioothoHEQtvVnQvTiFkJEzcy8oTtuijDAZLZg2sdpKDTWiV0SEV0AA42DyOQKwUQOSS6XYf7YaPQweKKosh4PfZyGugaz2GUR0V8w0DgANgQTOTZPtRIfThwIvdYN+/LK8ew3mRAEbo9A5EgYaBxAXmktKmoboFLI0cPAhmAiR9SxgxYL7xsAhVyGb/bk46Ntx8UuiYjOw0DjADLyywEAvYK8oFLybwmRoxrazQ//ubk3AOCV9Qfx++FikSsiIit+ezqAPxuC+XMTkaNLHBqOu2NCYRGARz9Lx4mSarFLIiIw0DgENgQTSYdMJsNLt0eif0c9jHWNmLJqNyrrGsQui8jlMdCI7PyG4L5sCCaSBLVSgSX3x8CgUyOnqApPrNkLi4VNwkRiYqAR2cmzNaisa4RKyYZgIikJ0GmwZEIsVEo5fj1YhLd+PSx2SUQujYFGZBnn7s70DtLBTcG/HURSEh2mR9LtfQEA7/2Wg3UZZ0SuiMh18RtUZFnW9WfYP0MkSXfGhGLKNZ0BAP/+ch/2n64QuSIi18RAI7KMU+UA2BBMJGWzR/XCsO5+qG0w46FVaThbVS92SUQuh4FGRBaLgKx8IwA2BBNJmVIhx4JxAxDeQYv88lo88mk6GswWscsicikMNCI6cbYaVfWNUCvl6B7gKXY5RHQVvLVuWDoxFh4qBXYeL8ULPxwQuyQil8JAIyLrdO2IYB2UbAgmkrzuBi+8fW9/yGTAxztO4rOduWKXROQy+C0qIuuCemwIJnIeN0YY8OSNPQAA877Pwq4TpSJXROQaGGhEZJ2yHclAQ+RUZozohtF9g9BgFjD9kzTkl9eKXRKR02OgEYnFImC/dcp2qF7cYoioTclkMrx+dxR6B+lQUmXCwx/vRq3JLHZZRE6NgUYkx0qqUW0yw91Nga7+HmKXQ0RtTKtS4oMJMfD1UCEr34hnvs6AIHB7BCJ7YaARSWZ+OQA2BBM5szBfLd4fPwBKuQzf7zuNxVuOiV0SkdPiN6lIMk+dW3+G/TNETm1wlw6YNyYCAPDaz4ew6VCRyBUROScGGpFY79Aw0BA5v/sHd8K4QR0hCMC/Pt+Do8VVYpdE5HQYaERgPm+F4CiuEEzk9GQyGf53Sx/EdvJBZX0jpq7cjYraBrHLInIqDDQiOFZchdoGM7QqBbr4c4VgIlegUsqx6P4YBHlrcKykGjNX74HZwiZhorbCQCOCjHML6vUJ1kEhl4lcDRG1F38vNT6YEAu1Uo7N2cV4/edssUsichoMNCKwbnnQN0QvbiFE1O76hnrjtbuiAACLtxzFd3vzRa6IyDkw0IjAFmhCdSJXQkRiuDU6BNOGdwUAPP1Vhm0bFCK6cgw07azRbMH+07xDQ+TqnhrZEyN6+qO+0YKHPt6N4sp6sUsikjQGmnZ2tLgadQ0WeKgU6OLHFYKJXJVCLsM74/qji78HzlTU4eGPd6OugdsjEF0pBpp2lnGqHADQJ8QbcjYEE7k0ncYNSyfGQqdRIj23nNsjEF0FBpp2lmXdkJIL6hERgK7+nlh0fwwUchm+23sa7/2WI3ZJRJLEQNPOMmwNwQw0RNRkaDc/vHhrJABg/sbD+GHfaZErIpIeBpp21Gi24MBp7uFERH93X1xHTLmmMwDgyS/3IT23TOSKiKSFgaYdHSmqQn2jBV5qJcI7sCGYiJqbc3NvJPQOgKnRgodW7capshqxSyKSDAaadmRda6JPiI4NwUT0Nwq5DO/c2x+9g3QoqTJhysrdqKzjnk9ELcFA046sC+pFherFLYSIHJaHWomPJsXC30uNQwWV+Nfn3POJqCUYaNqRtSE4kv0zRHQJwXp3LJ3YtOfTpuxivLzuoNglETm8Kwo0CxcuRHh4ODQaDeLi4pCamtqi81avXg2ZTIbbbrvtSt5W0hrMFhw809QQzCnbRHQ50WF6zL8nGgCwbPtxfLLjpLgFETm4VgeaNWvWYNasWZg3bx7S09PRr18/jBw5EkVFRZc878SJE/j3v/+NYcOGXXGxUna4sBKmRgu8NEp06qAVuxwikoDRUUH49009AADzvt+PrUeKRa6IyHG1OtDMnz8fU6dORWJiIiIiIrB48WJotVosW7bsoueYzWaMHz8e//vf/9ClS5erKliqrA3BfUO8IZOxIZiIWmbGiG64o38IzBYBj3yajpyiSrFLInJIrQo0JpMJaWlpSEhI+PMF5HIkJCQgJSXloue98MILCAgIwIMPPtii96mvr4fRaGz2kLpMLqhHRFdAJpMh6c6+iO3kg8q6RjywYjdKq01il0XkcFoVaEpKSmA2m2EwGJodNxgMKCgouOA527Ztw0cffYSlS5e2+H2SkpLg7e1te4SFhbWmTIdkCzTsnyGiVlIrFVgyIQYdfbXILa3BQ6u4kSXRX9l1llNlZSUmTJiApUuXws/Pr8XnzZkzBxUVFbZHXl6eHau0P1OjBYfONN0mjgrRi1sMEUlSB081lk2OhZdGid0ny/Dkl/tg4XRuIhtlawb7+flBoVCgsLCw2fHCwkIEBgb+bfzRo0dx4sQJjBkzxnbMYrE0vbFSiezsbHTt2vVv56nVaqjV6taU5tAOF1bCZLbA290NYb7uYpdDRBLVLcALSybEYNKyVKzLOINQH3fMGdVb7LKIHEKr7tCoVCrExMQgOTnZdsxisSA5ORnx8fF/G9+rVy9kZmZi7969tsctt9yCESNGYO/evU7xU1JLZLAhmIjayJCufvi/O6MAAEu2HON0bqJzWnWHBgBmzZqFSZMmITY2FoMGDcLbb7+N6upqJCYmAgAmTpyIkJAQJCUlQaPRIDIystn5er0eAP523JmxIZiI2tIdA0JxqqwW8zcextzvshCs1+D6XobLn0jkxFodaMaOHYvi4mLMnTsXBQUFiI6OxoYNG2yNwrm5uZDLuQDx+TLzywGwIZiI2s5j13fDqbIafLH7FB79bA/WPBTP/2kilyYTBMHhu8qMRiO8vb1RUVEBnU4ndjmtUt9oRuS8n9FgFrD16REI8+WiekTUNhrMFjywYhe2HimBv5caax8ZglAf/jeGHEd7fn/zVoqdZRdUosEswEfrhlAfNgQTUdtxU8jx/vgB6BXoheLKeiQu34WKWu7OTa6JgcbOrA3BkWwIJiI78NK4YXniQBh0ahwpqsK0j9NgarSIXRZRu2OgsbOscw3BUfxtm4jsJMjbHcsmD4SHSoGUY2cx++sMSKCbgKhNMdDY2flTtomI7KVPsDfevz8GCrkM3+zJx1u/HhG7JKJ2xUBjR3UNZhwubFohuG+oXtxiiMjpDe/hj5dva1oS493kI1izK1fkiojaDwONHR0qqESjRUAHDxWCvTVil0NELuDeQR3x6IhuAIBn12Yh+WDhZc4gcg4MNHaUeaocABuCiah9PXlTD9wdEwqzRcCMz9KRdrJM7JKI7I6Bxo4y2RBMRCKQyWRIuqMvru8VgLoGCx5cuQs5RZVil0VkVww0dnT+lG0iovakVMix4L7+iA7To7ymARM/SkVBRZ3YZRHZDQONndQ1mHGkqAoA79AQkTi0KiWWTR6ILv4eOF1Rh0nLUlFRw4X3yDkx0NjJgTNGmC0C/DzVCNSxIZiIxOHrocKqBwYhwEuN7MJKTF21G3UNZrHLImpzDDR2kmlbf0bHhmAiElWojxYrHxgEL7USqSdKMXP1HpgtXHiPnAsDjZ3YFtTj+jNE5AB6B+nwwcRYqBRy/Ly/EHO/y+JqwuRUGGjsxLrlAVcIJiJHEd+1A96+NxoyGfDpzly8m5wjdklEbYaBxg5qTI04cm6KJBuCiciR3Nw3CC/c0gcA8Navh7Eq5YS4BRG1EQYaOzh4xgiLAAR4qWFgQzAROZgJ8eH41/VNqwnP/W4/vt2TL3JFRFePgcYOuCElETm6J27sgclDwgEAT365D78e4BYJJG0MNHZgm+HEn5uIyEHJZDLM/WcE7ugfArNFwCOfpeOPoyVil0V0xRho7IBbHhCRFMjlMrx2VxRujDDA1GjB1JW7sS+vXOyyiK4IA00bq65vRE5x0wrB3PKAiBydUiHHe+P6Y0jXDqg2mTFpeSoOF3LfJ5IeBpo2duCMEYIABOo0CPBiQzAROT6NmwIfTIxFv3P7Pk34aCfySmvELouoVRho2hg3pCQiKfJUK7EycSB6GDxRaKzH+A93osjIzSxJOhho2ljmqXIA7J8hIunRa1X4+ME4dPTVIre0BhM+SkVZtUnssohahIGmjVkbgjnDiYikyKDT4NMpcTDomjazvP+jndyhmySBgaYNVdU34lhJNQCuQUNE0hXmq8WnU+Lg56nC/tNGTFyeiso6hhpybAw0bWh/fgUEAQj21sDPUy12OUREV6xbgBc+mRIHH60b9uWVY/LyXaiubxS7LKKLYqBpQ9afm9gQTETOoFegDh8/GAedRom0k2V4cOUu1JrMYpdFdEEMNG3IOsOJDcFE5CwiQ7yx6sE4eKqV2HGsFA99vBt1DQw15HgYaNpQlq0hWC9uIUREbSg6TI+VDwyEVqXA1iMlmP5JGuobGWrIsTDQtBFjXQMbgonIacV08sWyyQOhcZNjU3YxHv1sDxrMFrHLIrJhoGkj+/ONAIAQvTt8PVQiV0NE1PYGd+mADycOhEopx8YDhfjX5ww15DgYaNpIZn45AN6dISLndk13PyyZEAOVQo6fsgow49N0mBoZakh8DDRtxNoQzAX1iMjZjegZgCUTY6BSyvHLgUI88il7akh8DDRtxNoQzBlOROQKRvQMwIcTY6FWyvHrwSJM+ziNs59IVAw0baCitgEnzjbtTBsZzEBDRK7h2h7+zRqFp67ilG4SDwNNG9h/7u5MmK87fNgQTEQuZGg3PyyfPAjubk1Turn4HomFgaYNZFjXn2FDMBG5oPiuHbDygUHQqhTYnnMWiStSUWPiNgnUvhho2kCmtSE4RC9uIUREIhnU2RerHhhkW1F40rJUGLmhJbUjBpo2kMmGYCIixIb7YtWDg+ClUWLXiTLct3QHzlbVi10WuQgGmqtUXmNCbikbgomIAGBARx98PnUwOniokJVvxD1LUnCmolbsssgFMNBcpaxzKwR36qCFt9ZN5GqIiMQXGeKNNQ/HI8hbg6PF1bhrUQpOnNsahsheGGiuUsa5FYIj2RBMRGTTLcATX06LR3gHLfLLa3H3khQcKjCKXRY5MQaaq2RtCI5ioCEiaibUR4svpsWjV6AXiivrMXbJDuzJLRO7LHJSDDRXydoQzC0PiIj+LsBLgzUPxaN/Rz0qahsw/sOd2J5TInZZ5IQYaK5CWbUJp8qamt34kxMR0YV5a93wyYNxGNqtA2pMZiQu34Uf9p0WuyxyMgw0V8F6d6aznwd0GjYEExFdjIdaiY8mDcSoyECYzBY89vkefLj1mNhlkRNhoLkK1kDDuzNERJencVNgwX0DMCm+EwDgpXUH8fK6A7BYBJErI2fAQHMVMk6VA2BDMBFRSynkMvz3lj6YPaoXAGDp1uN4fM1emBotIldGUsdAcxWsa9CwIZiIqOVkMhmmDe+K+ff0g1Iuw/f7TiNxRSoquVUCXQUGmit0tqoe+eVNDcF9gnUiV0NEJD13DAjFsskD4XFuU8t7luxAQUWd2GWRRDHQXCFr/0wXfw94sSGYiOiKXNvDH2sejoefpxoHzxhx68JtyDr331ei1mCguUJ/7rDNn5uIiK5GZIg31j4yBN0DPFForMfdi1Pw8/4CscsiiWGguUIZ+Qw0RERtJcxXi68fGYJh3f1Q22DGtE/SsGTLUQgCZ0BRyzDQXCHrLdGoUL24hRAROQmdxg3LJw/EhMGdIAhA0k+HMPvrTM6AohZhoLkCxZX1OFNRB5mMDcFERG1JqZDjxdsi8d8xEZDLgDW78zBpWSrKa0xil0YOjoHmCljvznT194SHWilyNUREzmfy0M74aFLTDKiUY2dx68LtyC6oFLsscmBXFGgWLlyI8PBwaDQaxMXFITU19aJjly5dimHDhsHHxwc+Pj5ISEi45HgpyGBDMBGR3Y3oFYCvHxmCUB93nDxbg9vf3471mWfELoscVKsDzZo1azBr1izMmzcP6enp6NevH0aOHImioqILjt+8eTPGjRuHTZs2ISUlBWFhYbjpppuQn59/1cWLJTO/HAADDRGRvfUK1OGHR6+xbWz5yKfp+L8Nh2Dmdgn0FzKhlS3kcXFxGDhwIBYsWAAAsFgsCAsLw2OPPYbZs2df9nyz2QwfHx8sWLAAEydObNF7Go1GeHt7o6KiAjqd+D0rca/8ikJjPb6aFo/YcF+xyyEicnqNZgte/zkbS35v2tDy2h7+ePfeaOi1KpEro0tpz+/vVt2hMZlMSEtLQ0JCwp8vIJcjISEBKSkpLXqNmpoaNDQ0wNf34kGgvr4eRqOx2cNRFBnrUGish1wGRLAhmIioXSgVcsy5uTfeHdcfGjc5fj9cjFsWbMehAsf5fiBxtSrQlJSUwGw2w2AwNDtuMBhQUNCyRZCeeeYZBAcHNwtFf5WUlARvb2/bIywsrDVl2pV1heBuAZ7QqtgQTETUnm7pF4xvpg9FmK87cktrcNvC7fhidx7Xq6H2neX06quvYvXq1Vi7di00Gs1Fx82ZMwcVFRW2R15eXjtWeWnWhuBI9s8QEYkiIripr2Z4D3/UNVjw9FcZePLLfagxNYpdGomoVYHGz88PCoUChYWFzY4XFhYiMDDwkue+8cYbePXVV/HLL78gKirqkmPVajV0Ol2zh6Ow3qGJYqAhIhKNXqvC8skD8fQ/ekIhl+Gb9HzcsoBTu11ZqwKNSqVCTEwMkpOTbccsFguSk5MRHx9/0fNee+01vPjii9iwYQNiY2OvvFqRCYJgCzR9uUIwEZGo5HIZHrmuGz6fOhgGnRo5RVW4deE2fLHbce7qU/tp9U9Os2bNwtKlS7Fy5UocPHgQ06dPR3V1NRITEwEAEydOxJw5c2zj/+///g/PP/88li1bhvDwcBQUFKCgoABVVVVt9ynaSaGxHsWV5xqCgxznrhERkSsb1NkX6/81DNee9xPUE2v2wljXIHZp1I5aHWjGjh2LN954A3PnzkV0dDT27t2LDRs22BqFc3NzcebMnwsfLVq0CCaTCXfddReCgoJsjzfeeKPtPkU7sd6d6WHwgrtKIXI1RERk1cFTjRWTB+KpkT0hlwFr9+Rj1NtbkXq8VOzSqJ20eh0aMTjKOjTzf8nGu7/l4K6YULxxdz/R6iAiootLO1mGJ9bsRW5pDWQyYPrwrng8oQdUSu72094cdh0aV5dh22GbDcFERI4qppMP1s8chrtjQiEIwPubj+KORduRU8SGYWfGQNNCgiDYNqXklgdERI7NU63E63f3w6LxA6DXuiEr34jR727DR9uOc9sEJ8VA00JnKupQUmWCQi5DbzYEExFJwqi+Qfj58WsxrLsf6hstePHHA7h78R/IKZLexBS6NAaaFjq/IVjjxoZgIiKpMOg0WJk4CC/fHglPtRLpueW4+d2tWLgpB41mi9jlURthoGmhzFPWn5t4d4aISGrkchnGx3XCL09ci+t6+sPU2LTZ5W3vb8eB09wPyhkw0LRQBhfUIyKSvGC9O5ZPHog37+4Hb/em3poxC7bh5XUHUFXPrROkjIGmBc5vCOaWB0RE0iaTyXBnTCg2zroWoyIDYbYIWLr1OG54czN+zDjNjS4lioGmBfLLa1FabYJSLkPPQC+xyyEiojYQ4KXBovtjsDxxIDp10KLQWI9HP9uDictScayYTcNSw0DTAta7Mz0D2RBMRORsRvQMwM+PX4vHE7pDpZRj65ES/OPtrUj66SC3T5AQBpoWyDjF9WeIiJyZxk2BxxN6YKO1adhswZItx3Dd65uxKuUEGjgbyuEx0LTAnztsM9AQETmzTh08sHzyQHw0KRZd/T1QWm3C3O/2Y+Tbv+PXA4Xsr3FgDDSXIQiCLdBEhejFLYaIiOxOJpPhht4GbHj8Wrx4ax/4eqhwrLgaU1btxr0f7OCGlw6KgeYyTpXVorymAW4KGXoEeopdDhERtRM3hRwT4sOx+anrMG14V6iUcuw8Xop7lqRgwkc7kZ5bJnaJdB4Gmsuw3p3pFaiDWsmGYCIiV6PTuGH2qF7Y/O/rcF9cRyjlMmw9UoI73v8DictTkXGqXOwSCQw0l2VtCI5kQzARkUsL1rvjldv7YtO/r8M9saFQyGXYlF2MWxZsx/gPd2DrkWL22IiIgeYyMvPLAQBRbAgmIiIAYb5avHZXPyTPGo47+odAIZdhe85ZTPgoFaPf3Ybv9uZzjygRMNBcgiAI5+3hxEBDRER/CvfzwPyx0djy1HVIHBoOdzcFDpwxYubqvRj++mYs2nwUZ6vqxS7TZcgECdwfMxqN8Pb2RkVFBXS69tsc8uTZagx/fTNUCjmy/jcSKiXzHxERXVhZtQmf7DiJFX+cwNlqEwBApZBjdFQQ7h/cCQM66iGTyUSusn215/e30q6vLnHWhuDeQV4MM0REdEk+Hio8dkN3TL22C37Ydxqf7DiJfacqsHZPPtbuyUevQC/cFROKW6ND4O+lFrtcp8NAcwm2n5vYP0NERC2kcVPg7tgw3B0bhn155fhkx0l8v+80DhVU4qV1B5H00yFc290PdwwIxY0RBm6p00YYaC6BWx4QEdHV6BemR78wPf4zujd+2Hca3+zJx57ccmzKLsam7GJ4qBS4rlcARvYJxIie/vDSuIldsmQx0FyExSIg67Q10OjFLYaIiCRNr1VhQnw4JsSH41hxFdbuycc36fnIL6/FuowzWJdxBiqFHEO7dcCNEYEY1t0PYb5ascuWFAaaizhZWoPKukaolHJ0N3CFYCIiahtd/D3x5E098URCD2TkV+Dn/QX4OasAx0qqbXduAKBTBy2u6eaHYd39MLhLB+i1KpErd2wMNBdhbQiOCNLBTcGGYCIialtyuQzRYXpEh+nx9MieyCmqws/7C7A5uxh78spx8mwNTp7Nxac7cwEAXfw9MKCjT9Ojkx7d/D2h5PeTDQPNRWSeW8qaC+oREZG9yWQydDd4obvBC49e3x2VdQ3YcawU244UY1tOCY4WV+PYucdXaacAACqlHN38PdEr0As9Ar3Q0+CFMF8tQn3cXbLRmIHmIrjlARERicVL44YbIwy4McIAoGmNmz15ZUg/WY703DLsyytHtcmMA2eMOHDG+Lfz/b3UCPVxR7C3O3w9VPDxUMFX6wYfDxW8NEqoFAqo3eRQK+VQKeWQy2QI89HCXSXdIMRAcwEWi4D9p5v+AeEdGiIiEpuPhwrX9zLg+l5NAcdiEXCqrBaHCow4XFiJQwWVyCmqwqmyWlTVN6K4sh7FlfXYg/IWv8c3jwzBgI4+dvoE9sdAcwHHz1ajqr4RGrem23lERESORC6XoWMHLTp20OKmPoG244IgoKK2AXmltThVVoMzFXUorzGhtMaEsuoGnK2uR63JjPpGS9OjoemvBQBucmn34zDQXIB1Qb2IIB0broiISDJkMhn0WhX0WpXLLQrLb+sLsM5wigrVi1sIERERtQgDzQVksiGYiIhIUhho/sJsEbD/tPUODQMNERGRFDDQ/MXxkipUm8xwd1OgKxuCiYiIJIGB5i+s68/0CdZBIZeJXA0RERG1BAPNX1gbgl2tO5yIiEjKGGj+wtoQ3JcNwURERJLBQHMeM1cIJiIikiQGmvMcLa5CbYMZWpUCnf3YEExERCQVDDTnsW1IGezNhmAiIiIJYaA5TxYbgomIiCSJgeY8GafKAbAhmIiISGoYaM5pNFtw4ExTQzDv0BAREUkLA805OcVVqGuwwFOtROcOHmKXQ0RERK3AQHPO+SsEy9kQTEREJCkMNOdYG4K5/gwREZH0MNCcY5uyzYZgIiIiyWGgAdBgtuDgGesKwXpxiyEiIqJWY6ABcKSwCvWNFniplejkqxW7HCIiImolBhoAmfnlAJp+bmJDMBERkfQw0ADIZEMwERGRpDHQAMhkQzAREZGkuXygMTVacLCgEgDv0BAREUmVyweaw4WVMDVaoNMo0ZENwURERJLk8oEm87wdtmUyNgQTERFJEQONNdCE6MUthIiIiK4YA80pa6Bh/wwREZFUXVGgWbhwIcLDw6HRaBAXF4fU1NRLjv/yyy/Rq1cvaDQa9O3bF+vXr7+iYttafaMZhwqsKwQz0BAREUlVqwPNmjVrMGvWLMybNw/p6eno168fRo4ciaKioguO/+OPPzBu3Dg8+OCD2LNnD2677TbcdtttyMrKuurir9bhgio0mAV4u7sh1Mdd7HKIiIjoCskEQRBac0JcXBwGDhyIBQsWAAAsFgvCwsLw2GOPYfbs2X8bP3bsWFRXV+PHH3+0HRs8eDCio6OxePHiFr2n0WiEt7c3KioqoNPpWlPuJX268yT+szYLw7r74eMH49rsdYmIiMh+398X0qo7NCaTCWlpaUhISPjzBeRyJCQkICUl5YLnpKSkNBsPACNHjrzoeACor6+H0Whs9rCHrHz2zxARETmDVgWakpISmM1mGAyGZscNBgMKCgoueE5BQUGrxgNAUlISvL29bY+wsLDWlNliGWwIJiIicgoOOctpzpw5qKiosD3y8vLs8j4PDO2M+wd3RHRHvV1en4iIiNqHsjWD/fz8oFAoUFhY2Ox4YWEhAgMDL3hOYGBgq8YDgFqthlqtbk1pV+TOmFDcGRNq9/chIiIi+2rVHRqVSoWYmBgkJyfbjlksFiQnJyM+Pv6C58THxzcbDwAbN2686HgiIiKi1mrVHRoAmDVrFiZNmoTY2FgMGjQIb7/9Nqqrq5GYmAgAmDhxIkJCQpCUlAQAmDlzJoYPH44333wTo0ePxurVq7F792588MEHbftJiIiIyGW1OtCMHTsWxcXFmDt3LgoKChAdHY0NGzbYGn9zc3Mhl/9542fIkCH47LPP8Nxzz+HZZ59F9+7d8e233yIyMrLtPgURERG5tFavQyOG9pzHTkRERG3DYdehISIiInJEDDREREQkeQw0REREJHkMNERERCR5DDREREQkeQw0REREJHkMNERERCR5DDREREQkeQw0REREJHmt3vpADNbFjI1Go8iVEBERUUtZv7fbY1MCSQSayspKAEBYWJjIlRAREVFrVVZWwtvb267vIYm9nCwWC06fPg0vLy/IZLI2e12j0YiwsDDk5eVxj6h2xmsvDl538fDai4fXXhzW637gwAH07Nmz2cbV9iCJOzRyuRyhoaF2e32dTsd/yEXCay8OXnfx8NqLh9deHCEhIXYPMwCbgomIiMgJMNAQERGR5Ll0oFGr1Zg3bx7UarXYpbgcXntx8LqLh9dePLz24mjv6y6JpmAiIiKiS3HpOzRERETkHBhoiIiISPIYaIiIiEjyGGiIiIhI8lw60CxcuBDh4eHQaDSIi4tDamqq2CVJyu+//44xY8YgODgYMpkM3377bbPnBUHA3LlzERQUBHd3dyQkJODIkSPNxpSWlmL8+PHQ6XTQ6/V48MEHUVVV1WxMRkYGhg0bBo1Gg7CwMLz22mv2/mgOLSkpCQMHDoSXlxcCAgJw2223ITs7u9mYuro6zJgxAx06dICnpyfuvPNOFBYWNhuTm5uL0aNHQ6vVIiAgAE899RQaGxubjdm8eTMGDBgAtVqNbt26YcWKFfb+eA5t0aJFiIqKsi3QFh8fj59++sn2PK97+3j11Vchk8nw+OOP247x2tvHf//7X8hksmaPXr162Z53qOsuuKjVq1cLKpVKWLZsmbB//35h6tSpgl6vFwoLC8UuTTLWr18v/Oc//xG++eYbAYCwdu3aZs+/+uqrgre3t/Dtt98K+/btE2655Rahc+fOQm1trW3MP/7xD6Ffv37Cjh07hK1btwrdunUTxo0bZ3u+oqJCMBgMwvjx44WsrCzh888/F9zd3YUlS5a018d0OCNHjhSWL18uZGVlCXv37hVuvvlmoWPHjkJVVZVtzLRp04SwsDAhOTlZ2L17tzB48GBhyJAhtucbGxuFyMhIISEhQdizZ4+wfv16wc/PT5gzZ45tzLFjxwStVivMmjVLOHDggPDee+8JCoVC2LBhQ7t+Xkfy/fffC+vWrRMOHz4sZGdnC88++6zg5uYmZGVlCYLA694eUlNThfDwcCEqKkqYOXOm7TivvX3MmzdP6NOnj3DmzBnbo7i42Pa8I113lw00gwYNEmbMmGH7s9lsFoKDg4WkpCQRq5KuvwYai8UiBAYGCq+//rrtWHl5uaBWq4XPP/9cEARBOHDggABA2LVrl23MTz/9JMhkMiE/P18QBEF4//33BR8fH6G+vt425plnnhF69uxp508kHUVFRQIAYcuWLYIgNF1nNzc34csvv7SNOXjwoABASElJEQShKYzK5XKhoKDANmbRokWCTqezXeunn35a6NOnT7P3Gjt2rDBy5Eh7fyRJ8fHxET788ENe93ZQWVkpdO/eXdi4caMwfPhwW6DhtbefefPmCf369bvgc4523V3yJyeTyYS0tDQkJCTYjsnlciQkJCAlJUXEypzH8ePHUVBQ0Owae3t7Iy4uznaNU1JSoNfrERsbaxuTkJAAuVyOnTt32sZce+21UKlUtjEjR45EdnY2ysrK2unTOLaKigoAgK+vLwAgLS0NDQ0Nza59r1690LFjx2bXvm/fvjAYDLYxI0eOhNFoxP79+21jzn8N6xj+O9LEbDZj9erVqK6uRnx8PK97O5gxYwZGjx79t+vDa29fR44cQXBwMLp06YLx48cjNzcXgONdd5cMNCUlJTCbzc0uMAAYDAYUFBSIVJVzsV7HS13jgoICBAQENHteqVTC19e32ZgLvcb57+HKLBYLHn/8cQwdOhSRkZEAmq6LSqWCXq9vNvav1/5y1/ViY4xGI2pra+3xcSQhMzMTnp6eUKvVmDZtGtauXYuIiAhedztbvXo10tPTkZSU9LfneO3tJy4uDitWrMCGDRuwaNEiHD9+HMOGDUNlZaXDXXdJ7LZNRBc2Y8YMZGVlYdu2bWKX4jJ69uyJvXv3oqKiAl999RUmTZqELVu2iF2WU8vLy8PMmTOxceNGaDQasctxKaNGjbL9dVRUFOLi4tCpUyd88cUXcHd3F7Gyv3PJOzR+fn5QKBR/68QuLCxEYGCgSFU5F+t1vNQ1DgwMRFFRUbPnGxsbUVpa2mzMhV7j/PdwVY8++ih+/PFHbNq0CaGhobbjgYGBMJlMKC8vbzb+r9f+ctf1YmN0Op3D/YesPalUKnTr1g0xMTFISkpCv3798M477/C621FaWhqKioowYMAAKJVKKJVKbNmyBe+++y6USiUMBgOvfTvR6/Xo0aMHcnJyHO6feZcMNCqVCjExMUhOTrYds1gsSE5ORnx8vIiVOY/OnTsjMDCw2TU2Go3YuXOn7RrHx8ejvLwcaWlptjG//fYbLBYL4uLibGN+//13NDQ02MZs3LgRPXv2hI+PTzt9GsciCAIeffRRrF27Fr/99hs6d+7c7PmYmBi4ubk1u/bZ2dnIzc1tdu0zMzObBcqNGzdCp9MhIiLCNub817CO4b8jzVksFtTX1/O629ENN9yAzMxM7N271/aIjY3F+PHjbX/Na98+qqqqcPToUQQFBTneP/OtaiF2IqtXrxbUarWwYsUK4cCBA8JDDz0k6PX6Zp3YdGmVlZXCnj17hD179ggAhPnz5wt79uwRTp48KQhC07RtvV4vfPfdd0JGRoZw6623XnDadv/+/YWdO3cK27ZtE7p3795s2nZ5eblgMBiECRMmCFlZWcLq1asFrVbr0tO2p0+fLnh7ewubN29uNpWypqbGNmbatGlCx44dhd9++03YvXu3EB8fL8THx9uet06lvOmmm4S9e/cKGzZsEPz9/S84lfKpp54SDh48KCxcuNDlp7DOnj1b2LJli3D8+HEhIyNDmD17tiCTyYRffvlFEARe9/Z0/iwnQeC1t5cnn3xS2Lx5s3D8+HFh+/btQkJCguDn5ycUFRUJguBY191lA40gCMJ7770ndOzYUVCpVMKgQYOEHTt2iF2SpGzatEkA8LfHpEmTBEFomrr9/PPPCwaDQVCr1cINN9wgZGdnN3uNs2fPCuPGjRM8PT0FnU4nJCYmCpWVlc3G7Nu3T7jmmmsEtVothISECK+++mp7fUSHdKFrDkBYvny5bUxtba3wyCOPCD4+PoJWqxVuv/124cyZM81e58SJE8KoUaMEd3d3wc/PT3jyySeFhoaGZmM2bdokREdHCyqVSujSpUuz93BFDzzwgNCpUydBpVIJ/v7+wg033GALM4LA696e/hpoeO3tY+zYsUJQUJCgUqmEkJAQYezYsUJOTo7teUe67jJBEITW3dMhIiIiciwu2UNDREREzoWBhoiIiCSPgYaIiIgkj4GGiIiIJI+BhoiIiCSPgYaIiIgkj4GGiIiIJI+BhoiIiCSPgYaIiIgkj4GGiIiIJI+BhoiIiCSPgYaIiIgk7/8B5AeDHuRgH8oAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_lr_factor(step: int, config: TrainingConfig) -> float:\n",
    "    # TODO: Replace the next line with your own code\n",
    "    if step < config.n_warmup_steps:\n",
    "        return (step/config.n_warmup_steps)\n",
    "    elif step < config.n_warmup_steps + config.n_decay_steps:\n",
    "        # how far we are into the decay\n",
    "        x = (step - config.n_warmup_steps) / config.n_decay_steps\n",
    "        # The final factor at end of cosine decay\n",
    "        final_factor = config.min_lr / config.max_lr\n",
    "        \n",
    "        # Scaled cosine decay from 1 down to the final factor\n",
    "        return final_factor + (1/2) * (1 - final_factor) * (1 + math.cos(math.pi * x))      \n",
    "\n",
    "steps = []\n",
    "for step in range(config.n_steps):\n",
    "    steps.append(get_lr_factor(step, config))\n",
    "\n",
    "plt.plot(steps)\n",
    "#plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** For $x \\in [0, 1]$, the relevant cosine decay is described by the term $\\frac{1}{2} \\cdot (1 + \\text{cos}(\\pi x))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have everything in place to put together a first version of the training loop. Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "torch.set_float32_matmul_precision(precision=\"high\")\n",
    "def train(config: TrainingConfig):\n",
    "    model = configure_model(config)\n",
    "    model = model.to(config.device)\n",
    "    model = torch.compile(model)\n",
    "    batches = make_batches(config)\n",
    "    optimizer = configure_optimizer(model, config)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lambda lr: get_lr_factor(lr, config),\n",
    "    )\n",
    "    n_micro_steps = config.n_tokens_per_step // (\n",
    "        config.batch_size * config.sequence_len\n",
    "    )\n",
    "    for step in range(config.n_steps):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss = 0.0\n",
    "        for micro_step in range(n_micro_steps):\n",
    "            x, y = next(batches)\n",
    "            x, y = x.to(config.device), y.to(config.device)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                logits = model(x)\n",
    "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))/n_micro_steps\n",
    "                loss.backward()\n",
    "            running_loss += loss.item()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm)\n",
    "        optimizer.step()\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        scheduler.step()\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"step {step:4d} | loss: {running_loss:.4f} | lr: {lr:.4e} | Ts/s: {config.n_tokens_per_step/(time.time()-start_time)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the steps should look familiar from previous labs or earlier tasks in this notebook. However, a few key aspects will be new:\n",
    "\n",
    "**Moving the model and data to the training device.** The model and each batch are moved to the training device using `.to()`. In our training configuration, `config.device` is an NVIDIA GPU, which supports fast tensor computations.\n",
    "\n",
    "**Gradient accumulation.** Instead of taking an optimisation step after every batch, we accumulate gradients over multiple batches (‚Äúmicro-steps‚Äù). Each batch contributes to the gradients using `loss.backward()`.\n",
    "\n",
    "**Gradient clipping.** Before updating weights, the gradients are clipped using `clip_grad_norm_()`. This prevents excessively large updates that could destabilise training. The clipping threshold is set by `config.clip_norm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéì Task 3.07: Fixing the training loop\n",
    "\n",
    "Try to train a model by executing the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1049908/2671198488.py:8: FutureWarning: `nn.init.normal` is now deprecated in favor of `nn.init.normal_`.\n",
      "  nn.init.normal(mod_dict[param], mean=0, std=math.sqrt(0.02))\n",
      "/tmp/ipykernel_1049908/2671198488.py:10: FutureWarning: `nn.init.normal` is now deprecated in favor of `nn.init.normal_`.\n",
      "  nn.init.normal(mod_dict[param], mean=0, std=math.sqrt(0.01))\n",
      "/tmp/ipykernel_1049908/2671198488.py:15: FutureWarning: `nn.init.normal` is now deprecated in favor of `nn.init.normal_`.\n",
      "  nn.init.normal(mod_dict[param], mean=0, std=math.sqrt(0.02))\n",
      "/tmp/ipykernel_1049908/2671198488.py:13: FutureWarning: `nn.init.normal` is now deprecated in favor of `nn.init.normal_`.\n",
      "  nn.init.normal(mod_dict[param], mean=0, std=math.sqrt(0.02) * (1/math.sqrt(model.config.n_layer * 2))) # Factor for residual intitialization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 16:35:35.893000 1049908 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0 | loss: 10.9874 | lr: 0.0000e+00 | Ts/s: 11740.933795400488\n",
      "step    1 | loss: 10.9894 | lr: 8.3916e-07 | Ts/s: 46020.70758322623\n",
      "step    2 | loss: 10.9814 | lr: 1.6783e-06 | Ts/s: 47780.80198004678\n",
      "step    3 | loss: 10.9648 | lr: 2.5175e-06 | Ts/s: 47197.01128144733\n",
      "step    4 | loss: 10.9396 | lr: 3.3566e-06 | Ts/s: 47076.283421575994\n",
      "step    5 | loss: 10.9133 | lr: 4.1958e-06 | Ts/s: 47965.29520859033\n",
      "step    6 | loss: 10.8744 | lr: 5.0350e-06 | Ts/s: 48235.07882704233\n",
      "step    7 | loss: 10.8303 | lr: 5.8741e-06 | Ts/s: 48306.92192321992\n",
      "step    8 | loss: 10.7746 | lr: 6.7133e-06 | Ts/s: 48246.587600407074\n",
      "step    9 | loss: 10.7266 | lr: 7.5524e-06 | Ts/s: 48196.906929951525\n",
      "step   10 | loss: 10.6681 | lr: 8.3916e-06 | Ts/s: 48183.68292395567\n",
      "step   11 | loss: 10.6137 | lr: 9.2308e-06 | Ts/s: 48121.968786832396\n",
      "step   12 | loss: 10.5581 | lr: 1.0070e-05 | Ts/s: 48106.88619170862\n",
      "step   13 | loss: 10.5111 | lr: 1.0909e-05 | Ts/s: 47103.29082412323\n",
      "step   14 | loss: 10.4413 | lr: 1.1748e-05 | Ts/s: 46858.67986179499\n",
      "step   15 | loss: 10.3925 | lr: 1.2587e-05 | Ts/s: 47324.811074035024\n",
      "step   16 | loss: 10.3472 | lr: 1.3427e-05 | Ts/s: 48054.03339311361\n",
      "step   17 | loss: 10.2966 | lr: 1.4266e-05 | Ts/s: 48059.17942478688\n",
      "step   18 | loss: 10.2938 | lr: 1.5105e-05 | Ts/s: 48067.203131316746\n",
      "step   19 | loss: 10.2449 | lr: 1.5944e-05 | Ts/s: 48063.74771376131\n",
      "step   20 | loss: 10.1655 | lr: 1.6783e-05 | Ts/s: 48068.47553091277\n",
      "step   21 | loss: 10.1157 | lr: 1.7622e-05 | Ts/s: 48061.51545670622\n",
      "step   22 | loss: 10.0804 | lr: 1.8462e-05 | Ts/s: 45821.48246643575\n",
      "step   23 | loss: 10.0591 | lr: 1.9301e-05 | Ts/s: 45995.226200979865\n",
      "step   24 | loss: 9.9930 | lr: 2.0140e-05 | Ts/s: 46213.93466221543\n",
      "step   25 | loss: 9.9646 | lr: 2.0979e-05 | Ts/s: 46573.48881115004\n",
      "step   26 | loss: 9.9432 | lr: 2.1818e-05 | Ts/s: 46968.99016931704\n",
      "step   27 | loss: 9.9020 | lr: 2.2657e-05 | Ts/s: 47137.86286932575\n",
      "step   28 | loss: 9.8794 | lr: 2.3497e-05 | Ts/s: 47699.04549236965\n",
      "step   29 | loss: 9.8388 | lr: 2.4336e-05 | Ts/s: 47259.55415288027\n",
      "step   30 | loss: 9.8624 | lr: 2.5175e-05 | Ts/s: 47147.27092016885\n",
      "step   31 | loss: 9.8110 | lr: 2.6014e-05 | Ts/s: 45472.744156007706\n",
      "step   32 | loss: 9.7990 | lr: 2.6853e-05 | Ts/s: 46647.54799729494\n",
      "step   33 | loss: 9.7651 | lr: 2.7692e-05 | Ts/s: 47738.68030607921\n",
      "step   34 | loss: 9.6994 | lr: 2.8531e-05 | Ts/s: 48002.762510164735\n",
      "step   35 | loss: 9.7300 | lr: 2.9371e-05 | Ts/s: 48046.48437392565\n",
      "step   36 | loss: 9.7290 | lr: 3.0210e-05 | Ts/s: 47847.096126519355\n",
      "step   37 | loss: 9.6883 | lr: 3.1049e-05 | Ts/s: 48034.640660170284\n",
      "step   38 | loss: 9.6789 | lr: 3.1888e-05 | Ts/s: 48122.688043914386\n",
      "step   39 | loss: 9.6800 | lr: 3.2727e-05 | Ts/s: 47484.892946926855\n",
      "step   40 | loss: 9.6764 | lr: 3.3566e-05 | Ts/s: 48043.32268020878\n",
      "step   41 | loss: 9.6153 | lr: 3.4406e-05 | Ts/s: 48053.21328050027\n",
      "step   42 | loss: 9.6136 | lr: 3.5245e-05 | Ts/s: 48104.49944586347\n",
      "step   43 | loss: 9.6294 | lr: 3.6084e-05 | Ts/s: 47785.123319669336\n",
      "step   44 | loss: 9.6150 | lr: 3.6923e-05 | Ts/s: 47731.15129579589\n",
      "step   45 | loss: 9.6166 | lr: 3.7762e-05 | Ts/s: 48085.739959485516\n",
      "step   46 | loss: 9.5839 | lr: 3.8601e-05 | Ts/s: 47672.64056748356\n",
      "step   47 | loss: 9.5816 | lr: 3.9441e-05 | Ts/s: 47891.45988116684\n",
      "step   48 | loss: 9.5826 | lr: 4.0280e-05 | Ts/s: 47566.20134086384\n",
      "step   49 | loss: 9.5396 | lr: 4.1119e-05 | Ts/s: 45778.18162808392\n",
      "step   50 | loss: 9.5612 | lr: 4.1958e-05 | Ts/s: 45533.1595102887\n",
      "step   51 | loss: 9.5405 | lr: 4.2797e-05 | Ts/s: 44921.31785602275\n",
      "step   52 | loss: 9.5236 | lr: 4.3636e-05 | Ts/s: 45239.0154249422\n",
      "step   53 | loss: 9.5106 | lr: 4.4476e-05 | Ts/s: 46575.67869517975\n",
      "step   54 | loss: 9.5211 | lr: 4.5315e-05 | Ts/s: 47987.605832708505\n",
      "step   55 | loss: 9.5094 | lr: 4.6154e-05 | Ts/s: 47921.79297420335\n",
      "step   56 | loss: 9.5030 | lr: 4.6993e-05 | Ts/s: 47967.81987848761\n",
      "step   57 | loss: 9.4986 | lr: 4.7832e-05 | Ts/s: 47938.93963788324\n",
      "step   58 | loss: 9.4745 | lr: 4.8671e-05 | Ts/s: 47931.36300428426\n",
      "step   59 | loss: 9.4620 | lr: 4.9510e-05 | Ts/s: 46264.086319667105\n",
      "step   60 | loss: 9.4630 | lr: 5.0350e-05 | Ts/s: 46874.309680815335\n",
      "step   61 | loss: 9.4666 | lr: 5.1189e-05 | Ts/s: 47929.62252458323\n",
      "step   62 | loss: 9.4133 | lr: 5.2028e-05 | Ts/s: 47538.5650758458\n",
      "step   63 | loss: 9.3962 | lr: 5.2867e-05 | Ts/s: 47600.58272437052\n",
      "step   64 | loss: 9.3803 | lr: 5.3706e-05 | Ts/s: 47580.191097076604\n",
      "step   65 | loss: 9.3728 | lr: 5.4545e-05 | Ts/s: 47934.57789608055\n",
      "step   66 | loss: 9.3541 | lr: 5.5385e-05 | Ts/s: 47934.155766746015\n",
      "step   67 | loss: 9.3627 | lr: 5.6224e-05 | Ts/s: 47197.233124243285\n",
      "step   68 | loss: 9.3423 | lr: 5.7063e-05 | Ts/s: 47966.26089221231\n",
      "step   69 | loss: 9.2971 | lr: 5.7902e-05 | Ts/s: 47872.25230931104\n",
      "step   70 | loss: 9.2712 | lr: 5.8741e-05 | Ts/s: 46978.547705965\n",
      "step   71 | loss: 9.2933 | lr: 5.9580e-05 | Ts/s: 46322.42403443993\n",
      "step   72 | loss: 9.2466 | lr: 6.0420e-05 | Ts/s: 46186.431619043724\n",
      "step   73 | loss: 9.2209 | lr: 6.1259e-05 | Ts/s: 45850.42709569369\n",
      "step   74 | loss: 9.2106 | lr: 6.2098e-05 | Ts/s: 46733.62125144641\n",
      "step   75 | loss: 9.2133 | lr: 6.2937e-05 | Ts/s: 46910.49360329173\n",
      "step   76 | loss: 9.1824 | lr: 6.3776e-05 | Ts/s: 46636.65782280412\n",
      "step   77 | loss: 9.1954 | lr: 6.4615e-05 | Ts/s: 45826.69622006485\n",
      "step   78 | loss: 9.1655 | lr: 6.5455e-05 | Ts/s: 46868.449274796745\n",
      "step   79 | loss: 9.1446 | lr: 6.6294e-05 | Ts/s: 46547.14152247295\n",
      "step   80 | loss: 9.1091 | lr: 6.7133e-05 | Ts/s: 46919.31856271623\n",
      "step   81 | loss: 9.1253 | lr: 6.7972e-05 | Ts/s: 45611.196755288205\n",
      "step   82 | loss: 9.0786 | lr: 6.8811e-05 | Ts/s: 46180.88547692033\n",
      "step   83 | loss: 9.0401 | lr: 6.9650e-05 | Ts/s: 46334.190072426376\n",
      "step   84 | loss: 9.0514 | lr: 7.0490e-05 | Ts/s: 46596.77821865431\n",
      "step   85 | loss: 9.0132 | lr: 7.1329e-05 | Ts/s: 46922.362076856094\n",
      "step   86 | loss: 9.0100 | lr: 7.2168e-05 | Ts/s: 46454.82270958968\n",
      "step   87 | loss: 9.0007 | lr: 7.3007e-05 | Ts/s: 46612.484742245964\n",
      "step   88 | loss: 9.0006 | lr: 7.3846e-05 | Ts/s: 47146.82413222085\n",
      "step   89 | loss: 9.0249 | lr: 7.4685e-05 | Ts/s: 47456.486661275085\n",
      "step   90 | loss: 8.9503 | lr: 7.5524e-05 | Ts/s: 46795.27482850905\n",
      "step   91 | loss: 8.9468 | lr: 7.6364e-05 | Ts/s: 46658.74718773688\n",
      "step   92 | loss: 8.9598 | lr: 7.7203e-05 | Ts/s: 47039.97117889948\n",
      "step   93 | loss: 9.0058 | lr: 7.8042e-05 | Ts/s: 46147.98230840637\n",
      "step   94 | loss: 9.0073 | lr: 7.8881e-05 | Ts/s: 45211.749230287875\n",
      "step   95 | loss: 8.9932 | lr: 7.9720e-05 | Ts/s: 44803.90533424073\n",
      "step   96 | loss: 8.9709 | lr: 8.0559e-05 | Ts/s: 45326.82782163012\n",
      "step   97 | loss: 8.9183 | lr: 8.1399e-05 | Ts/s: 45583.60683143242\n",
      "step   98 | loss: 8.9154 | lr: 8.2238e-05 | Ts/s: 44710.986291195404\n",
      "step   99 | loss: 8.8855 | lr: 8.3077e-05 | Ts/s: 46440.92481281116\n",
      "step  100 | loss: 8.9184 | lr: 8.3916e-05 | Ts/s: 46287.38193952393\n",
      "step  101 | loss: 8.8751 | lr: 8.4755e-05 | Ts/s: 46757.88139562486\n",
      "step  102 | loss: 8.8576 | lr: 8.5594e-05 | Ts/s: 45295.658061646885\n",
      "step  103 | loss: 8.7668 | lr: 8.6434e-05 | Ts/s: 47857.425804189945\n",
      "step  104 | loss: 8.8348 | lr: 8.7273e-05 | Ts/s: 47928.28434253562\n",
      "step  105 | loss: 8.8286 | lr: 8.8112e-05 | Ts/s: 47870.69119196216\n",
      "step  106 | loss: 8.7689 | lr: 8.8951e-05 | Ts/s: 47028.10248108577\n",
      "step  107 | loss: 8.7567 | lr: 8.9790e-05 | Ts/s: 47335.49519189249\n",
      "step  108 | loss: 8.7822 | lr: 9.0629e-05 | Ts/s: 46791.98392534674\n",
      "step  109 | loss: 8.8086 | lr: 9.1469e-05 | Ts/s: 47823.252858284475\n",
      "step  110 | loss: 8.7406 | lr: 9.2308e-05 | Ts/s: 47623.39066318974\n",
      "step  111 | loss: 8.7097 | lr: 9.3147e-05 | Ts/s: 47593.53292218064\n",
      "step  112 | loss: 8.6884 | lr: 9.3986e-05 | Ts/s: 47895.247331294806\n",
      "step  113 | loss: 8.7354 | lr: 9.4825e-05 | Ts/s: 47923.33862726258\n",
      "step  114 | loss: 8.7074 | lr: 9.5664e-05 | Ts/s: 47241.65987932859\n",
      "step  115 | loss: 8.6788 | lr: 9.6503e-05 | Ts/s: 47529.75426365377\n",
      "step  116 | loss: 8.6137 | lr: 9.7343e-05 | Ts/s: 46104.602827136216\n",
      "step  117 | loss: 8.5365 | lr: 9.8182e-05 | Ts/s: 47883.20904847641\n",
      "step  118 | loss: 8.5577 | lr: 9.9021e-05 | Ts/s: 47487.8051796576\n",
      "step  119 | loss: 8.5788 | lr: 9.9860e-05 | Ts/s: 47107.717534658776\n",
      "step  120 | loss: 8.5450 | lr: 1.0070e-04 | Ts/s: 46737.05294886538\n",
      "step  121 | loss: 8.5315 | lr: 1.0154e-04 | Ts/s: 47193.33952896566\n",
      "step  122 | loss: 8.5059 | lr: 1.0238e-04 | Ts/s: 47831.760826672\n",
      "step  123 | loss: 8.4812 | lr: 1.0322e-04 | Ts/s: 47972.099764419996\n",
      "step  124 | loss: 8.4917 | lr: 1.0406e-04 | Ts/s: 47018.48858073253\n",
      "step  125 | loss: 8.4785 | lr: 1.0490e-04 | Ts/s: 47644.941421759846\n",
      "step  126 | loss: 8.4713 | lr: 1.0573e-04 | Ts/s: 47863.01318558743\n",
      "step  127 | loss: 8.4342 | lr: 1.0657e-04 | Ts/s: 47937.1651679903\n",
      "step  128 | loss: 8.3917 | lr: 1.0741e-04 | Ts/s: 47915.06427919837\n",
      "step  129 | loss: 8.3630 | lr: 1.0825e-04 | Ts/s: 47902.31689341315\n",
      "step  130 | loss: 8.3973 | lr: 1.0909e-04 | Ts/s: 47899.74068526531\n",
      "step  131 | loss: 8.3583 | lr: 1.0993e-04 | Ts/s: 47905.15219042303\n",
      "step  132 | loss: 8.3449 | lr: 1.1077e-04 | Ts/s: 47908.79150473474\n",
      "step  133 | loss: 8.2656 | lr: 1.1161e-04 | Ts/s: 47836.982154688456\n",
      "step  134 | loss: 8.2777 | lr: 1.1245e-04 | Ts/s: 47855.26474164317\n",
      "step  135 | loss: 8.3248 | lr: 1.1329e-04 | Ts/s: 47941.29221480454\n",
      "step  136 | loss: 8.2995 | lr: 1.1413e-04 | Ts/s: 47899.372380050205\n",
      "step  137 | loss: 8.2847 | lr: 1.1497e-04 | Ts/s: 47917.46776428394\n",
      "step  138 | loss: 8.2513 | lr: 1.1580e-04 | Ts/s: 47886.98059430577\n",
      "step  139 | loss: 8.2933 | lr: 1.1664e-04 | Ts/s: 47915.13422952789\n",
      "step  140 | loss: 8.3504 | lr: 1.1748e-04 | Ts/s: 47951.553863635425\n",
      "step  141 | loss: 8.2501 | lr: 1.1832e-04 | Ts/s: 47852.4728336616\n",
      "step  142 | loss: 8.3112 | lr: 1.1916e-04 | Ts/s: 47929.63819460487\n",
      "step  143 | loss: 8.2386 | lr: 1.2000e-04 | Ts/s: 47908.269630467315\n",
      "step  144 | loss: 8.2369 | lr: 1.2084e-04 | Ts/s: 47926.221327705745\n",
      "step  145 | loss: 8.2342 | lr: 1.2168e-04 | Ts/s: 47888.2914420543\n",
      "step  146 | loss: 8.2197 | lr: 1.2252e-04 | Ts/s: 47941.31938940977\n",
      "step  147 | loss: 8.2063 | lr: 1.2336e-04 | Ts/s: 47852.31039026095\n",
      "step  148 | loss: 8.1832 | lr: 1.2420e-04 | Ts/s: 47961.616969273135\n",
      "step  149 | loss: 8.1730 | lr: 1.2503e-04 | Ts/s: 47930.35693727691\n",
      "step  150 | loss: 8.1679 | lr: 1.2587e-04 | Ts/s: 47935.3688861634\n",
      "step  151 | loss: 8.0937 | lr: 1.2671e-04 | Ts/s: 47567.97417694161\n",
      "step  152 | loss: 8.0959 | lr: 1.2755e-04 | Ts/s: 47853.573520884864\n",
      "step  153 | loss: 8.1440 | lr: 1.2839e-04 | Ts/s: 45814.49446004658\n",
      "step  154 | loss: 8.0613 | lr: 1.2923e-04 | Ts/s: 46524.757056211936\n",
      "step  155 | loss: 8.0122 | lr: 1.3007e-04 | Ts/s: 46150.960473432075\n",
      "step  156 | loss: 8.0661 | lr: 1.3091e-04 | Ts/s: 46136.295151844184\n",
      "step  157 | loss: 8.0746 | lr: 1.3175e-04 | Ts/s: 45717.79147672622\n",
      "step  158 | loss: 8.0487 | lr: 1.3259e-04 | Ts/s: 44948.17321031768\n",
      "step  159 | loss: 7.9877 | lr: 1.3343e-04 | Ts/s: 46390.42642418\n",
      "step  160 | loss: 7.9392 | lr: 1.3427e-04 | Ts/s: 45800.139459887316\n",
      "step  161 | loss: 7.9886 | lr: 1.3510e-04 | Ts/s: 46643.89000142263\n",
      "step  162 | loss: 7.8958 | lr: 1.3594e-04 | Ts/s: 46324.88411111198\n",
      "step  163 | loss: 7.8558 | lr: 1.3678e-04 | Ts/s: 46697.95450792418\n",
      "step  164 | loss: 7.8767 | lr: 1.3762e-04 | Ts/s: 46691.02874073143\n",
      "step  165 | loss: 7.8827 | lr: 1.3846e-04 | Ts/s: 46372.80374011792\n",
      "step  166 | loss: 7.7216 | lr: 1.3930e-04 | Ts/s: 46702.09508505468\n",
      "step  167 | loss: 7.8108 | lr: 1.4014e-04 | Ts/s: 47004.504615958125\n",
      "step  168 | loss: 7.7826 | lr: 1.4098e-04 | Ts/s: 46700.92870689108\n",
      "step  169 | loss: 7.8130 | lr: 1.4182e-04 | Ts/s: 46571.10877885766\n",
      "step  170 | loss: 7.7696 | lr: 1.4266e-04 | Ts/s: 46888.399232309195\n",
      "step  171 | loss: 7.7782 | lr: 1.4350e-04 | Ts/s: 46482.30744510384\n",
      "step  172 | loss: 7.7629 | lr: 1.4434e-04 | Ts/s: 46321.83564546152\n",
      "step  173 | loss: 7.7082 | lr: 1.4517e-04 | Ts/s: 46864.247188963425\n",
      "step  174 | loss: 7.6779 | lr: 1.4601e-04 | Ts/s: 46155.85615283904\n",
      "step  175 | loss: 7.7250 | lr: 1.4685e-04 | Ts/s: 46903.244547262206\n",
      "step  176 | loss: 7.6665 | lr: 1.4769e-04 | Ts/s: 46440.32752506632\n",
      "step  177 | loss: 7.6759 | lr: 1.4853e-04 | Ts/s: 46350.988135115644\n",
      "step  178 | loss: 7.6304 | lr: 1.4937e-04 | Ts/s: 46228.5697013296\n",
      "step  179 | loss: 7.5983 | lr: 1.5021e-04 | Ts/s: 45984.019175990565\n",
      "step  180 | loss: 7.5820 | lr: 1.5105e-04 | Ts/s: 46471.78300903494\n",
      "step  181 | loss: 7.6017 | lr: 1.5189e-04 | Ts/s: 47331.51966389682\n",
      "step  182 | loss: 7.5922 | lr: 1.5273e-04 | Ts/s: 46486.33616461511\n",
      "step  183 | loss: 7.5713 | lr: 1.5357e-04 | Ts/s: 47222.946569293505\n",
      "step  184 | loss: 7.5687 | lr: 1.5441e-04 | Ts/s: 47139.45638496327\n",
      "step  185 | loss: 7.5933 | lr: 1.5524e-04 | Ts/s: 47293.436552342784\n",
      "step  186 | loss: 7.6147 | lr: 1.5608e-04 | Ts/s: 47389.10056076294\n",
      "step  187 | loss: 7.6678 | lr: 1.5692e-04 | Ts/s: 46494.185329360036\n",
      "step  188 | loss: 7.6262 | lr: 1.5776e-04 | Ts/s: 46301.298238710086\n",
      "step  189 | loss: 7.5729 | lr: 1.5860e-04 | Ts/s: 46808.76888518018\n",
      "hello!\n",
      "Excess 0\n",
      "step  190 | loss: 7.5958 | lr: 1.5944e-04 | Ts/s: 45940.193844509486\n",
      "step  191 | loss: 7.5585 | lr: 1.6028e-04 | Ts/s: 46599.86890618592\n",
      "step  192 | loss: 7.5512 | lr: 1.6112e-04 | Ts/s: 46435.43603219963\n",
      "step  193 | loss: 7.5789 | lr: 1.6196e-04 | Ts/s: 46895.41768270376\n",
      "step  194 | loss: 7.4927 | lr: 1.6280e-04 | Ts/s: 46082.07887014224\n",
      "step  195 | loss: 7.4737 | lr: 1.6364e-04 | Ts/s: 46711.60383523915\n",
      "step  196 | loss: 7.4633 | lr: 1.6448e-04 | Ts/s: 46016.72642294968\n",
      "step  197 | loss: 7.4680 | lr: 1.6531e-04 | Ts/s: 46083.13341942413\n",
      "step  198 | loss: 7.4947 | lr: 1.6615e-04 | Ts/s: 45808.4332319566\n",
      "step  199 | loss: 7.4479 | lr: 1.6699e-04 | Ts/s: 46067.29708475233\n",
      "step  200 | loss: 7.4227 | lr: 1.6783e-04 | Ts/s: 46198.43827299501\n",
      "step  201 | loss: 7.3912 | lr: 1.6867e-04 | Ts/s: 46947.53224800213\n",
      "step  202 | loss: 7.4036 | lr: 1.6951e-04 | Ts/s: 46798.68869613784\n",
      "step  203 | loss: 7.3976 | lr: 1.7035e-04 | Ts/s: 47273.62221801491\n",
      "step  204 | loss: 7.3698 | lr: 1.7119e-04 | Ts/s: 47229.855558539755\n",
      "step  205 | loss: 7.4422 | lr: 1.7203e-04 | Ts/s: 47029.561852982784\n",
      "step  206 | loss: 7.4151 | lr: 1.7287e-04 | Ts/s: 46296.01590157643\n",
      "step  207 | loss: 7.3577 | lr: 1.7371e-04 | Ts/s: 46746.733945084794\n",
      "step  208 | loss: 7.2887 | lr: 1.7455e-04 | Ts/s: 45381.53374035949\n",
      "step  209 | loss: 7.2410 | lr: 1.7538e-04 | Ts/s: 45151.85132956423\n",
      "step  210 | loss: 7.2383 | lr: 1.7622e-04 | Ts/s: 46207.31288557036\n",
      "step  211 | loss: 7.2852 | lr: 1.7706e-04 | Ts/s: 47224.016458093625\n",
      "step  212 | loss: 7.3155 | lr: 1.7790e-04 | Ts/s: 45919.44297928376\n",
      "step  213 | loss: 7.2920 | lr: 1.7874e-04 | Ts/s: 46807.16775871239\n",
      "step  214 | loss: 7.2748 | lr: 1.7958e-04 | Ts/s: 47912.95542486358\n",
      "step  215 | loss: 7.2869 | lr: 1.8042e-04 | Ts/s: 47897.31289663658\n",
      "step  216 | loss: 7.2234 | lr: 1.8126e-04 | Ts/s: 47911.005421795184\n",
      "step  217 | loss: 7.2171 | lr: 1.8210e-04 | Ts/s: 47982.85307679209\n",
      "step  218 | loss: 7.1837 | lr: 1.8294e-04 | Ts/s: 47932.22075416181\n",
      "step  219 | loss: 7.1747 | lr: 1.8378e-04 | Ts/s: 47904.62830828174\n",
      "step  220 | loss: 7.1393 | lr: 1.8462e-04 | Ts/s: 47903.79867773408\n",
      "step  221 | loss: 7.1289 | lr: 1.8545e-04 | Ts/s: 47840.98788772111\n",
      "step  222 | loss: 7.1730 | lr: 1.8629e-04 | Ts/s: 47879.54338753049\n",
      "step  223 | loss: 7.1012 | lr: 1.8713e-04 | Ts/s: 47873.90107649827\n",
      "step  224 | loss: 7.0933 | lr: 1.8797e-04 | Ts/s: 47936.85376102931\n",
      "step  225 | loss: 7.0699 | lr: 1.8881e-04 | Ts/s: 47961.55838982079\n",
      "step  226 | loss: 7.0613 | lr: 1.8965e-04 | Ts/s: 47912.65163985798\n",
      "step  227 | loss: 7.1457 | lr: 1.9049e-04 | Ts/s: 47939.54996884398\n",
      "step  228 | loss: 7.0802 | lr: 1.9133e-04 | Ts/s: 47840.81511428308\n",
      "step  229 | loss: 7.0575 | lr: 1.9217e-04 | Ts/s: 47896.51690263057\n",
      "step  230 | loss: 7.0754 | lr: 1.9301e-04 | Ts/s: 47952.007668751125\n",
      "step  231 | loss: 7.0202 | lr: 1.9385e-04 | Ts/s: 47911.90941847105\n",
      "step  232 | loss: 7.1943 | lr: 1.9469e-04 | Ts/s: 47926.23490646321\n",
      "step  233 | loss: 7.1775 | lr: 1.9552e-04 | Ts/s: 47869.21562324091\n",
      "step  234 | loss: 7.1603 | lr: 1.9636e-04 | Ts/s: 47938.27811519758\n",
      "step  235 | loss: 7.1769 | lr: 1.9720e-04 | Ts/s: 47864.74570543607\n",
      "step  236 | loss: 7.1867 | lr: 1.9804e-04 | Ts/s: 47914.23533114335\n",
      "step  237 | loss: 7.1733 | lr: 1.9888e-04 | Ts/s: 47432.65142809812\n",
      "step  238 | loss: 7.1255 | lr: 1.9972e-04 | Ts/s: 47514.5097720583\n",
      "step  239 | loss: 7.0882 | lr: 2.0056e-04 | Ts/s: 47231.92093954058\n",
      "step  240 | loss: 7.1477 | lr: 2.0140e-04 | Ts/s: 47924.02167058191\n",
      "step  241 | loss: 7.1380 | lr: 2.0224e-04 | Ts/s: 47862.432928866314\n",
      "step  242 | loss: 7.0974 | lr: 2.0308e-04 | Ts/s: 47840.47685664719\n",
      "step  243 | loss: 7.1079 | lr: 2.0392e-04 | Ts/s: 47893.62734334354\n",
      "step  244 | loss: 7.0219 | lr: 2.0476e-04 | Ts/s: 47941.4510821638\n",
      "step  245 | loss: 7.0110 | lr: 2.0559e-04 | Ts/s: 46896.59379613036\n",
      "step  246 | loss: 7.0207 | lr: 2.0643e-04 | Ts/s: 47876.10343131959\n",
      "step  247 | loss: 7.0774 | lr: 2.0727e-04 | Ts/s: 47866.54086581755\n",
      "step  248 | loss: 7.0521 | lr: 2.0811e-04 | Ts/s: 47914.91498290955\n",
      "step  249 | loss: 7.0028 | lr: 2.0895e-04 | Ts/s: 47379.13741217931\n",
      "step  250 | loss: 7.0461 | lr: 2.0979e-04 | Ts/s: 47922.70259984341\n",
      "step  251 | loss: 6.9698 | lr: 2.1063e-04 | Ts/s: 46312.59316761056\n",
      "step  252 | loss: 7.0605 | lr: 2.1147e-04 | Ts/s: 46221.28016903988\n",
      "step  253 | loss: 6.9882 | lr: 2.1231e-04 | Ts/s: 46569.17475026141\n",
      "step  254 | loss: 6.9976 | lr: 2.1315e-04 | Ts/s: 46545.06958736041\n",
      "step  255 | loss: 6.9272 | lr: 2.1399e-04 | Ts/s: 46471.51391944486\n",
      "step  256 | loss: 6.9576 | lr: 2.1483e-04 | Ts/s: 44313.20320500165\n",
      "step  257 | loss: 6.9437 | lr: 2.1566e-04 | Ts/s: 45891.01259696017\n",
      "step  258 | loss: 6.8941 | lr: 2.1650e-04 | Ts/s: 46047.407043319305\n",
      "step  259 | loss: 6.9265 | lr: 2.1734e-04 | Ts/s: 45858.845246698584\n",
      "step  260 | loss: 6.8268 | lr: 2.1818e-04 | Ts/s: 45618.29794997188\n",
      "step  261 | loss: 6.8949 | lr: 2.1902e-04 | Ts/s: 45353.84203619809\n",
      "step  262 | loss: 6.8522 | lr: 2.1986e-04 | Ts/s: 46169.91157268404\n",
      "step  263 | loss: 6.8280 | lr: 2.2070e-04 | Ts/s: 47874.79637755995\n",
      "step  264 | loss: 6.8633 | lr: 2.2154e-04 | Ts/s: 46433.008320183435\n",
      "step  265 | loss: 6.8793 | lr: 2.2238e-04 | Ts/s: 47872.63999909437\n",
      "step  266 | loss: 6.9098 | lr: 2.2322e-04 | Ts/s: 47857.12480594838\n",
      "step  267 | loss: 6.8061 | lr: 2.2406e-04 | Ts/s: 47173.69902850452\n",
      "step  268 | loss: 6.8053 | lr: 2.2490e-04 | Ts/s: 46303.64297274582\n",
      "step  269 | loss: 6.8583 | lr: 2.2573e-04 | Ts/s: 46388.93794698191\n",
      "step  270 | loss: 6.8437 | lr: 2.2657e-04 | Ts/s: 46855.378038189694\n",
      "step  271 | loss: 6.7954 | lr: 2.2741e-04 | Ts/s: 46648.832439208345\n",
      "step  272 | loss: 6.7690 | lr: 2.2825e-04 | Ts/s: 46619.548324996846\n",
      "step  273 | loss: 6.7633 | lr: 2.2909e-04 | Ts/s: 47644.50579730439\n",
      "step  274 | loss: 6.7339 | lr: 2.2993e-04 | Ts/s: 47891.41816100903\n",
      "step  275 | loss: 6.7995 | lr: 2.3077e-04 | Ts/s: 47841.60821709548\n",
      "step  276 | loss: 6.8768 | lr: 2.3161e-04 | Ts/s: 47908.66416636483\n",
      "step  277 | loss: 6.7424 | lr: 2.3245e-04 | Ts/s: 47864.688404217304\n",
      "step  278 | loss: 6.8893 | lr: 2.3329e-04 | Ts/s: 47941.28907927515\n",
      "step  279 | loss: 6.8409 | lr: 2.3413e-04 | Ts/s: 47719.77938878011\n",
      "step  280 | loss: 6.8881 | lr: 2.3497e-04 | Ts/s: 46598.15169722702\n",
      "step  281 | loss: 6.8583 | lr: 2.3580e-04 | Ts/s: 47859.03709445309\n",
      "step  282 | loss: 6.8577 | lr: 2.3664e-04 | Ts/s: 47524.440585590004\n",
      "step  283 | loss: 6.8719 | lr: 2.3748e-04 | Ts/s: 46555.31381902284\n",
      "step  284 | loss: 6.9089 | lr: 2.3832e-04 | Ts/s: 46332.359624221775\n",
      "step  285 | loss: 6.8614 | lr: 2.3916e-04 | Ts/s: 46542.05905843341\n",
      "step  286 | loss: 6.8675 | lr: 2.4000e-04 | Ts/s: 46288.56770039796\n",
      "step  287 | loss: 6.8257 | lr: 2.4084e-04 | Ts/s: 47712.714942845916\n",
      "step  288 | loss: 6.8011 | lr: 2.4168e-04 | Ts/s: 46986.73967786069\n",
      "step  289 | loss: 6.8376 | lr: 2.4252e-04 | Ts/s: 46212.619672439\n",
      "step  290 | loss: 6.7404 | lr: 2.4336e-04 | Ts/s: 47452.313638393025\n",
      "step  291 | loss: 6.7777 | lr: 2.4420e-04 | Ts/s: 47814.14491724816\n",
      "step  292 | loss: 6.6934 | lr: 2.4503e-04 | Ts/s: 47055.41911142376\n",
      "step  293 | loss: 6.7426 | lr: 2.4587e-04 | Ts/s: 47746.01990547213\n",
      "step  294 | loss: 6.8446 | lr: 2.4671e-04 | Ts/s: 47846.691152396146\n",
      "step  295 | loss: 6.7783 | lr: 2.4755e-04 | Ts/s: 47754.95049644753\n",
      "step  296 | loss: 6.7495 | lr: 2.4839e-04 | Ts/s: 47815.47361404162\n",
      "step  297 | loss: 6.7763 | lr: 2.4923e-04 | Ts/s: 46398.631888921285\n",
      "step  298 | loss: 6.7083 | lr: 2.5007e-04 | Ts/s: 46719.54315934011\n",
      "step  299 | loss: 6.7652 | lr: 2.5091e-04 | Ts/s: 45150.77871338437\n",
      "step  300 | loss: 6.8394 | lr: 2.5175e-04 | Ts/s: 45475.06590768709\n",
      "step  301 | loss: 6.6838 | lr: 2.5259e-04 | Ts/s: 46386.93878176446\n",
      "step  302 | loss: 6.6754 | lr: 2.5343e-04 | Ts/s: 47205.532961295845\n",
      "step  303 | loss: 6.6856 | lr: 2.5427e-04 | Ts/s: 46761.98487645365\n",
      "step  304 | loss: 6.6888 | lr: 2.5510e-04 | Ts/s: 45915.85130286886\n",
      "step  305 | loss: 6.6537 | lr: 2.5594e-04 | Ts/s: 46122.07445525823\n",
      "step  306 | loss: 6.6400 | lr: 2.5678e-04 | Ts/s: 46612.859212551586\n",
      "step  307 | loss: 6.6225 | lr: 2.5762e-04 | Ts/s: 46772.47703338269\n",
      "step  308 | loss: 6.6269 | lr: 2.5846e-04 | Ts/s: 46718.55456640785\n",
      "step  309 | loss: 6.6512 | lr: 2.5930e-04 | Ts/s: 46892.69364653369\n",
      "step  310 | loss: 6.6084 | lr: 2.6014e-04 | Ts/s: 46200.92499517774\n",
      "step  311 | loss: 6.5893 | lr: 2.6098e-04 | Ts/s: 45777.90907525063\n",
      "step  312 | loss: 6.5832 | lr: 2.6182e-04 | Ts/s: 46120.72889763018\n",
      "step  313 | loss: 6.6249 | lr: 2.6266e-04 | Ts/s: 46478.541715180814\n",
      "step  314 | loss: 6.5696 | lr: 2.6350e-04 | Ts/s: 46585.5554602905\n",
      "step  315 | loss: 6.5569 | lr: 2.6434e-04 | Ts/s: 46479.78641151288\n",
      "step  316 | loss: 6.5961 | lr: 2.6517e-04 | Ts/s: 46702.90146728601\n",
      "step  317 | loss: 6.5727 | lr: 2.6601e-04 | Ts/s: 46597.89299126363\n",
      "step  318 | loss: 6.6101 | lr: 2.6685e-04 | Ts/s: 46513.59945864581\n",
      "step  319 | loss: 6.5514 | lr: 2.6769e-04 | Ts/s: 46735.863965091674\n",
      "step  320 | loss: 6.5022 | lr: 2.6853e-04 | Ts/s: 46866.82708248221\n",
      "step  321 | loss: 6.5359 | lr: 2.6937e-04 | Ts/s: 46421.60852892091\n",
      "step  322 | loss: 6.4959 | lr: 2.7021e-04 | Ts/s: 46632.308318789255\n",
      "step  323 | loss: 6.5297 | lr: 2.7105e-04 | Ts/s: 46723.83846598868\n",
      "step  324 | loss: 6.6350 | lr: 2.7189e-04 | Ts/s: 46013.523889774355\n",
      "step  325 | loss: 6.6964 | lr: 2.7273e-04 | Ts/s: 45958.00977781324\n",
      "step  326 | loss: 6.6835 | lr: 2.7357e-04 | Ts/s: 46685.53024259426\n",
      "step  327 | loss: 6.7015 | lr: 2.7441e-04 | Ts/s: 46650.89185001415\n",
      "step  328 | loss: 6.6951 | lr: 2.7524e-04 | Ts/s: 46775.96319393875\n",
      "step  329 | loss: 6.6861 | lr: 2.7608e-04 | Ts/s: 46797.02053931219\n",
      "step  330 | loss: 6.6865 | lr: 2.7692e-04 | Ts/s: 46926.54254657005\n",
      "step  331 | loss: 6.6527 | lr: 2.7776e-04 | Ts/s: 46797.763476857865\n",
      "step  332 | loss: 6.6719 | lr: 2.7860e-04 | Ts/s: 46882.70022463164\n",
      "step  333 | loss: 6.6342 | lr: 2.7944e-04 | Ts/s: 46917.968129787\n",
      "step  334 | loss: 6.7223 | lr: 2.8028e-04 | Ts/s: 47229.98844353039\n",
      "step  335 | loss: 6.6456 | lr: 2.8112e-04 | Ts/s: 44694.92601801729\n",
      "step  336 | loss: 6.5569 | lr: 2.8196e-04 | Ts/s: 45323.72713145867\n",
      "step  337 | loss: 6.6215 | lr: 2.8280e-04 | Ts/s: 44782.57932408753\n",
      "step  338 | loss: 6.6147 | lr: 2.8364e-04 | Ts/s: 44686.49747140907\n",
      "step  339 | loss: 6.5996 | lr: 2.8448e-04 | Ts/s: 44319.77464654749\n",
      "step  340 | loss: 6.5853 | lr: 2.8531e-04 | Ts/s: 44611.58701703345\n",
      "step  341 | loss: 6.5991 | lr: 2.8615e-04 | Ts/s: 44500.84463798319\n",
      "step  342 | loss: 6.5959 | lr: 2.8699e-04 | Ts/s: 44647.91301338491\n",
      "step  343 | loss: 6.5134 | lr: 2.8783e-04 | Ts/s: 46489.250051805684\n",
      "step  344 | loss: 6.5729 | lr: 2.8867e-04 | Ts/s: 46613.642754318666\n",
      "step  345 | loss: 6.5107 | lr: 2.8951e-04 | Ts/s: 46545.25578796517\n",
      "step  346 | loss: 6.5286 | lr: 2.9035e-04 | Ts/s: 46378.74038638882\n",
      "step  347 | loss: 6.5608 | lr: 2.9119e-04 | Ts/s: 45555.328299153356\n",
      "step  348 | loss: 6.4580 | lr: 2.9203e-04 | Ts/s: 46530.75039726345\n",
      "step  349 | loss: 6.4809 | lr: 2.9287e-04 | Ts/s: 46812.89027204367\n",
      "step  350 | loss: 6.4856 | lr: 2.9371e-04 | Ts/s: 45839.04112060366\n",
      "step  351 | loss: 6.4972 | lr: 2.9455e-04 | Ts/s: 46171.06320949072\n",
      "step  352 | loss: 6.4481 | lr: 2.9538e-04 | Ts/s: 46244.57061680499\n",
      "step  353 | loss: 6.4587 | lr: 2.9622e-04 | Ts/s: 46289.62587520201\n",
      "step  354 | loss: 6.4960 | lr: 2.9706e-04 | Ts/s: 46428.56536262121\n",
      "step  355 | loss: 6.4817 | lr: 2.9790e-04 | Ts/s: 45615.55845303367\n",
      "step  356 | loss: 6.4158 | lr: 2.9874e-04 | Ts/s: 46309.31226306477\n",
      "step  357 | loss: 6.4307 | lr: 2.9958e-04 | Ts/s: 46588.12351319464\n",
      "step  358 | loss: 6.4397 | lr: 3.0042e-04 | Ts/s: 46710.38638006661\n",
      "step  359 | loss: 6.3955 | lr: 3.0126e-04 | Ts/s: 46598.80439946548\n",
      "step  360 | loss: 6.4072 | lr: 3.0210e-04 | Ts/s: 46693.69866074163\n",
      "step  361 | loss: 6.4376 | lr: 3.0294e-04 | Ts/s: 46820.042640322696\n",
      "step  362 | loss: 6.4549 | lr: 3.0378e-04 | Ts/s: 46069.79190756943\n",
      "step  363 | loss: 6.3626 | lr: 3.0462e-04 | Ts/s: 46412.54660231565\n",
      "step  364 | loss: 6.4344 | lr: 3.0545e-04 | Ts/s: 46213.64135668198\n",
      "step  365 | loss: 6.3464 | lr: 3.0629e-04 | Ts/s: 46593.20814559319\n",
      "step  366 | loss: 6.3461 | lr: 3.0713e-04 | Ts/s: 46324.093657352794\n",
      "step  367 | loss: 6.4230 | lr: 3.0797e-04 | Ts/s: 46460.92467443258\n",
      "step  368 | loss: 6.4266 | lr: 3.0881e-04 | Ts/s: 46856.947519537905\n",
      "step  369 | loss: 6.3424 | lr: 3.0965e-04 | Ts/s: 46251.165138490906\n",
      "step  370 | loss: 6.3789 | lr: 3.1049e-04 | Ts/s: 46790.8538732406\n",
      "step  371 | loss: 6.4736 | lr: 3.1133e-04 | Ts/s: 46552.99773158681\n",
      "step  372 | loss: 6.5543 | lr: 3.1217e-04 | Ts/s: 46377.09616462307\n",
      "step  373 | loss: 6.4782 | lr: 3.1301e-04 | Ts/s: 47001.748804620736\n",
      "step  374 | loss: 6.5291 | lr: 3.1385e-04 | Ts/s: 46903.8888170751\n",
      "step  375 | loss: 6.5123 | lr: 3.1469e-04 | Ts/s: 46643.12720793163\n",
      "step  376 | loss: 6.5293 | lr: 3.1552e-04 | Ts/s: 45825.55692314835\n",
      "step  377 | loss: 6.5234 | lr: 3.1636e-04 | Ts/s: 46630.67276708448\n",
      "step  378 | loss: 6.5573 | lr: 3.1720e-04 | Ts/s: 47286.2598301966\n",
      "step  379 | loss: 6.4292 | lr: 3.1804e-04 | Ts/s: 46779.22895610032\n",
      "step  380 | loss: 6.4773 | lr: 3.1888e-04 | Ts/s: 46565.52509108821\n",
      "hello!\n",
      "Excess 0\n",
      "step  381 | loss: 6.4573 | lr: 3.1972e-04 | Ts/s: 46133.43695488802\n",
      "step  382 | loss: 6.4497 | lr: 3.2056e-04 | Ts/s: 46885.82496062773\n",
      "step  383 | loss: 6.4099 | lr: 3.2140e-04 | Ts/s: 46386.5102019606\n",
      "step  384 | loss: 6.3848 | lr: 3.2224e-04 | Ts/s: 46202.047116723355\n",
      "step  385 | loss: 6.4813 | lr: 3.2308e-04 | Ts/s: 46399.969234803604\n",
      "step  386 | loss: 6.4740 | lr: 3.2392e-04 | Ts/s: 46150.78516266479\n",
      "step  387 | loss: 6.4622 | lr: 3.2476e-04 | Ts/s: 47528.88414841374\n",
      "step  388 | loss: 6.3753 | lr: 3.2559e-04 | Ts/s: 46880.4943678553\n",
      "step  389 | loss: 6.3806 | lr: 3.2643e-04 | Ts/s: 46600.182935083285\n",
      "step  390 | loss: 6.4176 | lr: 3.2727e-04 | Ts/s: 46947.414979739064\n",
      "step  391 | loss: 6.4255 | lr: 3.2811e-04 | Ts/s: 46509.397803404885\n",
      "step  392 | loss: 6.3500 | lr: 3.2895e-04 | Ts/s: 46077.83991449518\n",
      "step  393 | loss: 6.4167 | lr: 3.2979e-04 | Ts/s: 46848.59713054845\n",
      "step  394 | loss: 6.3131 | lr: 3.3063e-04 | Ts/s: 45408.82232157634\n",
      "step  395 | loss: 6.2892 | lr: 3.3147e-04 | Ts/s: 46730.799790391524\n",
      "step  396 | loss: 6.3068 | lr: 3.3231e-04 | Ts/s: 47152.08099728406\n",
      "step  397 | loss: 6.3171 | lr: 3.3315e-04 | Ts/s: 47249.600725801196\n",
      "step  398 | loss: 6.3130 | lr: 3.3399e-04 | Ts/s: 46867.65714346121\n",
      "step  399 | loss: 6.3229 | lr: 3.3483e-04 | Ts/s: 46945.728185699365\n",
      "step  400 | loss: 6.2959 | lr: 3.3566e-04 | Ts/s: 47524.01845952534\n",
      "step  401 | loss: 6.3008 | lr: 3.3650e-04 | Ts/s: 47440.538919513136\n",
      "step  402 | loss: 6.3106 | lr: 3.3734e-04 | Ts/s: 47773.01578217786\n",
      "step  403 | loss: 6.3048 | lr: 3.3818e-04 | Ts/s: 46992.12358263563\n",
      "step  404 | loss: 6.2961 | lr: 3.3902e-04 | Ts/s: 46962.538397491546\n",
      "step  405 | loss: 6.2721 | lr: 3.3986e-04 | Ts/s: 46869.725931206245\n",
      "step  406 | loss: 6.2289 | lr: 3.4070e-04 | Ts/s: 47282.07602438792\n",
      "step  407 | loss: 6.1972 | lr: 3.4154e-04 | Ts/s: 46777.229841583\n",
      "step  408 | loss: 6.2357 | lr: 3.4238e-04 | Ts/s: 46650.89877769998\n",
      "step  409 | loss: 6.2405 | lr: 3.4322e-04 | Ts/s: 47643.38890085438\n",
      "step  410 | loss: 6.2210 | lr: 3.4406e-04 | Ts/s: 47599.35145978567\n",
      "step  411 | loss: 6.2974 | lr: 3.4490e-04 | Ts/s: 47581.5181479487\n",
      "step  412 | loss: 6.2444 | lr: 3.4573e-04 | Ts/s: 47057.70892994149\n",
      "step  413 | loss: 6.2472 | lr: 3.4657e-04 | Ts/s: 47046.49155199215\n",
      "step  414 | loss: 6.2447 | lr: 3.4741e-04 | Ts/s: 47494.735473794826\n",
      "step  415 | loss: 6.2567 | lr: 3.4825e-04 | Ts/s: 46646.75243084496\n",
      "step  416 | loss: 6.2927 | lr: 3.4909e-04 | Ts/s: 47070.42782204031\n",
      "step  417 | loss: 6.4134 | lr: 3.4993e-04 | Ts/s: 46700.974329431854\n",
      "step  418 | loss: 6.3728 | lr: 3.5077e-04 | Ts/s: 47133.99825831108\n",
      "step  419 | loss: 6.4275 | lr: 3.5161e-04 | Ts/s: 46567.621528972704\n",
      "step  420 | loss: 6.3397 | lr: 3.5245e-04 | Ts/s: 45892.90411487622\n",
      "step  421 | loss: 6.3881 | lr: 3.5329e-04 | Ts/s: 47258.72437118677\n",
      "step  422 | loss: 6.3783 | lr: 3.5413e-04 | Ts/s: 47343.07316272497\n",
      "step  423 | loss: 6.3983 | lr: 3.5497e-04 | Ts/s: 45956.990721091206\n",
      "step  424 | loss: 6.3482 | lr: 3.5580e-04 | Ts/s: 46474.83651031308\n",
      "step  425 | loss: 6.3557 | lr: 3.5664e-04 | Ts/s: 47520.08000472991\n",
      "step  426 | loss: 6.3015 | lr: 3.5748e-04 | Ts/s: 47966.19497757588\n",
      "step  427 | loss: 6.3225 | lr: 3.5832e-04 | Ts/s: 46911.70850213139\n",
      "step  428 | loss: 6.2780 | lr: 3.5916e-04 | Ts/s: 47150.69994574618\n",
      "step  429 | loss: 6.3676 | lr: 3.6000e-04 | Ts/s: 46458.48251831636\n",
      "step  430 | loss: 6.3340 | lr: 3.6084e-04 | Ts/s: 47510.15306624239\n",
      "step  431 | loss: 6.2798 | lr: 3.6168e-04 | Ts/s: 47424.6816687385\n",
      "step  432 | loss: 6.3780 | lr: 3.6252e-04 | Ts/s: 46373.77482060983\n",
      "step  433 | loss: 6.3449 | lr: 3.6336e-04 | Ts/s: 46569.18362610936\n",
      "step  434 | loss: 6.2672 | lr: 3.6420e-04 | Ts/s: 46719.226527129984\n",
      "step  435 | loss: 6.2695 | lr: 3.6503e-04 | Ts/s: 47561.008093072036\n",
      "step  436 | loss: 6.2497 | lr: 3.6587e-04 | Ts/s: 46511.85220191421\n",
      "step  437 | loss: 6.3310 | lr: 3.6671e-04 | Ts/s: 47482.39937518228\n",
      "step  438 | loss: 6.3568 | lr: 3.6755e-04 | Ts/s: 46757.5224871643\n",
      "step  439 | loss: 6.3111 | lr: 3.6839e-04 | Ts/s: 46212.199163435645\n",
      "step  440 | loss: 6.1601 | lr: 3.6923e-04 | Ts/s: 47324.39044994001\n",
      "step  441 | loss: 6.2123 | lr: 3.7007e-04 | Ts/s: 47771.05535989809\n",
      "step  442 | loss: 6.2074 | lr: 3.7091e-04 | Ts/s: 47789.0082086696\n",
      "step  443 | loss: 6.1916 | lr: 3.7175e-04 | Ts/s: 47355.671454692914\n",
      "step  444 | loss: 6.2197 | lr: 3.7259e-04 | Ts/s: 47044.535954780906\n",
      "step  445 | loss: 6.1955 | lr: 3.7343e-04 | Ts/s: 47757.98099964802\n",
      "step  446 | loss: 6.1959 | lr: 3.7427e-04 | Ts/s: 47319.047239704094\n",
      "step  447 | loss: 6.2048 | lr: 3.7510e-04 | Ts/s: 46085.11324394606\n",
      "step  448 | loss: 6.2567 | lr: 3.7594e-04 | Ts/s: 47799.4904865565\n",
      "step  449 | loss: 6.1952 | lr: 3.7678e-04 | Ts/s: 47841.436480004006\n",
      "step  450 | loss: 6.1966 | lr: 3.7762e-04 | Ts/s: 46597.85053216238\n",
      "step  451 | loss: 6.1763 | lr: 3.7846e-04 | Ts/s: 47195.003648334794\n",
      "step  452 | loss: 6.2017 | lr: 3.7930e-04 | Ts/s: 47065.21231530551\n",
      "step  453 | loss: 6.1431 | lr: 3.8014e-04 | Ts/s: 46134.18897576575\n",
      "step  454 | loss: 6.1526 | lr: 3.8098e-04 | Ts/s: 46090.02684096323\n",
      "step  455 | loss: 6.0864 | lr: 3.8182e-04 | Ts/s: 46213.3684499101\n",
      "step  456 | loss: 6.1721 | lr: 3.8266e-04 | Ts/s: 46495.866374111596\n",
      "step  457 | loss: 6.1192 | lr: 3.8350e-04 | Ts/s: 46918.24942185977\n",
      "step  458 | loss: 6.1392 | lr: 3.8434e-04 | Ts/s: 45667.31432060537\n",
      "step  459 | loss: 6.1746 | lr: 3.8517e-04 | Ts/s: 47067.50308706259\n",
      "step  460 | loss: 6.1296 | lr: 3.8601e-04 | Ts/s: 45852.82103947893\n",
      "step  461 | loss: 6.1164 | lr: 3.8685e-04 | Ts/s: 45828.18035146821\n",
      "step  462 | loss: 6.0498 | lr: 3.8769e-04 | Ts/s: 46679.33941540933\n",
      "step  463 | loss: 6.1966 | lr: 3.8853e-04 | Ts/s: 47706.50331369035\n",
      "step  464 | loss: 6.1959 | lr: 3.8937e-04 | Ts/s: 46950.00704567184\n",
      "step  465 | loss: 6.2741 | lr: 3.9021e-04 | Ts/s: 47464.69352803192\n",
      "step  466 | loss: 6.3089 | lr: 3.9105e-04 | Ts/s: 47535.428771561325\n",
      "step  467 | loss: 6.2597 | lr: 3.9189e-04 | Ts/s: 47727.7295208968\n",
      "step  468 | loss: 6.2451 | lr: 3.9273e-04 | Ts/s: 47792.154175033014\n",
      "step  469 | loss: 6.2998 | lr: 3.9357e-04 | Ts/s: 47443.15399511684\n",
      "step  470 | loss: 6.2253 | lr: 3.9441e-04 | Ts/s: 47103.15764216981\n",
      "step  471 | loss: 6.2090 | lr: 3.9524e-04 | Ts/s: 46766.27108332102\n",
      "step  472 | loss: 6.2874 | lr: 3.9608e-04 | Ts/s: 46450.7297852533\n",
      "step  473 | loss: 6.2481 | lr: 3.9692e-04 | Ts/s: 46583.81463540786\n",
      "step  474 | loss: 6.2602 | lr: 3.9776e-04 | Ts/s: 46663.93141718134\n",
      "step  475 | loss: 6.2358 | lr: 3.9860e-04 | Ts/s: 47510.537992753496\n",
      "step  476 | loss: 6.1320 | lr: 3.9944e-04 | Ts/s: 47036.77857626803\n",
      "step  477 | loss: 6.1944 | lr: 4.0028e-04 | Ts/s: 47191.737304596565\n",
      "step  478 | loss: 6.1607 | lr: 4.0112e-04 | Ts/s: 46688.61883748862\n",
      "step  479 | loss: 6.2502 | lr: 4.0196e-04 | Ts/s: 47173.9864313261\n",
      "step  480 | loss: 6.1798 | lr: 4.0280e-04 | Ts/s: 46070.74068673799\n",
      "step  481 | loss: 6.2350 | lr: 4.0364e-04 | Ts/s: 45715.26050213327\n",
      "step  482 | loss: 6.1648 | lr: 4.0448e-04 | Ts/s: 46256.16188669296\n",
      "step  483 | loss: 6.2216 | lr: 4.0531e-04 | Ts/s: 46266.38153535184\n",
      "step  484 | loss: 6.1631 | lr: 4.0615e-04 | Ts/s: 47609.69503167018\n",
      "step  485 | loss: 6.1797 | lr: 4.0699e-04 | Ts/s: 46938.99825236794\n",
      "step  486 | loss: 6.1507 | lr: 4.0783e-04 | Ts/s: 45781.34195315338\n",
      "step  487 | loss: 6.1431 | lr: 4.0867e-04 | Ts/s: 47516.097026973664\n",
      "step  488 | loss: 6.1226 | lr: 4.0951e-04 | Ts/s: 46905.94279536284\n",
      "step  489 | loss: 6.0858 | lr: 4.1035e-04 | Ts/s: 45392.675738073696\n",
      "step  490 | loss: 6.1199 | lr: 4.1119e-04 | Ts/s: 46831.66391721493\n",
      "step  491 | loss: 6.1021 | lr: 4.1203e-04 | Ts/s: 47678.446418741514\n",
      "step  492 | loss: 6.1053 | lr: 4.1287e-04 | Ts/s: 47154.98086216363\n",
      "step  493 | loss: 6.1318 | lr: 4.1371e-04 | Ts/s: 45756.67518227048\n",
      "step  494 | loss: 6.1341 | lr: 4.1455e-04 | Ts/s: 46443.1434446682\n",
      "step  495 | loss: 6.0775 | lr: 4.1538e-04 | Ts/s: 47373.67059020205\n",
      "step  496 | loss: 6.0987 | lr: 4.1622e-04 | Ts/s: 47555.15264690067\n",
      "step  497 | loss: 6.0862 | lr: 4.1706e-04 | Ts/s: 47584.463868269595\n",
      "step  498 | loss: 6.0564 | lr: 4.1790e-04 | Ts/s: 47603.129944173525\n",
      "step  499 | loss: 6.0418 | lr: 4.1874e-04 | Ts/s: 47580.77791416687\n",
      "step  500 | loss: 6.0825 | lr: 4.1958e-04 | Ts/s: 46729.630985000476\n",
      "step  501 | loss: 6.0569 | lr: 4.2042e-04 | Ts/s: 47488.46560887786\n",
      "step  502 | loss: 6.0354 | lr: 4.2126e-04 | Ts/s: 47348.46359955793\n",
      "step  503 | loss: 6.0051 | lr: 4.2210e-04 | Ts/s: 46959.08453927464\n",
      "step  504 | loss: 6.0468 | lr: 4.2294e-04 | Ts/s: 47784.94264252979\n",
      "step  505 | loss: 6.0181 | lr: 4.2378e-04 | Ts/s: 47788.456746503696\n",
      "step  506 | loss: 6.0186 | lr: 4.2462e-04 | Ts/s: 47753.52457137159\n",
      "step  507 | loss: 6.0772 | lr: 4.2545e-04 | Ts/s: 47779.20737266873\n",
      "step  508 | loss: 5.9660 | lr: 4.2629e-04 | Ts/s: 47779.390082698315\n",
      "step  509 | loss: 6.0304 | lr: 4.2713e-04 | Ts/s: 47805.23478419926\n",
      "step  510 | loss: 6.1174 | lr: 4.2797e-04 | Ts/s: 47785.24273347125\n",
      "step  511 | loss: 6.1734 | lr: 4.2881e-04 | Ts/s: 47795.783612105435\n",
      "step  512 | loss: 6.1752 | lr: 4.2965e-04 | Ts/s: 47826.85685486129\n",
      "step  513 | loss: 6.1468 | lr: 4.3049e-04 | Ts/s: 47719.46769316616\n",
      "step  514 | loss: 6.1856 | lr: 4.3133e-04 | Ts/s: 47810.31622987183\n",
      "step  515 | loss: 6.1417 | lr: 4.3217e-04 | Ts/s: 47779.65169267349\n",
      "step  516 | loss: 6.2589 | lr: 4.3301e-04 | Ts/s: 47761.218317195686\n",
      "step  517 | loss: 6.2540 | lr: 4.3385e-04 | Ts/s: 47599.794501630335\n",
      "step  518 | loss: 6.1649 | lr: 4.3469e-04 | Ts/s: 47797.852035325785\n",
      "step  519 | loss: 6.1648 | lr: 4.3552e-04 | Ts/s: 47775.7330336783\n",
      "step  520 | loss: 6.1650 | lr: 4.3636e-04 | Ts/s: 47739.223365339116\n",
      "step  521 | loss: 6.1377 | lr: 4.3720e-04 | Ts/s: 47764.605471289724\n",
      "step  522 | loss: 6.1568 | lr: 4.3804e-04 | Ts/s: 47821.552459764374\n",
      "step  523 | loss: 6.0905 | lr: 4.3888e-04 | Ts/s: 47538.790141116944\n",
      "step  524 | loss: 6.0784 | lr: 4.3972e-04 | Ts/s: 47788.67795296244\n",
      "step  525 | loss: 6.0487 | lr: 4.4056e-04 | Ts/s: 47693.5232197659\n",
      "step  526 | loss: 6.1049 | lr: 4.4140e-04 | Ts/s: 47474.82798657066\n",
      "step  527 | loss: 6.0887 | lr: 4.4224e-04 | Ts/s: 46010.614458081145\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msuppress_errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainingConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     26\u001b[0m         loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m/\u001b[39mn_micro_steps\n\u001b[1;32m     27\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 28\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mclip_norm)\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "train(TrainingConfig())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will run into two problems:\n",
    "\n",
    "**Memory issues.** As mentioned above, the training has been set up for an A100 GPU with 80GB of memory. If you are using a less powerful GPU, you will see an out-of-memory error. To fix this, reduce the batch size to lower the memory load on the GPU. Start by halving the batch size and keep adjusting it until you find the largest batch size that fits on your GPU. (You may have to restart the Jupyter kernel to reset the GPU.)\n",
    "\n",
    "**High losses.** The training losses start at very high values. Recall that the model‚Äôs goal is to predict the next token. At the start, the model‚Äôs weights are random, so we expect it to output a uniform distribution over the vocabulary. This means each token should have a probability of $1/V$, where $V$ is the vocabulary size. Given this, what should the initial loss be? Think about the cross-entropy loss for a uniform distribution.\n",
    "\n",
    "#### solution for the task 3.07:\n",
    "When we tested with different batch sizes (just for testing):\n",
    "- 8 - ~ 700\n",
    "- 12 - ~500\n",
    "- 16 - ~350\n",
    "\n",
    "To calculate cross entropy loss we first need to look at the formula: sum(ln(1/q(x)) * p(x)). Since we know that q(x) is uniformly distributed, and that p(x) = 1 when summed over the vocabulary. We get that the loss should be ln(1/vocab_size) * 1 = (ln(1) - ln(1/50304)) * 1 = 0 - ln(1/50304) = 10,82... Which is the answer we get after fixing the gradient accumulation.\n",
    "\n",
    "(You will fix the problem with the high losses in the next task.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéì Task 3.08: Fixing the gradient accumulation\n",
    "\n",
    "The gradient accumulation in the `train()` function is not implemented correctly. To see this, consider the following example. We set up a linear layer and pass in some random input of shape $[2, 3]$. First, we compute the loss and gradients in the normal way. Then, we do the same thing using gradient accumulation over two singleton batches, in the way this is implemented in `train()`. As you will see, the two outputs are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0380, -0.3504, -0.4026],\n",
      "        [ 0.0380,  0.3504,  0.4026]])\n",
      "tensor([[-0.0380, -0.3504, -0.4026],\n",
      "        [ 0.0380,  0.3504,  0.4026]])\n"
     ]
    }
   ],
   "source": [
    "def accumulation_example():\n",
    "    # Set up a simple model and simulate some input\n",
    "    model = nn.Linear(3, 2)\n",
    "    x = torch.randn(2, 3)\n",
    "    y = torch.randint(0, 2, (2,))\n",
    "\n",
    "    # Compute the gradient on the complete input\n",
    "    model.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = F.cross_entropy(output, y)\n",
    "    loss.backward()\n",
    "    print(model.weight.grad)\n",
    "\n",
    "    # Compute the gradient using micro-batches (flawed)\n",
    "    model.zero_grad()\n",
    "    for i in range(2):\n",
    "        output = model(x[i : i + 1])\n",
    "        loss = F.cross_entropy(output, y[i : i + 1])/2# /2 devided by the microbatch size while calculating the loss before backpropagation.\n",
    "        loss.backward()\n",
    "    print(model.weight.grad)\n",
    "\n",
    "\n",
    "accumulation_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to fix the flawed implementation of gradient accumulation in the training loop.\n",
    "\n",
    "1. Propose a fix for the problem illustrated in the example.\n",
    "2. Validate your proposal by modifying the example.\n",
    "3. Once you are convinced that your fix is correct, apply it to the training loop.\n",
    "4. How does the fix affect the loss?\n",
    "\n",
    "### SOLUTION TO TASK 3.08 :\n",
    "Fix is to devide the loss by the number of micro_steps in a minibatch before backpropagation when calculating the cross entropy loss. The fix seem to make the loss equivalent with the one obtained by the gradient descent on the complete input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Efficiency optimisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training large language models from scratch is a computationally intensive task. Efficient training not only speeds up model convergence but also reduces hardware costs and energy consumption. As models grow in size, optimising the training process becomes increasingly important to ensure that resources are used effectively. In the second part of this lab, we will look into a few such optimisation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéì Task 3.09: Profiling the training loop\n",
    "\n",
    "Before we explore optimisations, we first need a way to measure training speed. We will define training speed as the **number of tokens processed per second**. Your task is to implement this measurement in the training loop.\n",
    "\n",
    "**Step&nbsp;1.** Keep track of the number of tokens processed in each optimisation step.\n",
    "\n",
    "**Step&nbsp;2.** Add a timer to measure the time taken per step. For accurate results, call `torch.cuda.synchronize()` before stopping the timer. This ensures all GPU computations finish before recording the time.\n",
    "\n",
    "Once you have added this measurement, train the model for a few steps and answer the following questions:\n",
    "\n",
    "* Based on your measured training speed, how long would it take to train on all data (300M tokens)?\n",
    "* How much data and time would be required to train a Chinchilla-optimal version of the model?\n",
    "\n",
    "#### solution for task 3.09 :\n",
    "Based on the numbers we got, it would take more than 6 hours to process all 300M tokens.\n",
    "\n",
    "A chinchilla optimal model would require ~20 tokens/parameter to train on. This means that the total amount of training data would be equal to 2.5 Billion tokens (with the model having ~125M parameters). Meaning that for us, it would take 50 hours to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FEEDBACK:To print the total time taken for the training, I think that you wanted to use the variable ‚ÄútotalStartTime‚Äù instead of ‚ÄústartTime‚Äù. I don‚Äôt see your answers to the final questions\n",
    "###### SOLUTION:this has been updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating-point representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the optimisations we can make when training large language models is choosing an appropriate floating-point representation. A floating-point number is represented using three components: a **sign bit**, an **exponent**, and a **fraction** (also called mantissa). The number of bits assigned to the exponent determines the *range* of the representation, while the number of bits in the fraction determines the *precision*.\n",
    "\n",
    "The most commonly used floating-point format is single-precision floating point (fp32), which uses 1&nbsp;sign bit, 8&nbsp;exponent bits, and 23&nbsp;fraction bits. However, most deep learning computations do not require the full 23-bit precision of fp32. As a result, modern hardware supports lower-precision formats that improve performance and memory efficiency while maintaining training stability. We are particularly interested in two of those:\n",
    "\n",
    "**TensorFloat-32 (tf32)** is a precision format introduced by NVIDIA. It keeps the same 8-bit exponent as fp32, preserving the same numerical range. However, tf32 reduces the fraction size to 10 bits, leading to lower precision but faster matrix multiplications on dedicated GPUs.\n",
    "\n",
    "**Brain Floating Point 16 (bf16)** is another reduced precision format widely used in NVIDIA GPUs. Like tf32, bf16 has the same 8-bit exponent as fp32, and therefore, the same range. However, bf16 has only 7 bits for the fraction. Unlike tf32, which is primarily used for internal computations, bf16 can be directly stored and used for activations, weights, and gradients, which can save memory (16 bits per value instead of 32)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéà Task 3.10: Exploring floating-point representations\n",
    "\n",
    "Your task is to rewrite the training loop to take advantage of specialised floating-point representations.\n",
    "\n",
    "**Step&nbsp;1.** By default, PyTorch uses the fp32 format for internal computations (‚Äúhighest precision‚Äù). Read the documentation of [`torch.set_float32_matmul_precision()`](https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html) to find out how to change this default and use tf32 if possible.\n",
    "\n",
    "**Step&nbsp;2.** Computing the forward pass and the loss requires even less precision than other parts of the training loop. Read the documentation of [`torch.autocast()`](https://pytorch.org/docs/stable/amp.html#torch.autocast) to find out how to execute these operations using the bf16 format.\n",
    "\n",
    "**Step&nbsp;3.** Repeat your profiling experiments with the modified training loop. How much time would be required to train the model now that you have implemented the floating-point optimisations?\n",
    "\n",
    "#### SOLUTION FOR TASK 3.10 :\n",
    "After the floating point optimizations the time to process all 300M tokens would be 1,89 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just-in-time compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second optimisation we will explore is **just-in-time (JIT) compilation** ‚Äì a technique to speed up code by compiling it at runtime, rather than before execution. This allows the compiler to optimise the code dynamically based on actual inputs and hardware conditions.\n",
    "\n",
    "PyTorch provides a function [`torch.compile()`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) that uses JIT compilation to automatically optimise deep learning models. Instead of executing PyTorch operations one by one, this function analyses the computation, restructures it, and generates highly efficient machine code. This optimised code can often be run as a single fused operation on the GPU.\n",
    "\n",
    "The performance boost from JIT compilation is especially noticeable on high-end GPUs like the NVIDIA A100 or H100, but it can still provide smaller speedups on less powerful GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéà Task 3.11: Exploring just-in-time compilation\n",
    "\n",
    "In this task, you will measure how much `torch.compile()` improves the training speed of the GPT-2 model. To enable JIT compilation, add the following line to the training loop:\n",
    "\n",
    "```\n",
    "model = torch.compile(model)\n",
    "```\n",
    "\n",
    "Make sure to place this *after* moving the model to the training device so the optimisation can be tailored to the hardware you are using. Once you have enabled JIT compilation, rerun your profiling experiments and compare the training speed before and efter.\n",
    "\n",
    "\n",
    "##### Answer:\n",
    "In total we have 300 million tokens to train on\n",
    "We process 524288 in each step. This means that we have 300 000 000/524288 which is ~572,2 which is the amount of steps it takes to complete an epoch.\n",
    "\n",
    "In training, we have 4768 steps, which is combined with the result above to get the amount of epochs for training, which is ~8.33...\n",
    "We calculated the time to process all 300M tokens to be ~1.74 hours with the JIT compilation.\n",
    "\n",
    "The final answer we get is 1.74*8.33 = ~14.5 hours for the total training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluate the pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all the code needed to train a GPT-2 model on the FineWeb-Edu dataset. However, training on the full dataset is not practical within the time and resource limits of this lab.\n",
    "\n",
    "To save time, we provide a pretrained model that was trained using the exact same settings you developed earlier. The only difference is that we used a compute node with 8 NVIDIA A100 GPUs (80GB each). With this setup, a full Chinchilla-optimal training run takes about 30 minutes.\n",
    "\n",
    "We provide the trained model in the file `gpt-2-fineweb-edu.pt`. You can load it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = Model(Config(n_vocab=50304))\n",
    "pretrained.load_state_dict(torch.load(\"/courses/TDDE09/labs/lab3/gpt-2-fineweb-edu.pt\"))\n",
    "pretrained.lm_head.weight = pretrained.wte.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéì Task 3.12: Evaluating the pretrained model\n",
    "\n",
    "The next cell contains code for evaluating the FineWeb-Edu model on the same small sample from the [HellaSwag](https://rowanzellers.com/hellaswag/) benchmark you already used in lab&nbsp;2. How does its score compare to that of the original GPT-2 model? What does this result tell you about the impact of data quality on the downstream performance of language models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution for taks 3.12 :\n",
    "The result from this lab is 0.78 points lower than the original GPT-2 model.\n",
    "\n",
    "We assume that the pretraining for GPT-2 in lab 2 used 300B tokens for pretraining.\n",
    "\n",
    "This shows how important the quality of the data is. With much less data, the model on lab 3 successfully scored very close to the model from lab 2 which was based on OpenAIs training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FineWeb-Edu Model Performance:\n",
    "- The FineWeb-Edu model achieves 29.69% accuracy, which is slightly lower than the original GPT-2 model's performance.\n",
    "- This suggests that the FineWeb-Edu dataset, while focused on educational content, may not have provided enough diversity or quality to outperform the original GPT-2 model on the HellaSwag benchmark.\n",
    "\n",
    "\n",
    "Impact of Data Quality:\n",
    "- Data Diversity: The original GPT-2 was trained on a diverse dataset (WebText), which includes a wide range of topics and styles. This diversity likely helps the model generalize better to tasks like HellaSwag,          which requires commonsense reasoning across various contexts.\n",
    "- Data Quality: FineWeb-Edu is focused on educational content, which may be more specialized and less diverse. While this could improve performance on educational tasks, it may not generalize as well to broader           benchmarks like HellaSwag.\n",
    "- Task-Specific Performance: The HellaSwag benchmark tests commonsense reasoning, which may not align perfectly with the educational focus of FineWeb-Edu. This could explain the slightly lower performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 29.69%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "with open(\"hellaswag-mini.jsonl\") as f:\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    for line in f:\n",
    "        sample = json.loads(line)\n",
    "        prefix = tokenizer.encode(sample[\"ctx\"])\n",
    "        ending_scores = []\n",
    "        for i, ending in enumerate(sample[\"endings\"]):\n",
    "            suffix = tokenizer.encode(\" \" + ending)\n",
    "            context = torch.tensor([prefix + suffix], dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                logits = pretrained(context)\n",
    "                ending_score = torch.nn.functional.cross_entropy(\n",
    "                    logits[0, -len(suffix) - 1 : -1], context[0, -len(suffix) :]\n",
    "                )\n",
    "            ending_scores.append((ending_score, i))\n",
    "        predicted = min(ending_scores)[1]\n",
    "        n_correct += int(predicted == sample[\"label\"])\n",
    "        n_total += 1\n",
    "    print(f\"Accuracy: {n_correct / n_total:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Feedback:Could you elaborate a bit more on your answer?\n",
    "###### solution :this feedback has been updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ü•≥ Congratulations on finishing lab&nbsp;3!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
